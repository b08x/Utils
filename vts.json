{
  "files": [
    {
      "filename": "audio_processing.py",
      "content": "import os\nimport sys\nimport subprocess\nimport json\nimport progressbar\nfrom pydub import AudioSegment\n\nparent_directory = os.path.abspath('..')\n\nsys.path.append(parent_directory)\n\n\nfrom vts import (\n    audio_processing,\n    metadata_generation,\n    segment_analysis,\n    topic_modeling,\n    utils,\n)\n\ndef convert_to_mono_and_resample(input_file, output_file, sample_rate=16000):\n    \"\"\"Converts audio to mono, resamples, applies gain control, and a high-pass filter.\"\"\"\n    try:\n        command = [\n            \"ffmpeg\",\n            \"-i\",\n            input_file,\n            \"-af\",\n            \"highpass=f=200, acompressor=threshold=-12dB:ratio=4:attack=5:release=50, loudnorm\",\n            \"-ar\",\n            str(sample_rate),\n            \"-ac\",\n            \"1\",\n            \"-c:a\",\n            \"aac\",\n            \"-b:a\",\n            \"128k\",  # Use AAC codec for M4A\n            output_file,\n        ]\n        subprocess.run(command, check=True)\n        return {\n            \"status\": \"success\",\n            \"message\": f\"Audio converted to mono, resampled to {sample_rate}Hz, gain-adjusted, high-pass filtered, and saved to {output_file}\",\n        }\n    except subprocess.CalledProcessError as e:\n        return {\n            \"status\": \"error\",\n            \"message\": f\"Error during audio conversion: {str(e)}\",\n        }\n\n\ndef normalize_audio(input_file, output_file, lowpass_freq=8000, highpass_freq=100):\n    \"\"\"Normalizes audio using ffmpeg-normalize.\"\"\"\n    try:\n        command = [\n            \"ffmpeg-normalize\",\n            \"-pr\",  # Preserve ReplayGain tags\n            \"-tp\",\n            \"-3.0\",\n            \"-nt\",\n            \"rms\",\n            input_file,\n            \"-prf\",\n            f\"highpass=f={highpass_freq}, loudnorm\",\n            \"-prf\",\n            \"dynaudnorm=p=0.4:s=15\",\n            \"-pof\",\n            f\"lowpass=f={lowpass_freq}\",\n            \"-ar\",\n            \"48000\",\n            \"-c:a\",\n            \"pcm_s16le\",\n            \"--keep-loudness-range-target\",\n            \"-o\",\n            output_file,\n        ]\n        subprocess.run(command, check=True)\n        return {\n            \"status\": \"success\",\n            \"message\": f\"Audio normalized and saved to {output_file}\",\n        }\n    except subprocess.CalledProcessError as e:\n        return {\n            \"status\": \"error\",\n            \"message\": f\"Error during audio normalization: {str(e)}\",\n        }\n\n\ndef remove_silence(input_file, output_file, duration=\"1.5\", threshold=\"-25\"):\n    \"\"\"Removes silence from audio using unsilence.\"\"\"\n    try:\n        command = [\n            \"unsilence\",\n            \"-y\",  # non-interactive mode\n            \"-d\",  # Delete silent parts\n            \"-ss\",\n            duration,  # Minimum silence duration\n            \"-sl\",\n            threshold,  # Silence threshold\n            input_file,\n            output_file,\n        ]\n        subprocess.run(command, check=True)\n        return {\n            \"status\": \"success\",\n            \"message\": f\"Silence removed from audio and saved to {output_file}\",\n        }\n    except subprocess.CalledProcessError as e:\n        return {\"status\": \"error\", \"message\": f\"Error during silence removal: {str(e)}\"}\n\n\ndef extract_audio(video_path, output_path):\n    print(\"Extracting audio from video...\")\n    try:\n        audio = AudioSegment.from_file(video_path)\n        audio.export(output_path, format=\"wav\")\n        print(\"Audio extraction complete.\")\n    except Exception as e:\n        print(f\"Error during audio extraction: {str(e)}\")\n        raise",
      "path": "/"
    },
    {
      "filename": "metadata_generation.py",
      "content": "import os\nimport sys\nimport json\nimport progressbar\n\nparent_directory = os.path.abspath('..')\n\nsys.path.append(parent_directory)\n\n\nfrom vts import (\n    audio_processing,\n    metadata_generation,\n    segment_analysis,\n    topic_modeling,\n    utils,\n)\n\ndef save_transcription(transcription, project_path):\n    transcription_path = os.path.join(project_path, \"transcription.json\")\n    with open(transcription_path, \"w\") as f:\n        json.dump(transcription, f, indent=2)\n    print(f\"Transcription saved to: {transcription_path}\")\n\n\ndef save_transcript(transcript, project_path):\n    transcript_path = os.path.join(project_path, \"transcript.json\")\n    with open(transcript_path, \"w\") as f:\n        json.dump(transcript, f, indent=2)\n    print(f\"Transcript saved to: {transcript_path}\")\n\n\ndef load_transcript(transcript_path):\n    with open(transcript_path, \"r\") as f:\n        return json.load(f)\n\n\ndef generate_metadata(segments, lda_model):\n    print(\"Generating metadata for segments...\")\n    metadata = []\n    for i, segment in enumerate(progressbar.progressbar(segments)):\n        segment_metadata = {\n            \"segment_id\": i + 1,\n            \"start_time\": segment[\"start\"],\n            \"end_time\": segment[\"end\"],\n            \"duration\": segment[\"end\"] - segment[\"start\"],\n            \"dominant_topic\": segment[\"topic\"],\n            \"top_keywords\": [\n                word for word, _ in lda_model.show_topic(segment[\"topic\"], topn=5)\n            ],\n            \"transcript\": segment[\"content\"],\n        }\n        metadata.append(segment_metadata)\n    print(\"Metadata generation complete.\")\n    return metadata\n",
      "path": "/"
    },
    {
      "filename": "segment_analysis.py",
      "content": "import os\nimport sys\nimport json\nimport progressbar\nimport google.generativeai as genai\nfrom PIL import Image\nfrom moviepy.editor import VideoFileClip\n\nparent_directory = os.path.abspath('..')\n\nsys.path.append(parent_directory)\n\n\nfrom vts import (\n    audio_processing,\n    metadata_generation,\n    segment_analysis,\n    topic_modeling,\n    utils,\n)\n\ndef analyze_segment_with_gemini(segment_path, transcript):\n    print(f\"Analyzing segment: {segment_path}\")\n    # Set up the Gemini model\n    genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n\n    # Load the video segment as an image (first frame)\n    video = VideoFileClip(segment_path)\n    frame = video.get_frame(0)\n    image = Image.fromarray(frame)\n    video.close()\n\n    # Prepare the prompt\n    prompt = f\"Analyze this video segment. The transcript for this segment is: '{transcript}'. Describe the main subject matter, key visual elements, and how they relate to the transcript.\"\n\n    # Generate content\n    response = model.generate_content([prompt, image])\n\n    return response.text\n\n\ndef split_and_analyze_video(input_video, segments, output_dir):\n    print(\"Splitting video into segments and analyzing...\")\n    try:\n        video = VideoFileClip(input_video)\n        analyzed_segments = []\n\n        for i, segment in enumerate(progressbar.progressbar(segments)):\n            start_time = segment[\"start_time\"]\n            end_time = segment[\"end_time\"]\n            segment_clip = video.subclip(start_time, end_time)\n            output_path = os.path.join(output_dir, f\"segment_{i+1}.mp4\")\n            segment_clip.write_videofile(\n                output_path, codec=\"libx264\", audio_codec=\"aac\"\n            )\n\n            # Analyze the segment with Gemini\n            gemini_analysis = analyze_segment_with_gemini(\n                output_path, segment[\"transcript\"]\n            )\n\n            analyzed_segments.append(\n                {\n                    \"segment_id\": i + 1,\n                    \"start_time\": start_time,\n                    \"end_time\": end_time,\n                    \"transcript\": segment[\"transcript\"],\n                    \"topic\": segment[\"dominant_topic\"],\n                    \"keywords\": segment[\"top_keywords\"],\n                    \"gemini_analysis\": gemini_analysis,\n                }\n            )\n\n        video.close()\n        print(\"Video splitting and analysis complete.\")\n        return analyzed_segments\n    except Exception as e:\n        print(f\"Error during video splitting and analysis: {str(e)}\")\n        raise",
      "path": "/"
    },
    {
      "filename": "topic_modeling.py",
      "content": "import os\nimport sys\nimport json\nimport progressbar\nimport spacy\nfrom gensim import corpora\nfrom gensim.models import LdaMulticore\n\nparent_directory = os.path.abspath('..')\n\nsys.path.append(parent_directory)\n\n\nfrom vts import (\n    audio_processing,\n    metadata_generation,\n    segment_analysis,\n    topic_modeling,\n    utils,\n)\n\n# Load spaCy model (move this to a setup function or main if possible)\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef preprocess_text(text):\n    print(\"Preprocessing text...\")\n    doc = nlp(text)\n    subjects = []\n\n    for sent in doc.sents:\n        for token in sent:\n            if \"subj\" in token.dep_:\n                if token.dep_ in [\"nsubj\", \"nsubjpass\", \"csubj\"]:\n                    subject = get_compound_subject(token)\n                    subjects.append(subject)\n\n    cleaned_subjects = [\n        [\n            token.lemma_.lower()\n            for token in nlp(subject)\n            if not token.is_stop and not token.is_punct and token.is_alpha\n        ]\n        for subject in subjects\n    ]\n\n    cleaned_subjects = [\n        list(s) for s in set(tuple(sub) for sub in cleaned_subjects) if s\n    ]\n\n    print(\n        f\"Text preprocessing complete. Extracted {len(cleaned_subjects)} unique subjects.\"\n    )\n    return cleaned_subjects\n\n\ndef get_compound_subject(token):\n    subject = [token.text]\n    for left_token in token.lefts:\n        if left_token.dep_ == \"compound\":\n            subject.insert(0, left_token.text)\n    for right_token in token.rights:\n        if right_token.dep_ == \"compound\":\n            subject.append(right_token.text)\n    return \" \".join(subject)\n\n\ndef perform_topic_modeling(subjects, num_topics=5):\n    print(f\"Performing topic modeling with {num_topics} topics...\")\n    dictionary = corpora.Dictionary(subjects)\n    corpus = [dictionary.doc2bow(subject) for subject in subjects]\n    lda_model = LdaMulticore(\n        corpus=corpus,\n        id2word=dictionary,\n        num_topics=num_topics,\n        random_state=100,\n        chunksize=100,\n        passes=10,\n        per_word_topics=True,\n    )\n    print(\"Topic modeling complete.\")\n    return lda_model, corpus, dictionary\n\n\ndef identify_segments(transcript, lda_model, dictionary, num_topics):\n    print(\"Identifying segments based on topics...\")\n    segments = []\n    current_segment = {\"start\": 0, \"end\": 0, \"content\": \"\", \"topic\": None}\n\n    for sentence in progressbar.progressbar(transcript):\n        subjects = preprocess_text(sentence[\"content\"])\n        if not subjects:\n            continue\n\n        bow = dictionary.doc2bow([token for subject in subjects for token in subject])\n        topic_dist = lda_model.get_document_topics(bow)\n        dominant_topic = max(topic_dist, key=lambda x: x[1])[0] if topic_dist else None\n\n        if dominant_topic != current_segment[\"topic\"]:\n            if current_segment[\"content\"]:\n                current_segment[\"end\"] = sentence[\"start\"]\n                segments.append(current_segment)\n            current_segment = {\n                \"start\": sentence[\"start\"],\n                \"end\": sentence[\"end\"],\n                \"content\": sentence[\"content\"],\n                \"topic\": dominant_topic,\n            }\n        else:\n            current_segment[\"end\"] = sentence[\"end\"]\n            current_segment[\"content\"] += \" \" + sentence[\"content\"]\n\n    if current_segment[\"content\"]:\n        segments.append(current_segment)\n\n    print(f\"Identified {len(segments)} segments.\")\n    return segments",
      "path": "/"
    },
    {
      "filename": "utils.py",
      "content": "import os\nimport sys\nimport time\nimport json\nimport progressbar\nimport videogrep  # Make sure videogrep is installed\nfrom deepgram import DeepgramClient, PrerecordedOptions, FileSource, DeepgramError\nfrom groq import Groq\n\nparent_directory = os.path.abspath('..')\n\nsys.path.append(parent_directory)\n\n\nfrom vts import (\n    audio_processing,\n    metadata_generation,\n    segment_analysis,\n    topic_modeling,\n    utils,\n)\n\n\ndef create_project_folder(input_path, base_output_dir):\n    base_name = os.path.splitext(os.path.basename(input_path))[0]\n    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n    project_name = f\"{base_name}_{timestamp}\"\n    project_path = os.path.join(base_output_dir, project_name)\n    os.makedirs(project_path, exist_ok=True)\n    return project_path\n\ndef transcribe_file_deepgram(client, file_path, options, max_retries=3, retry_delay=5):\n    print(\"Transcribing audio using Deepgram...\")\n    for attempt in range(max_retries):\n        try:\n            with open(file_path, \"rb\") as audio:\n                buffer_data = audio.read()\n                payload: FileSource = {\"buffer\": buffer_data, \"mimetype\": \"audio/mp4\"}\n                response = client.listen.rest.v(\"1\").transcribe_file(payload, options)\n            print(\"Transcription complete.\")\n            return json.loads(response.to_json())\n        except DeepgramError as e:\n            if attempt < max_retries - 1:\n                print(\n                    f\"API call failed. Retrying in {retry_delay} seconds... (Attempt {attempt + 1}/{max_retries})\"\n                )\n                time.sleep(retry_delay)\n            else:\n                print(f\"Transcription failed after {max_retries} attempts: {str(e)}\")\n                raise\n        except Exception as e:\n            print(f\"Unexpected error during transcription: {str(e)}\")\n            raise\n\n\ndef transcribe_file_groq(\n    client, file_path, model=\"whisper-large-v3\", language=\"en\", prompt=None\n):\n    print(\"Transcribing audio using Groq...\")\n    try:\n        with open(file_path, \"rb\") as file:\n            transcription = client.audio.transcriptions.create(\n                file=(file_path, file.read()),\n                model=model,\n                prompt=prompt,\n                response_format=\"verbose_json\",\n                language=language,\n                temperature=0.2\n            )\n        print(\"Transcription complete.\")\n        return json.loads(transcription.text)\n    except Exception as e:\n        print(f\"Error during Groq transcription: {str(e)}\")\n        raise\n\ndef handle_audio_video(video_path, project_path):\n    audio_dir = os.path.join(project_path, \"audio\")\n    os.makedirs(audio_dir, exist_ok=True)\n\n    normalized_video_path = os.path.join(project_path, \"normalized_video.mkv\")\n    normalize_result = audio_processing.normalize_audio(video_path, normalized_video_path)\n    if normalize_result[\"status\"] == \"error\":\n        print(f\"Error during audio normalization: {normalize_result['message']}\")\n        # Handle the error (e.g., exit or continue without normalization)\n    else:\n        print(normalize_result[\"message\"])\n\n    # Remove silence\n    unsilenced_video_path = os.path.join(project_path, \"unsilenced_video.mkv\")\n    silence_removal_result = audio_processing.remove_silence(\n        normalized_video_path, unsilenced_video_path\n    )\n    if silence_removal_result[\"status\"] == \"error\":\n        print(f\"Error during silence removal: {silence_removal_result['message']}\")\n        # Handle the error (e.g., exit or continue without silence removal)\n    else:\n        print(silence_removal_result[\"message\"])\n\n    # Extract audio from unsilenced video\n    raw_audio_path = os.path.join(audio_dir, \"extracted_audio.wav\")\n    audio_processing.extract_audio(unsilenced_video_path, raw_audio_path)\n\n    # Convert to mono and resample for transcription\n    mono_resampled_audio_path = os.path.join(audio_dir, \"mono_resampled_audio.m4a\")\n    conversion_result = audio_processing.convert_to_mono_and_resample(\n        raw_audio_path, mono_resampled_audio_path\n    )\n    if conversion_result[\"status\"] == \"error\":\n        print(f\"Error during audio conversion: {conversion_result['message']}\")\n        # Handle the error (e.g., exit or continue without conversion)\n    else:\n        print(conversion_result[\"message\"])\n\n    return unsilenced_video_path, mono_resampled_audio_path\n\n\ndef handle_transcription(\n    video_path, audio_path, project_path, api=\"deepgram\", num_topics=2, groq_prompt=None\n):\n    segments_dir = os.path.join(project_path, \"segments\")\n    os.makedirs(segments_dir, exist_ok=True)\n\n    print(\"Parsing transcript with Videogrep...\")\n    transcript = videogrep.parse_transcript(video_path)\n    print(\"Transcript parsing complete.\")\n\n    if not transcript:\n        print(\"No transcript found. Transcribing audio...\")\n        deepgram_key = os.getenv(\"DG_API_KEY\")\n        groq_key = os.getenv(\"GROQ_API_KEY\")\n\n        if not deepgram_key:\n            raise ValueError(\"DG_API_KEY environment variable is not set\")\n        if not groq_key and api == \"groq\":\n            raise ValueError(\"GROQ_API_KEY environment variable is not set\")\n\n        if api == \"deepgram\":\n            deepgram_client = DeepgramClient(deepgram_key)\n            deepgram_options = PrerecordedOptions(\n                model=\"nova-2\",\n                language=\"en\",\n                topics=True,\n                intents=True,\n                smart_format=True,\n                punctuate=True,\n                paragraphs=True,\n                utterances=True,\n                diarize=True,\n                filler_words=True,\n                sentiment=True,\n            )\n            # Transcribe the normalized audio\n            transcription = transcribe_file_deepgram(\n                deepgram_client, audio_path, deepgram_options\n            )\n            transcript = [\n                {\n                    \"content\": utterance[\"transcript\"],\n                    \"start\": utterance[\"start\"],\n                    \"end\": utterance[\"end\"],\n                }\n                for utterance in transcription[\"results\"][\"utterances\"]\n            ]\n        else:  # Groq\n            groq_client = Groq(api_key=groq_key)\n            # Transcribe the normalized audio\n            transcription = transcribe_file_groq(\n                groq_client, audio_path, prompt=groq_prompt\n            )\n            transcript = [\n                {\n                    \"content\": segment[\"text\"],\n                    \"start\": segment[\"start\"],\n                    \"end\": segment[\"end\"],\n                }\n                for segment in transcription[\"segments\"]\n            ]\n\n    metadata_generation.save_transcription(transcription, project_path)\n\n    metadata_generation.save_transcript(transcript, project_path)\n    results = process_transcript(transcript, project_path, num_topics)\n\n    # Split the video and analyze segments\n    analyzed_segments = segment_analysis.split_and_analyze_video(\n        video_path, results[\"segments\"], segments_dir\n    )\n\n    # Update results with analyzed segments\n    results[\"analyzed_segments\"] = analyzed_segments\n\n    # Save updated results\n    results_path = os.path.join(project_path, \"results.json\")\n    with open(results_path, \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # print(\"Cleaning up temporary files...\")\n    # os.remove(raw_audio_path)\n    # os.remove(normalized_audio_path) # Uncomment to remove normalized audio\n\n    return results\n\n\ndef process_video(\n    video_path, project_path, api=\"deepgram\", num_topics=2, groq_prompt=None\n):\n\n    unsilenced_video_path, mono_resampled_audio_path = handle_audio_video(\n        video_path, project_path\n    )\n    results = handle_transcription(\n        unsilenced_video_path,\n        mono_resampled_audio_path,\n        project_path,\n        api,\n        num_topics,\n        groq_prompt,\n    )\n\n    return results\n\n\ndef process_transcript(transcript, project_path, num_topics=5):\n    full_text = \" \".join([sentence[\"content\"] for sentence in transcript])\n    preprocessed_subjects = topic_modeling.preprocess_text(full_text)\n    lda_model, corpus, dictionary = topic_modeling.perform_topic_modeling(\n        preprocessed_subjects, num_topics\n    )\n\n    segments = topic_modeling.identify_segments(transcript, lda_model, dictionary, num_topics)\n    metadata = metadata_generation.generate_metadata(segments, lda_model)\n\n    results = {\n        \"topics\": [\n            {\n                \"topic_id\": topic_id,\n                \"words\": [word for word, _ in lda_model.show_topic(topic_id, topn=10)],\n            }\n            for topic_id in range(num_topics)\n        ],\n        \"segments\": metadata,\n    }\n\n    results_path = os.path.join(project_path, \"results.json\")\n    with open(results_path, \"w\") as f:\n        json.dump(results, f, indent=2)\n    print(f\"Results saved to: {results_path}\")\n\n    return results\n",
      "path": "/"
    },
    {
      "filename": "main.py",
      "content": "#!/usr/bin/env python \n\nimport os\nimport sys\nimport argparse\nimport json\nfrom dotenv import load_dotenv\nimport videogrep  # Make sure videogrep is installed\nimport progressbar  # Make sure progressbar2 is installed\n\nparent_directory = os.path.abspath('..')\n\nsys.path.append(parent_directory)\n\n\nfrom vts import (\n    audio_processing,\n    metadata_generation,\n    segment_analysis,\n    topic_modeling,\n    utils,\n)\n\n\nload_dotenv()\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Process video or transcript for topic-based segmentation and multi-modal analysis\"\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--input\",\n        required=True,\n        help=\"Path to the input video file or transcript JSON\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output\",\n        default=os.getcwd(),\n        help=\"Base output directory for project folders\",\n    )\n    parser.add_argument(\n        \"--api\",\n        choices=[\"deepgram\", \"groq\"],\n        default=\"deepgram\",\n        help=\"Choose API: deepgram or groq\",\n    )\n    parser.add_argument(\n        \"--topics\", type=int, default=5, help=\"Number of topics for LDA model\"\n    )\n    parser.add_argument(\"--groq-prompt\", help=\"Optional prompt for Groq transcription\")\n    args = parser.parse_args()\n\n    project_path = utils.create_project_folder(args.input, args.output)\n    print(f\"Created project folder: {project_path}\")\n\n    try:\n        if args.input.endswith(\".json\"):\n            transcript = load_transcript(args.input)\n            results = utils.process_transcript(transcript, project_path, args.topics)\n            print(\n                \"Note: Video splitting and Gemini analysis are not performed when processing a transcript file.\"\n            )\n        else:\n            results = utils.process_video(\n                args.input, project_path, args.api, args.topics, args.groq_prompt\n            )\n\n        print(f\"\\nProcessing complete. Project folder: {project_path}\")\n        print(f\"Results saved in: {os.path.join(project_path, 'results.json')}\")\n        print(\"\\nTop words for each topic:\")\n        for topic in results[\"topics\"]:\n            print(f\"Topic {topic['topic_id'] + 1}: {', '.join(topic['words'])}\")\n        print(f\"\\nGenerated and analyzed {len(results['analyzed_segments'])} segments\")\n\n        if not args.input.endswith(\".json\"):\n            print(f\"Video segments saved in: {os.path.join(project_path, 'segments')}\")\n    except Exception as e:\n        print(f\"\\nAn error occurred during processing: {str(e)}\")\n        print(\"Please check the project folder for any partial results or logs.\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "path": "/"
    },
    {
      "filename": "__init__.py",
      "content": null,
      "path": "/"
    }
  ]
}
