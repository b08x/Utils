{
  "files": [
    {
      "filename": "README.md",
      "path": "/",
      "content": "# Utility Scripts\n\n- [ ] index & descriptions\n\n\n```shell\n├── bin\n│   ├── backup.sh\n│   ├── brightness.rb\n│   ├── brightness.sh\n│   ├── cleanup.sh\n│   ├── convert2ogg.sh\n│   ├── disconnect_pulse.sh\n│   ├── docker-backup.sh\n│   ├── dots.sh\n│   ├── guake-utils\n│   ├── jack-load.sh\n│   ├── mic_mute.sh\n│   ├── process_video.sh\n│   ├── remote_docker_compose_manager.sh\n│   ├── ripfzf-vscode.sh\n│   ├── search_web.sh\n│   ├── set-govna.sh\n│   ├── timer.sh\n│   ├── whisper-stream\n│   └── yt-dlp-transcript.sh\n```\n\n\n* anything in dev/staging are WIP scripts\n\n* anything in defunct are scripts no longer in use\n\n\nThis Python script is designed to analyze video content by identifying key topics and splitting the video into segments based on those topics. It then uses a combination of NLP techniques, topic modeling, and potentially AI-powered analysis (like Google's Gemini) to provide a deeper understanding of the video's content.\n\nHere's a breakdown of the script's functionality:\n\n1. Preprocessing:\n\nAudio Extraction and Enhancement: If the input is a video file, the script first extracts the audio. It then applies audio normalization, silence removal, and format conversion to prepare it for transcription.\nTranscription: The script can transcribe the audio using either the Deepgram or Groq API. You can choose the API using the --api argument.\nText Preprocessing: The transcript is processed using spaCy, a natural language processing library. This involves tasks like tokenization, lemmatization, and removing stop words to prepare the text for topic modeling.\n2. Topic Modeling and Segmentation:\n\nSubject Extraction: The script identifies the main subjects discussed in the transcript using spaCy's dependency parsing capabilities.\nTopic Modeling: It performs Latent Dirichlet Allocation (LDA) using gensim, a topic modeling library. This identifies latent topics within the text based on the co-occurrence of words.\nSegment Identification: The script analyzes the dominant topic for each sentence in the transcript and groups consecutive sentences with the same dominant topic into segments.\n3. Metadata Generation and Analysis:\n\nMetadata Extraction: For each segment, the script extracts metadata such as start and end times, duration, dominant topic, top keywords related to the topic, and the transcript of the segment.\nVideo Splitting: The script uses the moviepy library to split the original video into segments based on the identified time boundaries.\nAI-Powered Analysis (Optional): If you have access to Google's Gemini API, the script can further analyze each segment by generating a description of the main subject matter, key visual elements, and their relation to the transcript.\n4. Output:\n\nThe script saves the results, including identified topics, segment metadata, and AI-generated analysis (if enabled), to a JSON file.\nIt also saves the individual video segments as separate files.\nKey Features:\n\nFlexibility: The script can process either a video file or a pre-existing transcript in JSON format.\nCustomizability: You can customize the number of topics for LDA, choose between different transcription APIs, and provide a prompt for Groq transcription.\nAI Integration: The optional integration with Google's Gemini allows for a more comprehensive analysis of the video content, combining visual and textual information.\nThis script provides a powerful tool for automatically analyzing and segmenting video content based on topics, making it useful for applications like video summarization, content organization, and deeper video understanding.\n"
    },
    {
      "filename": "backup.sh",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n\nexport timestampe=$(date +'%Y-%m-%d_%H-%M-%S')\necho $timestampe\n\ndeclare -rx DELETED=\"/mnt/usb/backup/DELETED\"\n\ngum style \\\n\t--foreground 014 --border-foreground 024 --border double \\\n\t--align center --width 50 --margin \"1 2\" --padding \"2 4\" \\\n\t'Hello.' && sleep 1 && clear\n\nif [[ -d $DELETED ]]; then\n\n\tmkdir -pv $DELETED/$timestampe\n\n\trsync -rtPpv --log-file=/tmp/rsync-$timestampe.log --stats --delete-before --delete-excluded --progress --ignore-existing -u -l -b -i -s \\\n\t\t--suffix=\"_backup\" --backup-dir=$DELETED/$timestampe \\\n\t\t--exclude-from=$HOME/.backup_exclude.txt /home/b08x/ /mnt/bender/backup/home/b08x/\n\n\tchown -R b08x:b08x $DELETED/$timestampe\nelse\n\techo \"no usb drive mounted. exiting\"\nfi\n\n# rsync -r -n -t -v --progress --ignore-existing -u -l -i -s --exclude=*.git --exclude=*.ray-snapshots/ --exclude=Recordings/*** /srv/storage/ /mnt/bender/backup/Library/\n"
    },
    {
      "filename": "bat-preview",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n\n# Stolen from fzf.vim preview.sh\n# Currently the neovim terminal resets the COLORTERM environment\n# variable, so all the colours in bat look wrong. I've just copied\n# the fzf.vim preview script and modified it export COLORTERM=truecolor\n# to fix that.\n\nif [ -z \"$1\" ]; then\n  echo \"usage: $0 FILENAME[:LINENO][:IGNORED]\"\n  exit 1\nfi\n\nIFS=':' read -r -a INPUT <<< \"$1\"\nFILE=${INPUT[0]}\nCENTER=${INPUT[1]}\n\nif [[ $1 =~ ^[A-Z]:\\\\ ]]; then\n  FILE=$FILE:${INPUT[1]}\n  CENTER=${INPUT[2]}\nfi\n\nif [[ -n \"$CENTER\" && ! \"$CENTER\" =~ ^[0-9] ]]; then\n  exit 1\nfi\nCENTER=${CENTER/[^0-9]*/}\n\nFILE=\"${FILE/#\\~\\//$HOME/}\"\nif [ ! -r \"$FILE\" ]; then\n  echo \"File not found ${FILE}\"\n  exit 1\nfi\n\nMIME=$(file --dereference --mime \"$FILE\")\nif [[ \"$MIME\" =~ binary ]]; then\n  echo \"$MIME\"\n  exit 0\nfi\n\nif [ -z \"$CENTER\" ]; then\n  CENTER=0\nfi\n\nCOLORTERM=\"truecolor\" bat --style=\"${BAT_STYLE:-plain}\" --color=always --pager=never \\\n      --highlight-line=$CENTER \"$FILE\"\n"
    },
    {
      "filename": "brightness.rb",
      "path": "/bin/",
      "content": "#!/usr/bin/env ruby\n# frozen_string_literal: true\n\n# a tui driven menu for:\n# https://github.com/philippnormann/xrandr-brightness-script\n\nrequire 'tty-prompt'\n\nmodule Brightness\n  module_function\n\n  def prompt\n    prompt = TTY::Prompt.new(interrupt: :exit)\n    return prompt\n  end\n  #TODO: ensure $PATH can be read, so this can just be set to a file name\n  # as when the path changes, this will cease to function.\n  def set(monitor,level)\n    `~/Utils/bin/brightness.sh = #{monitor} #{level}`\n  end\n\n  def reset(monitor)\n    `~/Utils/bin/brightness.sh = #{monitor}`\n  end\n\nend\n\ndef forkoff(command)\n  fork do\n    exec(command)\n  end\nend\n\ndef monitors\n  monitors = `xrandr --listmonitors | awk -F \"  \" '{print $2}' | xargs`.split\n  return monitors\nend\n\naction = Brightness.prompt.select(\"Choose action\") do |menu|\n  menu.choice \"adjust\"\n  menu.choice \"reset\"\nend\n\ncase action\n  when \"reset\"\n    monitors.each { |m| Brightness.reset(m) }\n  when \"adjust\"\n    choices = monitors.push(\"all\")\n\n    choice = Brightness.prompt.multi_select(\"select outputs to adjust\", choices, default: \"all\")\n\n    level = Brightness.prompt.slider(\"brightness\", min: 0, max: 1, step: 0.0125, default: 1.0)\n\n    if choice.include?(\"all\")\n      monitors.each { |m| Brightness.set(m, level) }\n    else\n      choice.each { |s| Brightness.set(s, level) }\n    end\nend\n"
    },
    {
      "filename": "brightness.sh",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\nset -e\n\nif ! command -v bc &> /dev/null\nthen\n    echo \"bc command could not be found, it's needed to run this script.\"\n    exit\nfi\n\nget_display_info() {\n    xrandr --verbose | grep -m 1 -w \"$1 connected\" -A8 | grep \"$2\" | cut -f2- -d: | tr -d ' '\n}\n\n# cribbed from redshift, https://github.com/jonls/redshift/blob/master/README-colorramp\nGAMMA_VALS=('1.0:0.7:0.4'  # 3000K\n            '1.0:0.7:0.5'  # 3500K\n            '1.0:0.8:0.6'  # 4000K\n            '1.0:0.8:0.7'  # 4500K\n            '1.0:0.9:0.8'  # 5000K\n            '1.0:0.9:0.9'  # 6000K\n            '1.0:1.0:1.0'  # 6500K\n            '0.9:0.9:1.0'  # 7000K\n            '0.8:0.9:1.0'  # 8000K\n            '0.8:0.8:1.0'  # 9000K\n            '0.7:0.8:1.0') # 10000K\n\nget_gamma_index() {\n    for i in \"${!GAMMA_VALS[@]}\"; do\n        [[ \"${GAMMA_VALS[$i]}\" = \"$1\" ]] && echo \"$i\" && break\n    done\n}\n\nget_temp_for_gamma() {\n    idx=$(get_gamma_index \"$1\")\n    awk '{printf \"%.1f\", $1 / 10}' <<< \"$idx\"\n}\n\nget_gamma_for_temp() {\n    idx=$(awk '{printf \"%d\", $1 * 10}' <<< \"$1\")\n    echo \"${GAMMA_VALS[$idx]}\"\n}\n\n# gamma values returned by xrandr --verbose are somehow inverted\n# https://gitlab.freedesktop.org/xorg/app/xrandr/issues/33\n# this function corrects this bug by reverting the calculation\ninvert_gamma() {\n    inv_r=$(cut -d: -f1 <<< \"$1\")\n    inv_g=$(cut -d: -f2 <<< \"$1\")\n    inv_b=$(cut -d: -f3 <<< \"$1\")\n    r=$(awk '{printf \"%.1f\", 1/$1}' <<< \"$inv_r\" 2>/dev/null)\n    g=$(awk '{printf \"%.1f\", 1/$1}' <<< \"$inv_g\" 2>/dev/null)\n    b=$(awk '{printf \"%.1f\", 1/$1}' <<< \"$inv_b\" 2>/dev/null)\n    echo \"$r:$g:$b\"\n}\n\nget_gamma() {\n    invert_gamma \"$(get_display_info \"$1\" 'Gamma: ')\"\n}\n\nget_brightness() {\n    get_display_info \"$1\" 'Brightness: '\n}\n\nlist_displays() {\n    echo 'displays:'\n    displist=''\n    connected=$(xrandr | grep -w connected | cut -f1 -d' ')\n    for display in $connected; do\n        brightness=$(get_brightness \"$display\")\n        gamma=$(get_gamma \"$display\")\n        temp=$(get_temp_for_gamma \"$gamma\")\n        displist+=\"$display brightness: $brightness gamma: $gamma temp: $temp\"\n        displist+=$'\\n'\n    done\n    echo \"$displist\" | column -t | sed 's/^/  /'\n}\n\ndisplay_usage() {\n    script_name=$(basename \"$0\")\n    echo \"Usage: $script_name op display [stepsize|value] [--temp]\"\n    echo\n    echo 'arguments:'\n    echo '  op:             '-' to decrease or '+' to increase brightness'\n    echo '                  '=' to set brightness to a specific value'\n    echo '  display:        name of a connected display to adjust'\n    echo '  stepsize:       size of adjustment step (default 0.1)'\n    echo '  value:          value to set (default 1.0 for brightness, 0.6 for color temperature)'\n    echo '  --temp:         adjusts color temperature instead of brightness'\n    echo\n    list_displays \n}\n\nexec_op() {\n    if [ \"$1\" = '+' ]; then\n        NEWVAL=$(echo \"$3 + $2\" | bc)\n    elif [ \"$1\" = '-' ]; then\n        NEWVAL=$(echo \"$3 - $2\" | bc)\n    elif [ \"$1\" = '=' ]; then\n        NEWVAL=$2\n    fi\n    if [ \"$(echo \"$NEWVAL < 0.0\" | bc)\" -eq 1 ]; then\n        NEWVAL='0.0'\n    fi\n    if [ \"$(echo \"$NEWVAL > 1.0\" | bc)\" -eq 1 ]; then\n        NEWVAL='1.0'\n    fi\n    echo \"$NEWVAL\"\n}\n\nif [[ \"$1\" = '+'  ||  \"$1\" = '-' || \"$1\" = '=' ]] && [[ -n \"$2\" ]]; then\n    OP=$1; DISP=$2; shift; shift\nelse\n    display_usage; exit 1\nfi\n\nif [[ \"$1\" =~ ^[0-9]+(.[0-9]+)?$ ]]; then\n    OPVAL=$1; shift\nelse\n    if [[ \"$OP\" = '=' ]]; then\n        if [[ \"$1\" = '--temp' ]]; then\n            OPVAL='0.6'\n        else\n            OPVAL='1.0'\n        fi\n    else\n        OPVAL='0.1'\n    fi\nfi\n\nCURRBRIGHT=$(get_brightness \"$DISP\")\nif [[ ! \"$CURRBRIGHT\" =~ ^[0-9]+.[0-9]+$ ]]; then\n    echo \"Error: Selected display $DISP has no brightness value!\"\n    echo; list_displays; exit 1\nfi\n\nCURRGAMMA=$(get_gamma \"$DISP\")\nif [[ ! \"$CURRGAMMA\" =~ ^[0-9].[0-9]:[0-9].[0-9]:[0-9].[0-9]$ ]]; then\n    echo \"Error: Selected display $DISP has no gamma value!\"\n    echo; list_displays; exit 1\nfi\n\nNEWBRIGHT=\"$CURRBRIGHT\"\nNEWGAMMA=\"$CURRGAMMA\"\n\nif [ \"$1\" = '--temp' ]; then\n    CURRTEMP=$(get_temp_for_gamma \"$CURRGAMMA\")\n    NEWTEMP=$(exec_op \"$OP\" \"$OPVAL\" \"$CURRTEMP\")\n    NEWGAMMA=$(get_gamma_for_temp \"$NEWTEMP\")\nelse\n    NEWBRIGHT=$(exec_op \"$OP\" \"$OPVAL\" \"$CURRBRIGHT\")\nfi\n\nxrandr --output \"$DISP\" --brightness \"$NEWBRIGHT\" --gamma \"$NEWGAMMA\"\n"
    },
    {
      "filename": "cleanup.sh",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n\nif [[ ! \"${EUID}\" -eq 0 ]]; then\n  echo \"please run as root. exiting\"\n  exit\nfi\n\ndeclare -rx timestampe=$(date +%Y%m%d%H%M)\n\nif [ \"${1-0}\" = \"0\" ]; then\n  days=\"0days\"\nelse\n  days=\"${1}days\"\nfi\n\nif [ $(command -v fd) ];then\n\n  if [ -d /tmp/removed ]; then\n    mv /tmp/removed \"/tmp/removed_${timestampe}\"\n  fi\n\n  mkdir -pv /tmp/removed\n\n  fd --change-older-than ${days} -u --show-errors -E \"proc/*\" -E \"sys/*\" -E \"*cache*\" -E \"lost+found/\" -E \"tmp/*\" -E \"*.gnupg\" -E \"pubring\" -E \"run/*\" -E \"mnt/*\" -H \"@\\d+:\\d+:\\d+~$\"  --full-path \"/\" -X mv -v '{}' /tmp/removed/\n\n  echo \"the ansible backup files moved to /tmp/removed and will be removed upon next reboot\"\n\nelse\n  echo \"fd is not installed. \"\nfi\n"
    },
    {
      "filename": "convert2ogg.sh",
      "path": "/bin/",
      "content": "#!/bin/bash\n\n# Get the input file name\nINPUT_FILE=$1\n\n# Get the file extension of the input file\nEXTENSION=${INPUT_FILE##*.}\n\n# Normalize the audio using ffmpeg-normalize\nffmpeg-normalize \"$INPUT_FILE\" -o \"${INPUT_FILE%.*}-normalized.${EXTENSION}\" \n\n# Convert the normalized file to OGG\nffmpeg -i \"${INPUT_FILE%.*}-normalized.${EXTENSION}\" -c:a libvorbis -b:a 192k -ar 44100 \"${INPUT_FILE%.*}.ogg\""
    },
    {
      "filename": "deploy_utils.sh",
      "path": "/bin/",
      "content": "#!/bin/sh\n\ndeclare ANSIBLE_HOME=\"$HOME/Workspace/syncopatedOS/cac\"\n\nansible-playbook -i \"${ANSIBLE_HOME}/inventory.ini\" \"${ANSIBLE_HOME}/playbooks/utils.yml\" \\\n--limit tinybot,soundbot,ninjabot\n\n# aplaybook -vv -i inventory.ini playbooks/utils.yml --limit tinybot,soundbot,ninjabot -e \"branch=development\"\n"
    },
    {
      "filename": "disconnect_pulse.sh",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n\ndisconnect_pulse() {\n  jack_disconnect \"PulseAudio JACK Sink:front-left\" system:playback_1\n  jack_disconnect \"PulseAudio JACK Sink:front-right\" system:playback_2\n  jack_disconnect \"PulseAudio JACK Source:front-left\" system:capture_1\n  jack_disconnect \"PulseAudio JACK Source:front-right\" system:capture_2\n\n  return 0\n}\n\ndisconnect_pulse\n"
    },
    {
      "filename": "docker-backup.sh",
      "path": "/bin/",
      "content": "#!/bin/bash\n\n# ------------------------------------------------------------------------------\n# Script Name: docker-volume-backup.sh\n# Description: This script provides an interactive way to back up and restore\n#              Docker volumes using the 'gum' tool for enhanced UI.\n#\n# Requirements:\n#   - gum: Install from https://github.com/charmbracelet/gum\n#   - fd:  Install from https://github.com/sharkdp/fd\n#   - sd:  Install from https://github.com/chmln/sd\n#   - docker:  Docker must be installed and running\n#\n# Usage:\n#   - Backup all volumes:\n#       ./docker-volume-backup.sh backup\n#\n#   - Backup specific volumes:\n#       ./docker-volume-backup.sh backup volume1 volume2 ...\n#\n#   - Restore volumes (interactive selection from backups):\n#       ./docker-volume-backup.sh restore\n# ------------------------------------------------------------------------------\n\n# --- Configuration ------------------------------------------------------------\n\n# Backup directory (change if needed)\nbackup_dir=\"$HOME/LLMOS/BACKUP/docker-volumes\"\n\n# --- Functions ---------------------------------------------------------------\n\n# Function to backup Docker volumes\n# Arguments: (optional) List of volumes to back up. If none are provided,\n#            all volumes on the system will be backed up.\nfunction backup_volumes {\n  local volumes_to_backup\n  local backup_name\n\n  # Get a list of all Docker volumes\n  all_volumes=($(docker volume ls -q))\n\n  # Use gum choose for interactive volume selection (if no arguments)\n  if [[ $# -eq 0 ]]; then\n    volumes_to_backup=$(gum choose \"${all_volumes[@]}\" --no-limit)\n  else\n    volumes_to_backup=(\"$@\")\n  fi\n\n  # Backup each selected volume\n  for volume in $volumes_to_backup; do\n    backup_name=\"${volume}_backup_$(date +%Y-%m-%d_%H-%M-%S)\"\n    local backup_path=\"$backup_dir/$backup_name\"\n\n    echo \"Backing up volume: $volume to $backup_path\"\n    mkdir -p \"$backup_path\"\n\n    # Check if network is available (to pull alpine if needed)\n    if ! ping -c 1 -W 1 google.com &>/dev/null; then\n      echo \"Error: Network connection unavailable. Cannot pull alpine image.\"\n      exit 1  # Exit the script if network is not available\n    fi\n\n    docker run --rm -v \"$volume\":/volume -v \"$backup_path\":/backup alpine tar cvf \"/backup/${volume}.tar\" -C / volume\n  done\n  echo \"Backup complete. Backups stored in: $backup_path\"\n}\n\n# Function to restore Docker volumes\nfunction restore_volumes {\n  local backup_name\n  local volumes_to_restore\n\n  # Use gum input for backup name\n  backup_name=$(gum input --placeholder \"Enter the name of the backup to restore from\")\n\n  local backup_path=\"$backup_dir/$backup_name\"\n  if [[ ! -d \"$backup_path\" ]]; then\n    echo \"Error: Backup directory not found: $backup_path\"\n    exit 1\n  fi\n\n  # Get available volumes in the backup directory using fd and sd\n  available_volumes=$(fd -t f -e .tar \"$backup_path\" | sd '.tar$' '')\n\n  # Use gum filter for interactive volume selection from available backups\n  volumes_to_restore=$(echo \"$available_volumes\" | gum filter --placeholder \"Filter volumes to restore\")\n\n  if [[ -z \"$volumes_to_restore\" ]]; then\n    echo \"No volumes selected for restoration.\"\n    exit 0\n  fi\n\n  # Restore each selected volume\n  for vol_to_restore in $volumes_to_restore; do\n    backup_file=\"$backup_path/${vol_to_restore}.tar\"\n    if [[ -f \"$backup_file\" ]]; then\n      echo \"Restoring volume: $vol_to_restore from $backup_file\"\n      docker run --rm -v \"$vol_to_restore\":/volume -v \"$backup_path\":/backup alpine tar xvf \"/backup/${vol_to_restore}.tar\" -C /volume\n    else\n      echo \"Error: Backup file not found: $backup_file\"\n    fi\n  done\n}\n\n# --- Main Script Execution ----------------------------------------------------\n\n# Handle script termination using trap for SIGINT (Ctrl+C) and SIGTSTP (Ctrl+Z)\ntrap handle_exit SIGINT SIGTSTP\n\n# Clear the screen\ntput clear && tput cup 15 0\n\n# Check for command line arguments\nif [[ $# -eq 0 ]]; then\n\n  choice=$(gum choose --cursor-prefix \">\" --selected-prefix \">\"  \"Backup volumes\" \"Restore volumes\")\n\n  case \"$choice\" in\n    \"Backup volumes\")\n      backup_volumes\n      ;;\n    \"Restore volumes\")\n      restore_volumes\n      ;;\n    *)\n      echo \"Invalid choice. Exiting.\"\n      exit 1\n      ;;\n  esac\nelse\n  # Arguments provided: Non-interactive mode\n  action=\"$1\"\n  shift # Remove action from arguments\n\n  case \"$action\" in\n    backup)\n      backup_volumes \"$@\"\n      ;;\n    restore)\n      restore_volumes\n      ;;\n    *)\n      echo \"Invalid action: $action. Choose either 'backup' or 'restore'.\"\n      exit 1\n      ;;\n  esac\nfi\n"
    },
    {
      "filename": "dots.sh",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n\ndots() {\n\tfolders=(\".config/yadm/alt/\" \"Workspace/syncopatedIaC/\" \"Utils\")\n\n\texport EDITOR=$(gum choose nvim micro code gedit)\n\n\tfolder=$(gum choose \"dots\" \"${folders[@]}\")\n\n\tif [[ $folder == \"dots\" ]]; then\n\t  $EDITOR $(yadm list -a |fzf --sort --preview='bat {}')\n\telse\n\t  if [[ $EDITOR == *\"gedit\"* ]]; then\n\t  \tfile=$(gum file -a $HOME/$folder)\n\t  \t$EDITOR -s ${file} &>/dev/null &\n\t  else\n\t  \t$EDITOR $(gum file -a $HOME/$folder)\n\t  fi\n\tfi\n}\n\ncd $HOME && dots\n"
    },
    {
      "filename": "dsnote.sh",
      "path": "/bin/",
      "content": "#!/bin/bash\n\nset -e\n\n\nhandle_exit() {\n  dsnote --action stop-listening\n}\n\n\ndsnote --action start-listening-active-window\n\n\n# Handle script termination using trap for SIGINT (Ctrl+C) and SIGTSTP (Ctrl+Z)\ntrap handle_exit SIGINT SIGTSTP\n"
    },
    {
      "filename": "guake-utils",
      "path": "/bin/",
      "content": "#!/bin/bash\n\necho\n\nif [ \"$1\" == \"restore\" ]; then\n\techo \"Restoring Guake...\"\n\tPARENT=$(xwininfo -name \"Guake!\" -int -tree | sed -ne '/Root/s/[^0-9]//gp')\n\txdotool search --name \"Guake!\" windowreparent $PARENT\n\techo \"Done!\"\n\nelif [ \"$1\" == \"import\" ]; then\n\techo \"Importing preferences from ~/.guake\"\n\tguake --restore-preferences ~/.guake/guake-settings\n\techo \"Done!\"\n\nelif [ \"$1\" == \"export\" ]; then\n\techo \"Exporting preferences to ~/.guake\"\n\tguake --save-preferences ~/.guake/guake-settings\n\techo \"Done!\"\n\nelse\n\techo \"Error: Command not recognised\"\n\techo\n\techo \"- restore - Restores Guake when minimised (Issue #45)\"\n\techo \"- import - Imports Guake settings from ~/.guake\"\n\techo \"- export - Exports Guake settings to ~/.guake\"\nfi\n\necho\n"
    },
    {
      "filename": "jack-load.sh",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n#set -vx\n\ndeclare OUTPUTS=\"/tmp/jack_load/outputs\"\ndeclare INPUTS=\"/tmp/jack_load/inputs\"\n\nif [[ ! -d /tmp/jack_load ]]; then\n  mkdir -pv /tmp/jack_load\nfi\n\nfunction loadInput {\n\n local name=$1\n local device=$2\n\n jack_load $name zalsa_in -i \"-d hw:$device,0\"\n\n}\n\nfunction loadOutput {\n\n local name=$1\n local device=$2\n\n jack_load $name zalsa_out -i \"-d hw:$device\"\n\n}\n\necho \"Select action for jack_load \"\nchoice=$(gum choose input output unload)\n\ncase $choice in\n  input)\n    arecord -l | grep card >> $INPUTS\n    echo \"select input device\"\n    device=$(cat $INPUTS | gum choose | awk '{print $2}' | sed 's/://g')\n    echo \"give this device an alias(no spaces)\"\n    name=$(gum input|sed 's/ /_/g')\n    echo \"${name}\" >> $INPUTS\n    loadInput $name $device\n    ;;\n  output)\n    aplay -l | grep card >> $OUTPUTS\n    echo \"select output device\"\n    device=$(cat $OUTPUTS | gum choose | awk '{print $2}' | sed 's/://g')\n    echo \"give this device an alieas(no spaces)\"\n    name=$(gum input|sed 's/ /_/g')\n    echo \"${name}\" >> $OUTPUTS\n    loadOutput $name $device\n    ;;\n  unload)\n    echo \"unload which device?\"\n    name=$(cat $OUTPUTS $INPUTS|gum choose)\n    #TODO: improve this logic, check against jack_lsp\n    # name=$(gum input)\n    jack_unload $name\n    ;;\n\nesac\n"
    },
    {
      "filename": "mic_mute.sh",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n#\n\nconnectmixer(){\n  jack_connect \\\n  \"a2j:Midi Through [14] (capture): Midi Through Port-0\" \"jack_mixer_2:midi in\"\n\n  return 0\n}\n\nmute(){\n  sendmidi dev 'Midi Through Port-0' cc 32 127\n}\n\nunmute(){\n  sendmidi dev 'Midi Through Port-0' cc 32 1\n}\n\nif ! [[ $(jack_lsp | grep jack_mixer |grep midi) ]]; then\n  echo \"jack mixer is not running\"\n  exit\nfi\n\n#if ! [[ $(jack_lsp -c | grep \"  jack_mixer_2:midi in\" -B1) ]];then\n  connectmixer\n#fi\n\naction=$1\n\nif [[ $action == 'mute' ]];then\n  mute\nelif [[ $action == 'unmute' ]];then\n  unmute\nfi\n"
    },
    {
      "filename": "open",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n\nfile=\"$@\"\nmime_type=`file \"$file\" --dereference --mime-type | sed -n 's/^.*: \\(.*\\)$/\\1/p'`\n\n\ncase $mime_type in\n  image/*)\n\n    open-image \"$file\"\n\n    ;;\n\n  video/*)\n\n    mpv \"$file\"\n\n    ;;\n\n  inode/directory)\n    (\n      set -e\n      cd $file\n      open \"$(fzf)\"\n    )\n\n    ;;\n\n  *)\n\n    echo $mime_type\n\n    bat \"$file\"\n\n    ;;\n\nesac\n\n\n\n# if [ ! -z \"$WAYLAND_DISPLAY\" ]; then\n#   imv -s full $FILE -w __floating\n# else\n#   feh $FILE --class __floating\n# fi\n"
    },
    {
      "filename": "open-image",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n\nset -e\n\nif [ ! -z \"$WAYLAND_DISPLAY\" ]; then\n  imv -s full \"$@\" -w __floating\nelse\n  feh \"$@\" --class __floating\nfi\n"
    },
    {
      "filename": "open-link",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n\nset -x\n\nWORKSPACE_FILTER=$(hyprctl monitors -j | jq -r '. | map(.activeWorkspace.id) | map(\".workspace.id == \\(.)\") | join(\" or \")')\n# Get a Firefox that is on currently active workspaces\nFIREFOX_TO_OPEN=$(hyprctl clients -j | jq -r \".[] | select(.class == \\\"firefox\\\" or .class == \\\"firefox-work\\\") | select($WORKSPACE_FILTER) | .class\" | head -n 1)\n\nif [[ -z \"$FIREFOX_TO_OPEN\" ]]; then\n  # Get the last active one then!\n  FIREFOX_TO_OPEN=$(hyprctl clients -j | jq -s -r \".[] | sort_by(.focusHistoryID) | .[] | select(.class == \\\"firefox\\\" or .class == \\\"firefox-work\\\") | .class\" | head -n 1)\nfi\n\nif [ \"$FIREFOX_TO_OPEN\" = \"firefox-work\" ]; then\n  firefox-work --url $1\nelse\n  firefox --url $1\nfi\n"
    },
    {
      "filename": "open-link-popup",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n\nfirefox --new-instance --no-remote --name __float_front --new-window $1\n"
    },
    {
      "filename": "prompt_library_folders.rb",
      "path": "/bin/",
      "content": "#!/usr/bin/env ruby\n# frozen_string_literal: true\n\nrequire 'fileutils'\n\ndef create_folder_structure(base_path)\n  structure = {\n    'Text_Generation' => ['General_Expansion', 'Topic_Specific', 'Creative_Writing'],\n    'Formatting' => ['Punctuation_Correction', 'Structure_Modification', 'Style_Adjustment'],\n    'Image_Generation' => ['Descriptive', 'Style-based', 'Scene_Composition'],\n    'System_Instructions' => ['DevOps_Engineer', 'Data_Scientist', 'Creative_Writer', 'Customer_Support', 'Other_Specialized_Roles'],\n    'Text_Merging' => ['Information_Synthesis', 'Comparative_Analysis', 'Multi-source_Integration'],\n    'Templates' => ['Titles', 'Tags', 'Headings', 'Product_Descriptions'],\n    'Perspective_Shifting' => ['Alternative_Viewpoints', 'Contrarian_Angles', 'Empathy_Building'],\n    'Meta' => ['Best_Practices', 'Prompt_Engineering_Tips', 'Version_History'],\n    'Custom' => []\n  }\n\n  structure.each do |folder, subfolders|\n    folder_path = File.join(base_path, folder)\n    FileUtils.mkdir_p(folder_path)\n    puts \"Created folder: #{folder_path}\"\n\n    subfolders.each do |subfolder|\n      subfolder_path = File.join(folder_path, subfolder)\n      FileUtils.mkdir_p(subfolder_path)\n      puts \"Created subfolder: #{subfolder_path}\"\n    end\n  end\nend\n\ndef get_base_directory\n  if ARGV.empty?\n    print \"Enter the base directory for the Prompt Library: \"\n    gets.chomp.strip\n  else\n    ARGV[0]\n  end\nend\n\ndef process_directory(base_directory)\n  # Remove surrounding quotes if present\n  base_directory = base_directory.gsub(/\\A[\"']|[\"']\\Z/, '')\n\n  # Expand the path to handle '~' for home directory\n  base_directory = File.expand_path(base_directory)\n\n  # Check if the directory exists\n  unless Dir.exist?(base_directory)\n    puts \"The specified directory does not exist. Would you like to create it? (y/n)\"\n    response = gets.chomp.downcase\n    if response == 'y'\n      FileUtils.mkdir_p(base_directory)\n      puts \"Created directory: #{base_directory}\"\n    else\n      puts \"Exiting without creating the folder structure.\"\n      exit\n    end\n  end\n\n  base_directory\nend\n\n# Get and process the base directory\nbase_directory = process_directory(get_base_directory)\n\n# Create the folder structure\ncreate_folder_structure(base_directory)\n\nputs \"Folder structure created successfully!\"\n"
    },
    {
      "filename": "remote_docker_compose_manager.sh",
      "path": "/bin/",
      "content": "#!/bin/bash\n\n# ------------------------------------------------------------------------------\n# Script Name: remote_docker_compose_manager.sh\n# Description: This script provides an interactive way to manage Docker Compose\n#              services (Up/Down/Stop) on a remote server using 'gum' for enhanced UI.\n#\n# Requirements:\n#   - gum: Install from https://github.com/charmbracelet/gum\n#   - ssh:  Must be configured for passwordless access to the remote server\n#   - docker-compose: Must be installed on the remote server\n#\n# Usage:\n#   - Run the script:\n#       ./remote_docker_compose_manager.sh\n#\n#   - The script will prompt you to choose between:\n#       - Up docker-compose services\n#       - Down docker-compose services\n#       - Stop docker-compose services\n#\n#   - The script will then manage the services in each of the specified compose\n#     files.\n# ------------------------------------------------------------------------------\n\n# --- Configuration ------------------------------------------------------------\n\n# Remote host details\nremote_host=\"b08x@ninjabot.syncopated.net\"\nLLMOS=\"$HOME/LLMOS\"\n\n# Array of compose files to manage\ncompose_files=(\n  \"$LLMOS/flowise/docker/docker-compose.yml\"\n  \"$LLMOS/dify/docker/docker-compose.yaml\"\n  \"$LLMOS/SillyTavern/docker/docker-compose-with-extras.yml\"\n)\n\n# --- Functions ---------------------------------------------------------------\n\n# Function to execute a command remotely via SSH\n# Arguments:\n#   - command: The command to execute on the remote server\n#   - compose_file:  The path to the Docker Compose file\nfunction run_remote_command {\n  local command=\"$1\"\n  local compose_file=\"$2\"\n  ssh \"$remote_host\" \"cd \\\"$(dirname '$compose_file')\\\" && docker compose -f \\\"$compose_file\\\" $command\"\n}\n\n# --- Script Logic ------------------------------------------------------------\n# Handle script termination using trap for SIGINT (Ctrl+C) and SIGTSTP (Ctrl+Z)\ntrap handle_exit SIGINT SIGTSTP\n\n# Clear the screen\ntput clear && tput cup 15 0\n\n# Use gum choose for action selection\naction=$(gum choose --cursor-prefix \">\" --selected-prefix \">\"  \"Up docker-compose services\" \"Down docker-compose services\" \"Restart docker-compose services\" \"Stop docker-compose services\")\n\n# Loop through each compose file\nfor compose_file in \"${compose_files[@]}\"; do\n  echo \"Managing docker-compose services in file: $compose_file\"\n\n  # Determine the command based on the user's choice\n  case \"$action\" in\n    \"Up docker-compose services\")\n      command=\"up -d\"\n      echo \"Bringing docker-compose services up...\"\n      ;;\n    \"Down docker-compose services\")\n      command=\"down\"\n      echo \"Bringing docker-compose services down...\"\n      ;;\n    \"Restart docker-compose services\")\n      command=\"restart\"\n      echo \"Restarting docker-compose services...\"\n      ;;\n    \"Stop docker-compose services\")\n      command=\"stop\"\n      echo \"Stopping docker-compose services...\"\n      ;;\n    *)\n      echo \"Invalid action. Exiting.\"\n      exit 1\n      ;;\n  esac\n\n  # Execute the command remotely\n  run_remote_command \"$command\" \"$compose_file\"\n\n  # Check for errors and provide more specific information\n  if [[ $? -ne 0 ]]; then\n    echo \"Error managing docker-compose services in file: $compose_file\"\n    echo \"Check the remote server logs for more details.\"\n    # Optionally, capture the error output from the remote server\n    # error_output=$(ssh \"$remote_host\" \"cd $(dirname '$compose_file') && docker compose -f '$compose_file' $command 2>&1\")\n    # echo \"Error output: $error_output\"\n  fi\n\n    # Add a 20-second sleep with gum spin animation\n    gum spin --title \"Waiting 20 Seconds Before Processing next compose file...\" -- sleep 20\n\ndone\n\necho \"Docker Compose service management completed.\"\n"
    },
    {
      "filename": "ripfzf-vscode.sh",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n# ~/scripts/ripfzfsubl\n#\n# Uses Sublime Text (subl) as the text editor\n#\n# brew install bat ripgrep fzf\n#\n# bat  Clone of cat(1) with syntax highlighting and Git integration\n#      |__ https://github.com/sharkdp/bat\n#\n# ripgrep  Search tool like grep and The Silver Searcher\n#          |__ https://github.com/BurntSushi/ripgrep\n#\n# fzf  Command-line fuzzy finder written in Go\n#      |__ https://github.com/junegunn/fzf\n#\n#\n# 1. Search for text in files using Ripgrep\n# 2. Interactively narrow down the list using fzf\n# 3. Open the file in Sublime Text Editor\n#\n# source: https://github.com/MadBomber/scripts\n\nrga --rga-accurate --rga-adapters='poppler,pandoc,zip,decompress,tar,sqlite,ffmpeg' -j4 --color=always --line-number --no-heading --smart-case \"${*:-}\" |\n\tfzf -e --ansi \\\n\t\t--color \"hl:-1:underline,hl+:-1:underline:reverse\" \\\n\t\t--delimiter : \\\n\t\t--preview 'bat --color=always {1} --highlight-line {2}' \\\n\t\t--preview-window '60%:wrap,border-bottom,+{2}+3/3,~3' \\\n\t\t--bind 'enter:become(code -g {1}:{2})'\n"
    },
    {
      "filename": "screenshot",
      "path": "/bin/",
      "content": "#!/bin/bash\n\nset -e\n\nINPUT=${@:-\"$HOME/screenshots/$(date +%s).png\"}\nFILE=\"${INPUT%.png}.png\"\n\nif [ ! -z \"$WAYLAND_DISPLAY\" ]; then\n  grim -g \"$(slurp)\" $FILE\n  wl-copy < $FILE\nelse\n  maim -s $FILE\nfi\n\nnotify-send \"Saved screenshot to \\\"$FILE\\\"\" -t 5000\n\necho $FILE\n"
    },
    {
      "filename": "screenshot-and-open",
      "path": "/bin/",
      "content": "#!/bin/bash\n\nset -e\n\nfile=\"$(screenshot \"$@\")\"\nopen-image \"$file\"\n"
    },
    {
      "filename": "search_web.sh",
      "path": "/bin/",
      "content": "#!/bin/bash\n# A quick documentation finder based on rofi and devdocs\n# Requires: rofi, devdocs, i3-sensible-terminal, qutebrowser nerdfonts\nfiles=~/.cache/rofi-search_term_list\n\nBROWSER=\"google-chrome-stable\"\nGOOGLE_SEARCH_URL=\"https://google.com/search?q\"\nBRAVE_SEARCH_URL=\"https://search.brave.com/search?q\"\n\nappend_new_term() {\n\t# Delete term. Append on the first line.\n\tsed -i \"/$input/d\" $files\n\tsed -i \"1i $input\" \"$files\"\n\t# Max cache limited to 20 entries: https://github.com/Zeioth/rofi-devdocs/issues/3\n\tsed -i 20d \"$files\"\n}\n\nif [ -e $files ]; then\n\t# If file list exist, use it\n\t#input=$(cat $files | rofi -dmenu -p \"manual\")\n\tinput=$(xsel -ob)\n\nelse\n\t# There is no file list, create it and show menu only after that\n\ttouch $files\n\t#input=$(cat $files | rofi -dmenu -p \"manual\")\n\tinput=$(xsel -ob)\n\n\t#\tThe file if empty, initialize it, so we can insert on the top later\n  if [ ! -s \"$_file\" ]\n  then\n    echo \" \" > \"$files\"\n  fi\nfi\n\ncase \"$(echo $input | cut -d \" \" -f 1)\" in\n\n\tdd)\n\t\t# Search dev docs\n\t\tappend_new_term\n\t\tquery=$(echo \"$input\" | cut -c 3- | xargs -0)\n\t  exec devdocs-desktop \"$(echo $query)\" &> /dev/null &\n\t  ;;\n\tw)\n\t\t# Search dictionary\n\t\tappend_new_term\n\t\tquery=$(echo \"$input\" | cut -c 2- | xargs -0)\n\t\techo $query\n\t\tif ! [ -z $query ]\n\t\tthen\n\t\t\texec $BROWSER \"$GOOGLE_SEARCH_URL=Define+$query\" &> /dev/null &\n\t\tfi\n\t\t;;\n\t*)\n\t  # Search the web\n\t\tappend_new_term\n\t  query=$(echo \"$input\" | cut -c 1- | xargs -0)\n\t\tif ! [[ -z $query ]]\n\t\tthen\n\t\t\texec $BROWSER \"$BRAVE_SEARCH_URL=$query&tf=py\" &> /dev/null &\n\t\t\tsleep 0.25\n\t\t\texec $BROWSER \"$GOOGLE_SEARCH_URL=site+gist.github.com+$query&source=lnt&tbs=qdr:y\" &> /dev/null &\n\t\t\tsleep 0.25\n\t\t\texec $BROWSER \"$GOOGLE_SEARCH_URL=$query&source=lnt&tbs=qdr:y\" &> /dev/null &\n\t\tfi\n\t  ;;\n\nesac\n"
    },
    {
      "filename": "set-govna.sh",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n\nset_govna () {\n  local governor=$1\n  # sudo cpupower frequency-set -r -g \"$1\"\n  echo \"${governor}\" | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n}\n\n\nif [ \"${1-nothing}\" = \"nothing\" ]; then\n  action=$(whiptail --title \"Menu example\" --menu \"Choose an option\" 15 42 5 \\\n  \"performance\" \"Select performance profile\" \\\n  \"powersave\" \"Select powersave profile\" 3>&1 1>&2 2>&3)\nelse\n  set_govna $1\nfi\n\nsleep 0.5\n\nexit\n"
    },
    {
      "filename": "ssr-hidden.sh",
      "path": "/bin/",
      "content": "#!/bin/bash\n\nset -e\n\nsimplescreenrecorder --start-hidden > /dev/null 2>&1 &!\n"
    },
    {
      "filename": "temp_transcriptions.txt",
      "path": "/bin/",
      "content": "\n\nand\n\nso right i don't really think it's worth continuing\n"
    },
    {
      "filename": "timer.sh",
      "path": "/bin/",
      "content": "#!/usr/bin/env\n#\n#\ntermdown -b -c 5 10s --exec-cmd \"if [ '{0}' > '5' ];\nthen notify-send -t 1000 -u critical '{1}'; fi\"\n"
    },
    {
      "filename": "yt-dlp-transcript.sh",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n\nfunction mo_ytdlp_transcript_clean(){\n    local url=$1\n    yt-dlp --skip-download --write-subs --write-auto-subs --sub-lang en --sub-format ttml --convert-subs srt --output \"transcript.%(ext)s\" ${url};\n    cat ./transcript.en.srt | sed '/^$/d' | grep -v '^[0-9]*$' | grep -v '\\-->' | sed 's/<[^>]*>//g' | tr '\\n' ' ' > output.txt;\n}\n\nmo_ytdlp_transcript_clean $1"
    },
    {
      "filename": "code-packager",
      "path": "/bin/",
      "content": "#!/bin/bash\n\n# Version information\nVERSION=\"0.2.2\"\n\n# Default values\nINCLUDE_EXT=()  # Array to store include extensions\nEXCLUDE_EXT=()  # Array to store exclude extensions\nMAX_SIZE=10240   # 10MB in KB\nRESPECT_GITIGNORE=1  # Enable respecting files as per .gitignore by default\nINCLUDE_DOT_FILES=0  # Exclude dot files and folders by default\nDIRECTORY_PATH=\"\"   # Initialize directory path as empty\nOUTPUT_FILE=\"\"    # Initialize output file path as empty\nZIP_OUTPUT=0     # Disable zipping the output file by default\nMAX_DEPTH=\"\"     # No limit on depth by default\n\n# Function to display help\nshow_help() {\n    echo \"Usage: $0 -t <directory_path> -o <output_file> [options]\"\n    echo \"\"\n    echo \"Options:\"\n    echo \"\"\n    echo \"  -t <directory_path>     Target directory to process.\"\n    echo \"  -o <output_file>        Output file path.\"\n    echo \"  -i <include_extension>  Include files with the specified extension (with or without the leading dot).\"\n    echo \"  -e <exclude_extension>  Exclude files with the specified extension (with or without the leading dot).\"\n    echo \"  -s <max_size_in_kb>     Include files up to the specified size in kilobytes.\"\n    echo \"  -g <respect_gitignore>  0 to disable, 1 to enable respecting files as per .gitignore (default: enabled).\"\n    echo \"  -d <include_dot_files>  0 to exclude, 1 to include dot files and folders (default: excluded).\"\n    echo \"  -z <zip_output>         0 to disable, 1 to enable zipping the output file (default: disabled).\"\n    echo \"  -m <max_depth>          Limit the maximum depth of the search (default: unlimited).\"\n    echo \"  -v, --version           Display version information and exit.\"\n    echo \"  -h, --help              Display this help and exit.\"\n    echo \"\"\n    echo \"Example:\"\n    echo \"\"\n    echo \"  $0 -t ~/project -o output.json -i .txt -i .md -s 500 -g 0 -d 1 -z 1 -m 2\"\n    echo \"\"\n    echo \"  This command will search in '~/project' including only '.txt' and '.md' files,\"\n    echo \"  considering files up to 500KB, not respecting files listed in .gitignore, including dot files,\"\n    echo \"  zipping the output file named 'output.json', and limiting the search depth to 2 levels.\"\n    echo \"\"\n    echo \"Note: Information of binary files is included in the JSON output, but their contents are not stored.\"\n}\n\n# Function to display version\nshow_version() {\n    echo \"Code Packager for Language Models - Version $VERSION\"\n}\n\n# Function to check if a file is binary\nis_binary() {\n    local file=\"$1\"\n    if [[ $(file --mime \"$file\" | grep -o 'charset=binary') ]]; then\n        return 0 # It's a binary file\n    else\n        return 1 # It's not a binary file\n    fi\n}\n\n# Check for required dependencies\ncheck_dependencies() {\n    local dependencies=(\"jq\" \"git\" \"file\")\n    local missing_deps=0\n    for dep in \"${dependencies[@]}\"; do\n        if ! command -v \"$dep\" &> /dev/null; then\n            echo \"Error: Required dependency '$dep' is not installed.\"\n            missing_deps=1\n        fi\n    done\n    if [ \"$missing_deps\" -ne 0 ]; then\n        echo \"Please install the missing dependencies and try again.\"\n        exit 1\n    fi\n}\n\n# Parse command line arguments\nwhile getopts \"t:o:i:e:s:g:d:z:m:vh-\" opt; do\n    case $opt in\n        t) DIRECTORY_PATH=\"${OPTARG}\" ;;\n        o) OUTPUT_FILE=\"${OPTARG}\" ;;\n        i) INCLUDE_EXT+=(\"${OPTARG}\") ;;  # Store in INCLUDE_EXT array\n        e) EXCLUDE_EXT+=(\"${OPTARG}\") ;;  # Store in EXCLUDE_EXT array\n        s) MAX_SIZE=\"${OPTARG}\"\n            # Validate if MAX_SIZE is a valid number\n            if ! [[ \"$MAX_SIZE\" =~ ^[0-9]+$ ]]; then\n                echo \"Error: Invalid value for -s option. Please provide a positive integer.\"\n                exit 1\n            fi\n            ;;\n        g) RESPECT_GITIGNORE=\"${OPTARG}\" ;;\n        d) INCLUDE_DOT_FILES=\"${OPTARG}\" ;;\n        z) ZIP_OUTPUT=\"${OPTARG}\" ;;\n        m) MAX_DEPTH=\"${OPTARG}\"\n            # Validate if MAX_DEPTH is a valid number\n            if ! [[ \"$MAX_DEPTH\" =~ ^[0-9]+$ ]]; then\n                echo \"Error: Invalid value for -m option. Please provide a positive integer.\"\n                exit 1\n            fi\n            ;;\n        v) show_version\n           exit 0 ;;\n        h) show_help\n           exit 0 ;;\n        -) case \"${OPTARG}\" in\n             version) show_version\n                      exit 0 ;;\n             help) show_help\n                  exit 0 ;;\n             *) echo \"Error: Invalid option -${OPTARG}. Use -h or --help for usage information.\" >&2\n                exit 1 ;;\n           esac ;;\n    esac\ndone\n\n# Ensure required parameters are provided\nif [ -z \"$DIRECTORY_PATH\" ]; then\n    echo \"Directory path is required.\"\n    show_help\n    exit 1\nfi\n\n# Check if directory exists\nif [[ ! -d \"$DIRECTORY_PATH\" ]]; then\n  echo \"Error: Directory '$DIRECTORY_PATH' does not exist.\"\n  exit 1\nfi\n\n# Determine the output file name if a directory is specified\nif [ -d \"$OUTPUT_FILE\" ]; then\n    base_dir=$(basename \"$(realpath \"$DIRECTORY_PATH\")\")\n    OUTPUT_FILE=\"$OUTPUT_FILE/$base_dir.json\"\nfi\n\n# Validate output file path\noutput_dir=$(dirname \"$OUTPUT_FILE\")\nif [[ ! -d \"$output_dir\" || ! -w \"$output_dir\" ]]; then\n  echo \"Error: Cannot write to output directory '$output_dir'.\"\n  exit 1\nfi\n\n# Check dependencies before proceeding\ncheck_dependencies\n\n# Normalize the include and exclude extensions to ensure they start with a dot\nfor i in \"${!INCLUDE_EXT[@]}\"; do\n    if [[ \"${INCLUDE_EXT[$i]}\" != \"\" && \"${INCLUDE_EXT[$i]:0:1}\" != \".\" ]]; then\n        INCLUDE_EXT[$i]=\".${INCLUDE_EXT[$i]}\"\n    fi\ndone\n\nfor i in \"${!EXCLUDE_EXT[@]}\"; do\n    if [[ \"${EXCLUDE_EXT[$i]}\" != \"\" && \"${EXCLUDE_EXT[$i]:0:1}\" != \".\" ]]; then\n        EXCLUDE_EXT[$i]=\".${EXCLUDE_EXT[$i]}\"\n    fi\ndone\n\n# Determine OS and set the appropriate stat command\nif [[ \"$OSTYPE\" == \"darwin\"* ]]; then\n    STAT_CMD=\"stat -f%z\"\nelse\n    STAT_CMD=\"stat -c%s\"\nfi\n\n# Function to process each file\nprocess_file() {\n    local file=\"$1\"\n    # Check if respecting .gitignore and if the file is ignored\n    if [[ \"$RESPECT_GITIGNORE\" -eq 1 && -d \"$DIRECTORY_PATH/.git\" ]]; then\n        if git --git-dir=\"$DIRECTORY_PATH/.git\" --work-tree=\"$DIRECTORY_PATH\" check-ignore \"$file\" > /dev/null; then\n            return # Skip file if it is ignored by .gitignore\n        fi\n    fi\n\n    local filesize=$($STAT_CMD \"$file\")\n    if [ \"$filesize\" -le $((MAX_SIZE * 1024)) ]; then\n        local filename=$(basename \"$file\")\n        local dirpath=$(dirname \"$file\" | sed \"s|^$DIRECTORY_PATH||\")\n\n        # Create a temporary file for the JSON output\n        local temp_json=$(mktemp)\n\n        if is_binary \"$file\"; then\n            # For binary files, create JSON with null content\n            jq -n --arg filename \"$filename\" \\\n               --arg path \"$dirpath/\" \\\n               '{filename: $filename, path: $path, content: null}' > \"$temp_json\"\n        else\n            # For text files, read content and create JSON\n            jq -n --arg filename \"$filename\" \\\n               --arg path \"$dirpath/\" \\\n               --rawfile content \"$file\" \\\n               '{filename: $filename, path: $path, content: $content}' > \"$temp_json\"\n        fi\n\n        # Output the JSON and remove temp file\n        cat \"$temp_json\"\n        rm \"$temp_json\"\n    fi\n}\n\nexport -f process_file is_binary\nexport STAT_CMD MAX_SIZE DIRECTORY_PATH RESPECT_GITIGNORE INCLUDE_DOT_FILES\n\n# Construct the find command\nfind_command=\"find \\\"$DIRECTORY_PATH\\\" -type f\"\n\n# Exclude dot files if INCLUDE_DOT_FILES is 0\nif [ \"$INCLUDE_DOT_FILES\" -eq 0 ]; then\n    find_command+=\" -not -path '*/.*'\"\nfi\n\n# Include extensions\nif [ ${#INCLUDE_EXT[@]} -gt 0 ]; then\n    find_command+=\" \\\\( -name \\\"*${INCLUDE_EXT[0]}\\\"\"\n    for ext in \"${INCLUDE_EXT[@]:1}\"; do\n        find_command+=\" -o -name \\\"*${ext}\\\"\"\n    done\n    find_command+=\" \\\\)\"\nfi\n\n# Exclude extensions \n# (Only if include extensions are NOT specified to avoid redundancy)\nif [ ${#INCLUDE_EXT[@]} -eq 0 ] && [ ${#EXCLUDE_EXT[@]} -gt 0 ]; then\n    find_command+=\" \\\\( -not -name \\\"*${EXCLUDE_EXT[0]}\\\"\"\n    for ext in \"${EXCLUDE_EXT[@]:1}\"; do\n        find_command+=\" -and -not -name \\\"*${ext}\\\"\"\n    done\n    find_command+=\" \\\\)\"\nfi\n\n# Add max depth to find command if specified\nif [ -n \"$MAX_DEPTH\" ]; then\n    find_command+=\" -maxdepth $MAX_DEPTH\"\nfi\n\n# Execute find command, filter out excluded files, and process files\nfind_result=$(eval \"$find_command\")\nif [[ $? -ne 0 ]]; then\n  echo \"Error: find command failed with error:\"\n  echo \"$find_result\"\n  exit 1\nfi\njson_array=$(echo \"$find_result\" | grep -v \"\\.${EXCLUDE_EXT##*.}$\" | xargs -I {} bash -c 'process_file \"{}\"' | jq -s .)\n\n# Output the JSON object using jq and pretty print\necho \"{\\\"files\\\":$json_array}\" | jq . > \"$OUTPUT_FILE\"\n\n# Zip the output file if requested\nif [ \"$ZIP_OUTPUT\" -eq 1 ]; then\n    zip_file=\"${OUTPUT_FILE%.*}.zip\"\n    zip -jq \"$zip_file\" \"$OUTPUT_FILE\"\n    # Check zip exit status\n    if [[ $? -ne 0 ]]; then\n        echo \"Error: Failed to zip the output file.\"\n        exit 1\n    fi\n    echo \"Output file zipped: $zip_file\"\nfi\n\necho \"JSON output saved to: $OUTPUT_FILE\"\n\n# Generate directory tree using git ls-files command\necho \"Directory tree:\"\nif [[ \"$RESPECT_GITIGNORE\" -eq 1 && -d \"$DIRECTORY_PATH/.git\" ]]; then\n  # Use git ls-files to get the list of tracked files, respecting .gitignore\n  git --git-dir=\"$DIRECTORY_PATH/.git\" --work-tree=\"$DIRECTORY_PATH\" ls-files -z | \n  while IFS= read -r -d '' file; do\n    # Print the file path relative to the target directory\n    echo \"${file#$DIRECTORY_PATH/}\"\n  done\nelse\n  # If not respecting .gitignore, use find to list all files\n  eval \"$find_command -print\"\nfi\n"
    },
    {
      "filename": "start_containers.sh",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n\n# Function to start containers for a given directory and compose file (optional)\nstart_containers() {\n  local directory=\"$1\"\n  local compose_file=\"$2\" # Optional: Specify compose file if different from docker-compose.yml\n  cd \"$directory\"\n\n  # Use specified compose file or default to docker-compose.yml\n  if [[ -n \"$compose_file\" ]]; then\n    docker-compose -f \"$compose_file\" up -d\n  else\n    docker-compose up -d\n  fi\n  sleep 5\n}\n\n# Function to stop containers for a given directory and compose file (optional)\nstop_containers() {\n  local directory=\"$1\"\n  local compose_file=\"$2\" # Optional: Specify compose file if different from docker-compose.yml\n  cd \"$directory\"\n\n  # Use specified compose file or default to docker-compose.yml\n  if [[ -n \"$compose_file\" ]]; then\n    docker-compose -f \"$compose_file\" down\n  else\n    docker-compose down\n  fi\n  sleep 5\n}\n\n# Check for command line arguments\nif [[ $# -eq 0 ]]; then\n  echo \"Usage: $0 [start|stop]\"\n  exit 1\nfi\n\n# Get the action (start or stop) from the command line argument\naction=\"$1\"\n\ncase \"$action\" in\n  start)\n    # Start containers for all applications\n    start_containers \"$HOME/Workspace/flowbots\" \n    start_containers \"$HOME/LocalLLM/LLMOS/dify/docker\" \n    start_containers \"$HOME/LocalLLM/LLMOS/big-AGI\"\n    start_containers \"$HOME/LocalLLM/SillyTavern/docker\"\n    start_containers \"$HOME/LocalLLM/SillyTavern-Extras/docker\"\n    start_containers \"$HOME/LocalLLM/lobe-chat/docker-compose/local\"\n    docker container start ollama\n    docker container start local-ai\n    ;;\n  stop)\n    # Stop containers for all applications\n    stop_containers \"$HOME/Workspace/flowbots\" \n    stop_containers \"$HOME/LocalLLM/LLMOS/dify/docker\" \n    stop_containers \"$HOME/LocalLLM/LLMOS/big-AGI\"\n    stop_containers \"$HOME/LocalLLM/SillyTavern/docker\"\n    stop_containers \"$HOME/LocalLLM/SillyTavern-Extras/docker\"\n    stop_containers \"$HOME/LocalLLM/lobe-chat/docker-compose/local\"\n    docker container stop ollama\n    docker container stop local-ai\n    ;;\n  *)\n    echo \"Invalid action: $action. Choose either 'start' or 'stop'.\"\n    exit 1\n    ;;\nesac\n"
    },
    {
      "filename": "dictation.sh",
      "path": "/bin/",
      "content": "#!/usr/bin/env bash\n\n\nset -e\n\n# uses: gum, xsel, jack_capture, sox, curl\n\n# Configuration (possibly source from a .env file)\ndeclare -rx timestamp_format='%Y-%m-%d_%H-%M-%S'\n# declare -rx dictations=\"$HOME/Desktop/Dictations\"\ndeclare -rx dictations_tmp=\"/tmp/audio/dictations\"\n\ndeclare -rx localwhisper=\"http://tinybot.syncopated.net:8082/inference\"\n\nif ! /usr/bin/curl -s \"$localwhisper\" > /dev/null; then\n  echo \"Error: Whisper service is not running at $localwhisper. Please start it before running this script.\"\n  exit 1\nfi\n\nTMPDIR=$(mktemp --directory)\n\ndeclare -rx tmpmarkdown=$(mktemp -p ${TMPDIR} --suffix .md)\n\n# Get the notebook to record to\necho \"choose a notebook\\n\"\nnotebook=$(gum file --height=10 --directory ~/Desktop)\ndictations=\"${notebook}/_dictations\"\n\n# Function to get the output file path\nget_output_file() {\n  if [[ -z \"$1\" ]]; then\n\t# No file name provided, prompt for one\n\toutput_file=$(gum file --height=10 ~/Desktop)\n  else\n\t# Use the provided file name\n\toutput_file=\"$1\"\n  fi\n  echo \"$output_file\"\n}\n\n# Capture audio with default settings for Reaper outputs\nfunction capture_audio() {\n\tcapture_args=(\n\t\t-s\n\t\t-f wav\n\t\t-b 16  # Adjust bit depth if needed\n\t\t-d 600   # Adjust duration limit if needed\n\t\t-c 2    # Capture from Reaper outputs (out1, out2)\n\t\t-p 'REAPER:out1' -p 'REAPER:out2'\n\t)\n\n\ttput cup 15 0\n\t# Execute capture and conversion\n\tjack_capture \"${capture_args[@]}\" \"${dictations}/${wavfile}\" &&\n\t\tsox \"${dictations}/${wavfile}\" -r 16000 -b 16 \"${tmpwavfile}\" remix 1-2\n}\n\nfunction termput() {\n\ttput clear && tput cup 15 0\n}\n\nfunction cleanup() {\n\tfd \".wav\" $dictations_tmp -X rm {}\n}\n\n# Main Script Logic\ntrap cleanup SIGINT SIGTERM ERR EXIT\n\ngum style \\\n\t--foreground 014 --border-foreground 024 --border double \\\n\t--align center --width 50 --margin \"1 2\" --padding \"2 4\" \\\n\t'Hello.' && sleep 1\n\nif ! [ -d $dictations_tmp ]; then\n\tmkdir -pv $dictations_tmp\n\techo \"tmp dictations dir created!\"\nfi\n\n# Get the output file path\noutput_file=$(get_output_file \"$1\")\n\nwhile true; do\n\n\t# Determine jack_capture options and filenames\n\tdeclare -x timestamp=$(date +\"$timestamp_format\")\n\tdeclare -x capture_file=\"jack_capture_${timestamp}\"\n\tdeclare -x wavfile=\"${capture_file}.wav\"\n\tdeclare -x tmpwavfile=\"${dictations_tmp}/${capture_file}.wav\"\n\n\n\tcapture_audio\n\n\tif [[ $? -ne 0 ]]; then\n\t\tnotify-send -t 5000 -u critical \"Capture initialization failed\"\n\t\texit\n\telse\n\t\ttput clear && notify-send -t 5000 'transcribing....'\n\t\tlogger -t dictation --priority user.debug transcribing ${dictations}/${wavfile}\n        # Use gum spin for transcription\n        text=$(gum spin -s line --title \"Transcribing...\" --show-output -- \\\n            curl $localwhisper \\\n            -H \"Content-Type: multipart/form-data\" \\\n            -F file=\"@${tmpwavfile}\" \\\n            -F temperature=\"0.2\" | jq -r .text | ruby -pe 'gsub(/^\\s/, \"\")')\n\n\t\tif [[ \"$text\" == \"null\" ]]; then\n\t\t\tnotify-send -t 5000 -u critical \"Transcription failed\"\n\t\telse\n\t\t\techo \"$text\" >> \"$output_file\"\n\t\t\tnotify-send -t 5000 \"Transcription saved to: $output_file\"\n\t\tfi\n\tfi\n\n\ttermput\n\n\tnext=$(gum choose --timeout=6s --selected=\"continue\" \"continue\" \"exit\")\n\n\tif [[ \"${next}\" == \"exit\" ]]; then\n\t\tbreak\n\tfi\ndone\n\ntermput\n\ncleanup\n\ntput clear && tput cup 20 40 && gum spin --spinner dot --spinner.margin=\"2 2\" --title \"exiting in 2 seconds...\" -- sleep 2\n"
    },
    {
      "filename": "whisper-stream",
      "path": "/bin/",
      "content": "#!/bin/bash\n\n# Default configuration and versioning\nVERSION=\"1.0.9\"\n\n# Setting the default values for the script parameters\nMIN_VOLUME=1%      # Minimum volume threshold\nSILENCE_LENGTH=1.5 # Minimum silence duration in seconds\nONESHOT=false      # Flag to determine if the script should run once or continuously\nDURATION=0         # Duration of the recording in seconds (0 means continuous)\nMODEL=\"\"  # Model for the OpenAI API\nTOKEN=\"\"           # OpenAI API token\nOUTPUT_DIR=\"\"      # Directory to save the transcriptions\nDEST_FILE=\"\"       # File to write the transcriptions to\nPROMPT=\"\"          # Prompt for the API call\nLANGUAGE=\"\"        # Language code for transcription\nTRANSLATE=\"\"       # Flag to indicate translation to English\nAUDIO_FILE=\"\"      # Specific audio file for transcription\nPIPE_TO_CMD=\"\"     # Command to pipe the transcribed text to\nQUIET_MODE=false   # Flag to determine if the banner and settings should be suppressed\n\n# Display help information for script usage\nfunction display_help() {\n\techo \"Usage: $0 [options]\"\n\techo \"Options:\"\n\techo \"  -v, --volume <value>     Set the minimum volume threshold (default: 1%)\"\n\techo \"  -s, --silence <value>    Set the minimum silence length (default: 1.5)\"\n\techo \"  -o, --oneshot            Enable one-shot mode\"\n\techo \"  -d, --duration <value>   Set the recording duration in seconds (default: 0, continuous)\"\n\techo \"  -t, --token <value>      Set the OpenAI API token\"\n\techo \"  -p, --path <value>       Set the output directory path to create the transcription file\"\n\techo \"  -e, --dictation <value>  Set the destination file to write transcriptions to\"\n\techo \"  -r, --prompt <value>     Set the prompt for the API call\"\n\techo \"  -i, --diarization\t\t\t\t Enable diarization mode, record in stereo\"\n\techo \"  -l, --language <value>   Set the language in ISO-639-1 format\"\n\techo \"  -f, --file <value>       Set the audio file to be transcribed\"\n\techo \"  -tr, --translate         Translate the transcribed text to English\"\n\techo \"  -p2, --pipe-to <cmd>     Pipe the transcribed text to the specified command (e.g., 'wc -m')\"\n\techo \"  -V, --version            Show the version number\"\n\techo \"  -q, --quiet              Suppress the banner and settings\"\n\techo \"  -h, --help               Display this help message\"\n\techo \"To stop the app, press Ctrl+C\"\n\texit 0\n}\n\n# Check the validity of the provided audio file\nfunction check_audio_file() {\n\tlocal file=$1\n\n\t# Check if the file exists\n\tif [ ! -f \"$file\" ]; then\n\t\techo \"File does not exist: $file\"\n\t\texit 1\n\tfi\n\n\t# Check if the file is not empty\n\tif [ ! -s \"$file\" ]; then\n\t\techo \"File is empty: $file\"\n\t\texit 1\n\tfi\n\n\t# Check if the file size is under 25MB\n\tlocal filesize\n\tif [[ \"$OSTYPE\" == \"linux-gnu\"* ]]; then\n\t\tfilesize=$(stat -c%s \"$file\")\n\telif [[ \"$OSTYPE\" == \"darwin\"* ]]; then\n\t\tfilesize=$(stat -f%z \"$file\")\n\telse\n\t\techo \"Unknown operating system\"\n\t\texit 1\n\tfi\n\tif [ $filesize -gt 26214400 ]; then\n\t\techo \"File size is over 25MB: $file\"\n\t\texit 1\n\tfi\n\n\t# Check if the file format is acceptable\n\tlocal ext=\"${file##*.}\"\n\tcase \"$ext\" in\n\tm4a | mp3 | webm | mp4 | mpga | wav | mpeg) ;;\n\t*)\n\t\techo \"File format is not acceptable: $file\"\n\t\texit 1\n\t\t;;\n\tesac\n}\n\n# Parse command-line arguments to set script parameters\nwhile [[ $# -gt 0 ]]; do\n\tkey=\"$1\"\n\tcase $key in\n\t-v | --volume)\n\t\tif [[ ! $2 || $2 == -* ]]; then\n\t\t\techo \"Error: Missing value for $1\"\n\t\t\texit 1\n\t\tfi\n\t\tMIN_VOLUME=\"$2\"\n\t\tif [[ \"$MIN_VOLUME\" != *% ]]; then\n\t\t\tMIN_VOLUME+=\"%\"\n\t\tfi\n\t\tshift\n\t\tshift\n\t\t;;\n\t-s | --silence)\n\t\tif [[ ! $2 || $2 == -* ]]; then\n\t\t\techo \"Error: Missing value for $1\"\n\t\t\texit 1\n\t\tfi\n\t\tSILENCE_LENGTH=\"$2\"\n\t\tshift\n\t\tshift\n\t\t;;\n\t-o | --oneshot)\n\t\tONESHOT=true\n\t\tshift\n\t\t;;\n\t-i | --diarization)\n\t\tDIARIZATION=true\n\t\tshift\n\t\t;;\n\t-d | --duration)\n\t\tif [[ ! $2 || $2 == -* ]]; then\n\t\t\techo \"Error: Missing value for $1\"\n\t\t\texit 1\n\t\tfi\n\t\tDURATION=\"$2\"\n\t\tshift\n\t\tshift\n\t\t;;\n\t-t | --token)\n\t\tif [[ ! $2 || $2 == -* ]]; then\n\t\t\techo \"Error: Missing value for $1\"\n\t\t\texit 1\n\t\tfi\n\t\tTOKEN=\"$2\"\n\t\tshift\n\t\tshift\n\t\t;;\n\t-p | --path)\n\t\tif [[ ! $2 || $2 == -* ]]; then\n\t\t\techo \"Error: Missing value for $1\"\n\t\t\texit 1\n\t\tfi\n\t\tOUTPUT_DIR=\"$2\"\n\t\t# check if the output directory exists\n\t\tif [ ! -d \"$OUTPUT_DIR\" ]; then\n\t\t\techo \"Directory does not exist: $OUTPUT_DIR\"\n\t\t\texit 1\n\t\tfi\n\t\tshift\n\t\tshift\n\t\t;;\n\t-e | --dictation)\n\t\tif [[ ! $2 || $2 == -* ]]; then\n\t\t\techo \"Error: Missing value for $1\"\n\t\t\texit 1\n\t\tfi\n\t\tDEST_FILE=\"$2\"\n\n\t\t# OUTPUT_DIR=\"$HOME/Workspace/Notebook/_transcriptions\"\n\t\t# check if the file exists\n\t\tif [ ! -f \"$DEST_FILE\" ]; then\n\t\t\techo \"File does not exist: $DEST_FILE\"\n\t\t\texit 1\n\t\tfi\n\t\tshift\n\t\tshift\n\t\t;;\n\t-r | --prompt)\n\t\tif [[ ! $2 || $2 == -* ]]; then\n\t\t\techo \"Error: Missing value for $1\"\n\t\t\texit 1\n\t\tfi\n\t\tPROMPT=\"$2\"\n\t\tshift\n\t\tshift\n\t\t;;\n\t-l | --language)\n\t\tif [[ ! $2 || $2 == -* ]]; then\n\t\t\techo \"Error: Missing value for $1\"\n\t\t\texit 1\n\t\tfi\n\t\tLANGUAGE=\"$2\"\n\t\tshift\n\t\tshift\n\t\t;;\n\t-tr | --translate)\n\t\tTRANSLATE=true\n\t\tshift\n\t\t;;\n\t-p2 | --pipe-to)\n\t\tif [[ ! $2 || $2 == -* ]]; then\n\t\t\techo \"Error: Missing cmd for $1\"\n\t\t\texit 1\n\t\tfi\n\t\tPIPE_TO_CMD=\"$2\"\n\t\tshift\n\t\tshift\n\t\t;;\n\t-f | --file)\n\t\tif [[ ! $2 || $2 == -* ]]; then\n\t\t\techo \"Error: Missing value for $1\"\n\t\t\texit 1\n\t\tfi\n\t\tAUDIO_FILE=\"$2\"\n\t\tcheck_audio_file \"$AUDIO_FILE\"\n\t\tshift\n\t\tshift\n\t\t;;\n\t-V | --version)\n\t\tSHOW_VERSION=true\n\t\tshift\n\t\t;;\n\t-q | --quiet)\n\t\tQUIET_MODE=true\n\t\tshift\n\t\t;;\n\t-h | --help)\n\t\tdisplay_help\n\t\t;;\n\t*)\n\t\techo \"Unknown option: $1\"\n\t\texit 1\n\t\t;;\n\tesac\ndone\n\n# Display version if the version flag is set\nif [ \"$SHOW_VERSION\" = true ]; then\n\techo \"Whisper Stream Speech-to-Text Transcriber (version: $VERSION)\"\n\texit\nfi\n\n\noutput_files=() # Array to store the names of output audio files\n\n# Function to get the name of the current audio input device on macOS\nfunction get_macos_input_device() {\n\t# if SwitchAudioSource command available\n\tif [ -x \"$(command -v SwitchAudioSource)\" ]; then\n\t\tlocal input_device=$(SwitchAudioSource -t input -c)\n\t\techo \"$input_device\"\n\t\treturn\n\tfi\n}\n\n# Function to get the volume of the audio input on macOS\nfunction get_macos_input_volume() {\n\tlocal input_volume=$(osascript -e \"input volume of (get volume settings)\")\n\techo \"$input_volume%\"\n}\n\n# Function to get the name of the current audio input device on Linux\nfunction get_linux_input_device() {\n\t# if arecord command available\n\tif [ -x \"$(command -v arecord)\" ]; then\n\t\tlocal input_device=$(arecord -l | grep -oP \"(?<=card )\\d+(?=:\\s.*\\[)\")\n\t\techo \"hw:1\"\n\t\treturn\n\tfi\n}\n\nfunction get_linux_input_volume() {\n\t# Check if amixer command is available and executable\n\tif [ -x \"$(command -v amixer)\" ]; then\n\t\tlocal input_volume=$(amixer sget Capture | grep 'Left:' | awk -F'[][]' '{ print $2 }')\n\t\techo \"$input_volume\"\n\t\treturn\n\tfi\n}\n\n# Function to display current settings\nfunction display_settings() {\n\tif [ \"$QUIET_MODE\" = true ]; then\n\t\treturn\n\tfi\n\n\techo \"\"\n\techo $'\\e[1;34m'Whisper Stream Speech-to-Text Transcriber$'\\e[0m' ${VERSION}\n\techo $'\\e[1;33m'-----------------------------------------------$'\\e[0m'\n\techo \"Current settings:\"\n\techo \"  Volume threshold: $MIN_VOLUME\"\n\techo \"  Silence length: $SILENCE_LENGTH seconds\"\n\techo \"  Input language: ${LANGUAGE:-Not specified}\"\n\n\tif [ -n \"$TRANSLATE\" ]; then\n\t\techo \"  Translate to English: $TRANSLATE\"\n\tfi\n\n\tif [ -n \"$OUTPUT_DIR\" ]; then\n\t\techo \"  Output Dir: $OUTPUT_DIR\"\n\tfi\n\n\tif [ -n \"$DEST_FILE\" ]; then\n\t\techo \"  Destination File: $DEST_FILE\"\n\tfi\n\n\t# Get the input device based on the operating system\n\tlocal input_device=$(get_input_device)\n\tif [ -n \"$input_device\" ]; then\n\t\techo \"  Input device: $input_device\"\n\tfi\n\n\t# Get the input volume based on the operating system\n\tlocal input_volume=$(get_input_volume)\n\tif [ -n \"$input_volume\" ]; then\n\t\techo \"  Input volume: $input_volume\"\n\tfi\n\n\techo $'\\e[1;33m'-----------------------------------------------$'\\e[0m'\n\techo To stop the app, press $'\\e[0;36m'Ctrl+C$'\\e[0m'\n\techo \"\"\n}\n\n# Get the name of the current audio input device based on OS\nfunction get_input_device() {\n\tcase \"$(uname)\" in\n\tDarwin)\n\t\tget_macos_input_device\n\t\t;;\n\tLinux)\n\t\tget_linux_input_device\n\t\t;;\n\t*)\n\t\techo \"Unknown operating system\"\n\t\t;;\n\tesac\n}\n\n# Get the volume level of the current audio input device based on OS\nfunction get_input_volume() {\n\tcase \"$(uname)\" in\n\tDarwin)\n\t\tget_macos_input_volume\n\t\t;;\n\tLinux)\n\t\tget_linux_input_volume\n\t\t;;\n\t*)\n\t\techo \"Unknown operating system\"\n\t\t;;\n\tesac\n}\n\n# Display a rotating spinner animation\nfunction spinner() {\n\tlocal pid=$1\n\tlocal delay=0.1\n\tlocal spinstr='|/-\\\\'\n\twhile kill -0 $pid 2>/dev/null; do\n\t\tlocal temp=${spinstr#?}\n\t\tprintf \"\\r\\e[1;31m%c\\e[0m\" \"$spinstr\"\n\t\tlocal spinstr=$temp${spinstr%\"$temp\"}\n\t\tsleep $delay\n\tdone\n\tprintf \"\\r\\e[K\"\n}\n\n# Convert the audio to text using the OpenAI Whisper API\nfunction convert_audio_to_text() {\n\tlocal output_file=$1\n\tif [ -n \"$TRANSLATE\" ]; then\n\t\t# base_url=\"http://localhost:8021/v1/audio/transcriptions\"\n\t\tbase_url=\"http://tinybot.syncopated.net:8082/v1/audio/transcriptions\"\n\telse\n\t\t# base_url=\"http://localhost:8021/v1/audio/transcriptions\"\n\t\tbase_url=\"http://tinybot.syncopated.net:8082/v1/audio/transcriptions\"\n\tfi\n\tlocal curl_command=\"/usr/bin/curl -s $base_url \\\n    --header \\\"Content-Type: multipart/form-data\\\" \\\n    --form \\\"file=@$output_file\\\" \\\n    --form \\\"response_format=json\\\"\"\n\n\tif [ -n \"$PROMPT\" ]; then\n\t\tcurl_command+=\" --form \\\"prompt=$PROMPT\\\"\"\n\tfi\n\n\tif [ -n \"$LANGUAGE\" ]; then\n\t\tcurl_command+=\" --form \\\"language=$LANGUAGE\\\"\"\n\tfi\n\n\tmaxretries=3\n\n\tresponse=$(eval $curl_command)\n\t# Check if the curl command was successful\n\t# retry until successful or max retries reached\n\twhile [ $? -ne 0 ] && [ $maxretries -gt 0 ]; do\n\t\t# print a red dot to indicate a failed API call\n\t\tprintf \"\\e[1;31m.\\e[0m\"\n\t\tresponse=$(eval $curl_command)\n\t\tmaxretries=$((maxretries - 1))\n\tdone\n\n\ttranscription=$(echo \"$response\" | jq -r '.text' | sd '^\\s' '')\n\n\t# Check if the curl command was successful\n\t# retry until successful or max retries reached\n\twhile [ $? -ne 0 ] && [ $maxretries -gt 0 ]; do\n\t\t# print a red dot to indicate a failed API call\n\t\tprintf \"\\e[1;31m.\\e[0m\"\n\t\tresponse=$(eval $curl_command)\n\t\tmaxretries=$((maxretries - 1))\n\tdone\n\n\tprintf \"\\r\\e[K\"\n\txsel -cb\n\txsel -a -b <<<$transcription\n\techo \"$transcription\"\n\n\tif [ -n \"${DEST_FILE}\" ]; then\n\t\t# Write the transcription to the destination file\n\t\t# LLMEDIT=$(aichat -r transcription_editor 'NLP' \"${transcription}\")\n\t\techo \"$transcription\" >> \"${DEST_FILE}\"\n\t\t# echo \"$LLMEDIT\" >> \"${DEST_FILE}\"\n\tfi\n\n\tif [ \"$ONESHOT\" ]; then\n\t\t# Write the transcription to the destination file\n\t\techo \"$transcription\" | xsel -i -b\n\tfi\n\n\t#TODO: send transcription to LLM API for editing\n\n\tif [ -n \"$PIPE_TO_CMD\" ]; then\n\t\tresult=$(echo \"$transcription\" | $PIPE_TO_CMD)\n\t\techo $result\n\tfi\n\n\t# Remove the output audio file unless the `-f` option is specified\n\tif [ -z \"$AUDIO_FILE\" ]; then\n\t\trm -f \"$output_file\"\n\tfi\n\n\t# Accumulate the transcribed text in a temporary file\n\t# this is necessary for the data to be available when the script terminates\n\techo \"$transcription\" >>temp_transcriptions.txt\n}\n\n# Handle the script termination: clean up and save transcriptions\nfunction handle_exit() {\n\n\t# Wait for all background jobs to finish\n\twait\n\n\t# Kill all child processes\n\tpkill -P $$\n\n\t# Remove all output audio files\n\tfor file in \"${output_files[@]}\"; do\n\t\trm -f \"$file\"\n\tdone\n\n\t# if temp_transcriptions.txt exists, remove it\n\tif [ -f temp_transcriptions.txt ]; then\n\t\t# read data from temp_transcriptions.txt into the accumulated_text variable\n\t\t# and remove temp_transcriptions.txt\n\t\taccumulated_text=$(cat temp_transcriptions.txt)\n\t\trm -f temp_transcriptions.txt\n\t\t# Clear the current line\n\t\tprintf \"\\r\\e[K\\n\"\n\telse\n\t\tprintf \"\\r\\e[K\"\n\t\texit\n\tfi\n\n\t# If output directory is specified, create a text file with the accumulated text in the specified directory\n\tif [ -n \"$OUTPUT_DIR\" ]; then\n\t\ttimestamp=$(date +\"%Y-%m-%d_%H-%M-%S\")\n\t\techo \"$accumulated_text\" >\"$OUTPUT_DIR/$timestamp.md\"\n\tfi\n\n\t# Copy the accumulated text to the clipboard\n\tcase \"$(uname -s)\" in\n\tDarwin)\n\t\t# if $accumulated_text is not empty\n\t\tif [ -n \"$accumulated_text\" ]; then\n\t\t\techo \"$accumulated_text\" >temp.txt\n\t\t\tcat temp.txt | pbcopy\n\t\t\trm temp.txt\n\t\t\techo $'\\e[0;36m'Transcription copied to clipboard.$'\\e[0m'\n\t\tfi\n\t\t;;\n\tLinux)\n\t\techo \"$accumulated_text\" | xclip -selection clipboard >&1\n\t\t;;\n\tCYGWIN* | MINGW32* | MSYS* | MINGW*)\n\t\t# This is a rough guess that you're on Windows Subsystem for Linux\n\t\t# if $accumulated_text is not empty\n\t\tif [ -n \"$accumulated_text\" ]; then\n\t\t\techo \"$accumulated_text\" | clip.exe >&1\n\t\t\techo $'\\e[0;36m'Transcription copied to clipboard.$'\\e[0m'\n\t\tfi\n\t\t;;\n\t*)\n\t\techo \"Unknown OS, cannot copy to clipboard\"\n\t\t;;\n\tesac\n\n\texit\n}\n\n# If an audio file is provided, convert it to text and then exit\nif [ -n \"$AUDIO_FILE\" ]; then\n\n\t# print banner and settings unless quiet mode is enabled\n\t# this is necessary for the data to be available when the script terminates\n\tif [ \"$QUIET_MODE\" = false ]; then\n\t\techo \"\"\n\t\techo $'\\e[1;34m'Whisper Stream Transcriber$'\\e[0m' ${VERSION}\n\t\techo $'\\e[1;33m'-----------------------------------------------$'\\e[0m'\n\t\techo \"Current settings:\"\n\t\techo \"  Input language: ${LANGUAGE:-Not specified}\"\n\n\t\tif [ -n \"$TRANSLATE\" ]; then\n\t\t\techo \"  Translate to English: $TRANSLATE\"\n\t\tfi\n\n\t\tif [ -n \"$OUTPUT_DIR\" ]; then\n\t\t\techo \"  Output Dir: $OUTPUT_DIR\"\n\t\tfi\n\n\t\techo \"  Input file: $AUDIO_FILE\"\n\t\techo $'\\e[1;33m'-----------------------------------------------$'\\e[0m'\n\t\techo $'\\e[0;36m'Please wait ...$'\\e[0m'\n\t\techo \"\"\n\tfi\n\n\tconvert_audio_to_text \"$AUDIO_FILE\"\n\thandle_exit\nfi\n\ntemp_dir=$(mktemp -d -p /tmp)\n\n# Display the current configuration/settings of the script\ndisplay_settings\n\n# Handle script termination using trap for SIGINT (Ctrl+C) and SIGTSTP (Ctrl+Z)\ntrap handle_exit SIGINT SIGTSTP\n\necho \"Choose a notebook:\"\nnotebook=$(gum file --height=10 --directory \"$HOME/Desktop\" --file=false)\nif [ ! -d \"$notebook\" ]; then\n  echo \"Error: Selected notebook directory does not exist.\"\n  exit 1\nfi\ndictations=\"${notebook}/_dictations\"\nmkdir -p \"$dictations\"\n\n# Main loop to continuously record audio, detect silence, and transcribe audio\nwhile true; do\n\t# Set the path to the output audio file\n\tOUTPUT_FILE=\"${temp_dir}/output_$(date +%s).wav\"\n\n\t# Add the output file to the array\n\toutput_files+=(\"$OUTPUT_FILE\")\n\n\t# Add a prompt at the beginning of the recording\n\t# echo -n $'\\e[1;32m'▶ $'\\e[0m'\n\n\t# Record audio in raw format then convert to mp3\n\tif [ \"$DURATION\" -gt 0 ]; then\n\t\trec -q -V0 -e signed -L -c 1 -b 16 -r 16000 -t raw \\\n\t\t\t- trim 0 \"$DURATION\" silence 1 0.1 \"$MIN_VOLUME\" 1 \"$SILENCE_LENGTH\" \"$MIN_VOLUME\" |\n\t\t\tsox -t raw -r 16000 -b 16 -e signed -c 1 - \"$OUTPUT_FILE\"\n\telif [ \"$DIARIZATION\" ]; then\n\t\trec -q -V0 -e signed -L -c 2 -b 16 -r 16000 -t raw \\\n\t\t\t- silence 1 0.1 \"$MIN_VOLUME\" 1 \"$SILENCE_LENGTH\" \"$MIN_VOLUME\" |\n\t\t\tsox -t raw -r 16000 -b 16 -e signed -c 2 - \"$OUTPUT_FILE\"\n\telse\n\t\trec -q -V0 -e signed -L -c 1 -b 16 -r 16000 -t raw \\\n\t\t\t- silence 1 0.1 \"$MIN_VOLUME\" 1 \"$SILENCE_LENGTH\" \"$MIN_VOLUME\" |\n\t\t\tsox -t raw -r 16000 -b 16 -e signed -c 1 - \"$OUTPUT_FILE\"\n\tfi\n\n\t# Check if the audio file is created successfully\n\tif [ -s \"$OUTPUT_FILE\" ]; then\n\t\t# Convert the MP3 audio to text using the Whisper API in the background\n\t\tconvert_audio_to_text \"$OUTPUT_FILE\" &\n\n\t\t# Captures the process ID of the last executed background command.\n\t\tpid=$!\n\t\tspinner $pid &\n\t\t# Read the transcriptions into the accumulated_text variable\n\telse\n\t\techo \"No audio recorded.\"\n\tfi\n\n\tif [ \"$ONESHOT\" = true ]; then\n\t\tbreak\n\tfi\ndone\n\n# Handle final cleanup and exit\nhandle_exit\n"
    },
    {
      "filename": "deepgram_transcribe.py",
      "path": "/bin/",
      "content": "import os\nimport argparse\nimport time\nimport subprocess\nfrom dotenv import load_dotenv\nfrom deepgram import DeepgramClient, PrerecordedOptions, FileSource, DeepgramError\nfrom groq import Groq\nimport sys\n\nload_dotenv()\n\nMAX_FILE_SIZE = 25 * 1024 * 1024  # 25MB in bytes\nAPI_CALL_DELAY = 1  # Delay between API calls in seconds\n\ndef preprocess_audio(input_file, output_file):\n    \"\"\"Convert, apply HPF, and normalize audio file to M4A format.\"\"\"\n    command = [\n        \"ffmpeg\", \"-i\", input_file,\n        \"-af\", \"highpass=f=60, acompressor=threshold=-12dB:ratio=4:attack=5:release=50, loudnorm\",\n        \"-ar\", \"16000\", \"-ac\", \"1\",\n        \"-c:a\", \"aac\", \"-b:a\", \"128k\",  # Use AAC codec for M4A\n        \"-progress\", \"pipe:1\",  # Output progress information to stdout\n        output_file\n    ]\n    \n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n    \n    duration = None\n    for line in process.stdout:\n        if line.startswith(\"Duration: \"):\n            duration = line.split()[1].strip(',')\n        if line.startswith(\"out_time=\"):\n            current_time = line.split('=')[1].strip()\n            if duration:\n                progress = (time_to_seconds(current_time) / time_to_seconds(duration)) * 100\n                print_progress_bar(progress)\n    \n    process.wait()\n    print()  # New line after progress bar\n\ndef time_to_seconds(time_str):\n    h, m, s = time_str.split(':')\n    return int(h) * 3600 + int(m) * 60 + float(s)\n\ndef print_progress_bar(progress):\n    bar_length = 50\n    filled_length = int(bar_length * progress // 100)\n    bar = '=' * filled_length + '-' * (bar_length - filled_length)\n    sys.stdout.write(f'\\rProgress: [{bar}] {progress:.1f}%')\n    sys.stdout.flush()\n\ndef transcribe_file_deepgram(client, file_path, options, max_retries=3, retry_delay=5):\n    for attempt in range(max_retries):\n        try:\n            with open(file_path, \"rb\") as audio:\n                buffer_data = audio.read()\n                payload: FileSource = {\n                    \"buffer\": buffer_data,\n                    \"mimetype\": \"audio/mp4\"  # Changed to audio/mp4 for M4A files\n                }\n                response = client.listen.rest.v(\"1\").transcribe_file(payload, options)\n            return response\n        except DeepgramError as e:\n            if attempt < max_retries - 1:\n                print(f\"API call failed. Retrying in {retry_delay} seconds... (Attempt {attempt + 1}/{max_retries})\")\n                time.sleep(retry_delay)\n            else:\n                raise e\n        except Exception as e:\n            raise e\n\ndef transcribe_file_groq(client, file_path, model=\"whisper-large-v3\", language=\"en\"):\n    with open(file_path, \"rb\") as file:\n        transcription = client.audio.transcriptions.create(\n            file=(os.path.basename(file_path), file.read()),\n            model=model,\n            language=language,\n            response_format=\"text\"\n        )\n    return transcription\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Transcribe audio files in a folder using Deepgram or Groq API.\")\n    parser.add_argument(\"folder_path\", help=\"Path to the folder containing audio files\")\n    parser.add_argument(\"--api\", choices=[\"deepgram\", \"groq\"], default=\"deepgram\", help=\"Choose API: deepgram or groq\")\n    parser.add_argument(\"--delay\", type=float, default=API_CALL_DELAY, help=\"Delay between API calls in seconds\")\n    args = parser.parse_args()\n\n    try:\n        # Initialize both clients\n        deepgram_key = os.getenv(\"DG_API_KEY\")\n        groq_key = os.getenv(\"GROQ_API_KEY\")\n\n        if not deepgram_key:\n            raise ValueError(\"DG_API_KEY environment variable is not set\")\n        if not groq_key and args.api == \"groq\":\n            raise ValueError(\"GROQ_API_KEY environment variable is not set\")\n\n        deepgram_client = DeepgramClient(deepgram_key)\n        groq_client = Groq(api_key=groq_key) if groq_key else None\n\n        deepgram_options = PrerecordedOptions(\n            model=\"nova-2\",\n            smart_format=True,\n            language=\"en\",\n            punctuate=True,\n            utterances=True,\n            diarize=True,\n        )\n\n        for filename in os.listdir(args.folder_path):\n            if filename.lower().endswith(('.wav', '.mp3', '.ogg', '.opus', '.m4a')):\n                file_path = os.path.join(args.folder_path, filename)\n                print(f\"Processing {filename}...\")\n                \n                try:\n                    # Preprocess the audio file\n                    preprocessed_file = os.path.join(args.folder_path, f\"preprocessed_{filename}.m4a\")\n                    preprocess_audio(file_path, preprocessed_file)\n                    \n                    # Check file size after preprocessing\n                    file_size = os.path.getsize(preprocessed_file)\n                    \n                    if file_size > MAX_FILE_SIZE or args.api == \"deepgram\":\n                        response = transcribe_file_deepgram(deepgram_client, preprocessed_file, deepgram_options)\n                        transcription = response.to_json(indent=4)\n                        api_used = \"deepgram\"\n                    else:  # Groq\n                        transcription = transcribe_file_groq(groq_client, preprocessed_file)\n                        api_used = \"groq\"\n                    \n                    # Save the transcription to a text file\n                    output_filename = os.path.splitext(filename)[0] + f\"_{api_used}_transcription.txt\"\n                    output_path = os.path.join(args.folder_path, output_filename)\n                    with open(output_path, \"w\") as output_file:\n                        output_file.write(transcription)\n                    \n                    print(f\"Transcription saved to {output_filename} using {api_used} API\")\n                    \n                    # Remove the preprocessed file\n                    os.remove(preprocessed_file)\n                    \n                    # Sleep to respect rate limits\n                    time.sleep(args.delay)\n                \n                except (DeepgramError, Exception) as e:\n                    print(f\"Error while processing {filename}: {str(e)}\")\n                    if os.path.exists(preprocessed_file):\n                        os.remove(preprocessed_file)\n\n    except Exception as e:\n        print(f\"Fatal error: {str(e)}\")\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "filename": "video_topic_splitter.py",
      "path": "/bin/",
      "content": "#!/usr/bin/env python\n\nimport os\nimport subprocess\nimport argparse\nimport json\nfrom dotenv import load_dotenv\nfrom deepgram import DeepgramClient, PrerecordedOptions, FileSource, DeepgramError\nfrom groq import Groq\nimport sys\nimport time\nfrom pydub import AudioSegment\nimport spacy\nfrom gensim import corpora\nfrom gensim.models import LdaMulticore\nimport videogrep\nfrom moviepy.editor import VideoFileClip\nimport progressbar\nimport google.generativeai as genai\nfrom PIL import Image\n\nload_dotenv()\n\n# Load spaCy model\nnlp = spacy.load(\"en_core_web_sm\")\n\n\ndef create_project_folder(input_path, base_output_dir):\n    base_name = os.path.splitext(os.path.basename(input_path))[0]\n    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n    project_name = f\"{base_name}_{timestamp}\"\n    project_path = os.path.join(base_output_dir, project_name)\n    os.makedirs(project_path, exist_ok=True)\n    return project_path\n\n\ndef convert_to_mono_and_resample(input_file, output_file, sample_rate=16000):\n    \"\"\"Converts audio to mono, resamples, applies gain control, and a high-pass filter.\"\"\"\n    try:\n        command = [\n            \"ffmpeg\",\n            \"-i\",\n            input_file,\n            \"-af\",\n            \"highpass=f=200, acompressor=threshold=-12dB:ratio=4:attack=5:release=50, loudnorm\",\n            \"-ar\",\n            str(sample_rate),\n            \"-ac\",\n            \"1\",\n            \"-c:a\",\n            \"aac\",\n            \"-b:a\",\n            \"128k\",  # Use AAC codec for M4A\n            output_file,\n        ]\n        subprocess.run(command, check=True)\n        return {\n            \"status\": \"success\",\n            \"message\": f\"Audio converted to mono, resampled to {sample_rate}Hz, gain-adjusted, high-pass filtered, and saved to {output_file}\",\n        }\n    except subprocess.CalledProcessError as e:\n        return {\n            \"status\": \"error\",\n            \"message\": f\"Error during audio conversion: {str(e)}\",\n        }\n\n\ndef normalize_audio(input_file, output_file, lowpass_freq=8000, highpass_freq=100):\n    \"\"\"Normalizes audio using ffmpeg-normalize.\"\"\"\n    try:\n        command = [\n            \"ffmpeg-normalize\",\n            \"-pr\",  # Preserve ReplayGain tags\n            \"-tp\",\n            \"-3.0\",\n            \"-nt\",\n            \"rms\",\n            input_file,\n            \"-prf\",\n            f\"highpass=f={highpass_freq}\",\n            \"-prf\",\n            \"dynaudnorm=p=0.4:s=15\",\n            \"-pof\",\n            f\"lowpass=f={lowpass_freq}\",\n            \"-ar\",\n            \"48000\",\n            \"-c:a\",\n            \"pcm_s16le\",\n            \"--keep-loudness-range-target\",\n            \"-o\",\n            output_file,\n        ]\n        subprocess.run(command, check=True)\n        return {\n            \"status\": \"success\",\n            \"message\": f\"Audio normalized and saved to {output_file}\",\n        }\n    except subprocess.CalledProcessError as e:\n        return {\n            \"status\": \"error\",\n            \"message\": f\"Error during audio normalization: {str(e)}\",\n        }\n\n\ndef remove_silence(input_file, output_file, duration=\"1.5\", threshold=\"-25\"):\n    \"\"\"Removes silence from audio using unsilence.\"\"\"\n    try:\n        command = [\n            \"unsilence\",\n            \"-y\",  # non-interactive mode\n            \"-d\",  # Delete silent parts\n            \"-ss\",\n            duration,  # Minimum silence duration\n            \"-sl\",\n            threshold,  # Silence threshold\n            input_file,\n            output_file,\n        ]\n        subprocess.run(command, check=True)\n        return {\n            \"status\": \"success\",\n            \"message\": f\"Silence removed from audio and saved to {output_file}\",\n        }\n    except subprocess.CalledProcessError as e:\n        return {\"status\": \"error\", \"message\": f\"Error during silence removal: {str(e)}\"}\n\n\ndef extract_audio(video_path, output_path):\n    print(\"Extracting audio from video...\")\n    try:\n        audio = AudioSegment.from_file(video_path)\n        audio.export(output_path, format=\"wav\")\n        print(\"Audio extraction complete.\")\n    except Exception as e:\n        print(f\"Error during audio extraction: {str(e)}\")\n        raise\n\n\ndef transcribe_file_deepgram(client, file_path, options, max_retries=3, retry_delay=5):\n    print(\"Transcribing audio using Deepgram...\")\n    for attempt in range(max_retries):\n        try:\n            with open(file_path, \"rb\") as audio:\n                buffer_data = audio.read()\n                payload: FileSource = {\"buffer\": buffer_data, \"mimetype\": \"audio/mp4\"}\n                response = client.listen.rest.v(\"1\").transcribe_file(payload, options)\n            print(\"Transcription complete.\")\n            return json.loads(response.to_json())\n        except DeepgramError as e:\n            if attempt < max_retries - 1:\n                print(\n                    f\"API call failed. Retrying in {retry_delay} seconds... (Attempt {attempt + 1}/{max_retries})\"\n                )\n                time.sleep(retry_delay)\n            else:\n                print(f\"Transcription failed after {max_retries} attempts: {str(e)}\")\n                raise\n        except Exception as e:\n            print(f\"Unexpected error during transcription: {str(e)}\")\n            raise\n\n\ndef transcribe_file_groq(\n    client, file_path, model=\"whisper-large-v3\", language=\"en\", prompt=None\n):\n    print(\"Transcribing audio using Groq...\")\n    try:\n        with open(file_path, \"rb\") as file:\n            transcription = client.audio.transcriptions.create(\n                file=(file_path, file.read()),\n                model=model,\n                prompt=prompt,\n                response_format=\"verbose_json\",\n                language=language,\n                temperature=0.2\n            )\n        print(\"Transcription complete.\")\n        return json.loads(transcription.text)\n    except Exception as e:\n        print(f\"Error during Groq transcription: {str(e)}\")\n        raise\n\n\ndef save_transcription(transcription, project_path):\n    transcription_path = os.path.join(project_path, \"transcription.json\")\n    with open(transcription_path, \"w\") as f:\n        json.dump(transcription, f, indent=2)\n    print(f\"Transcription saved to: {transcription_path}\")\n\ndef save_transcript(transcript, project_path):\n    transcript_path = os.path.join(project_path, \"transcript.json\")\n    with open(transcript_path, \"w\") as f:\n        json.dump(transcript, f, indent=2)\n    print(f\"Transcript saved to: {transcript_path}\")\n\n\ndef load_transcript(transcript_path):\n    with open(transcript_path, \"r\") as f:\n        return json.load(f)\n\n\ndef preprocess_text(text):\n    print(\"Preprocessing text...\")\n    doc = nlp(text)\n    subjects = []\n\n    for sent in doc.sents:\n        for token in sent:\n            if \"subj\" in token.dep_:\n                if token.dep_ in [\"nsubj\", \"nsubjpass\", \"csubj\"]:\n                    subject = get_compound_subject(token)\n                    subjects.append(subject)\n\n    cleaned_subjects = [\n        [\n            token.lemma_.lower()\n            for token in nlp(subject)\n            if not token.is_stop and not token.is_punct and token.is_alpha\n        ]\n        for subject in subjects\n    ]\n\n    cleaned_subjects = [\n        list(s) for s in set(tuple(sub) for sub in cleaned_subjects) if s\n    ]\n\n    print(\n        f\"Text preprocessing complete. Extracted {len(cleaned_subjects)} unique subjects.\"\n    )\n    return cleaned_subjects\n\n\ndef get_compound_subject(token):\n    subject = [token.text]\n    for left_token in token.lefts:\n        if left_token.dep_ == \"compound\":\n            subject.insert(0, left_token.text)\n    for right_token in token.rights:\n        if right_token.dep_ == \"compound\":\n            subject.append(right_token.text)\n    return \" \".join(subject)\n\n\ndef perform_topic_modeling(subjects, num_topics=5):\n    print(f\"Performing topic modeling with {num_topics} topics...\")\n    dictionary = corpora.Dictionary(subjects)\n    corpus = [dictionary.doc2bow(subject) for subject in subjects]\n    lda_model = LdaMulticore(\n        corpus=corpus,\n        id2word=dictionary,\n        num_topics=num_topics,\n        random_state=100,\n        chunksize=100,\n        passes=10,\n        per_word_topics=True,\n    )\n    print(\"Topic modeling complete.\")\n    return lda_model, corpus, dictionary\n\n\ndef identify_segments(transcript, lda_model, dictionary, num_topics):\n    print(\"Identifying segments based on topics...\")\n    segments = []\n    current_segment = {\"start\": 0, \"end\": 0, \"content\": \"\", \"topic\": None}\n\n    for sentence in progressbar.progressbar(transcript):\n        subjects = preprocess_text(sentence[\"content\"])\n        if not subjects:\n            continue\n\n        bow = dictionary.doc2bow([token for subject in subjects for token in subject])\n        topic_dist = lda_model.get_document_topics(bow)\n        dominant_topic = max(topic_dist, key=lambda x: x[1])[0] if topic_dist else None\n\n        if dominant_topic != current_segment[\"topic\"]:\n            if current_segment[\"content\"]:\n                current_segment[\"end\"] = sentence[\"start\"]\n                segments.append(current_segment)\n            current_segment = {\n                \"start\": sentence[\"start\"],\n                \"end\": sentence[\"end\"],\n                \"content\": sentence[\"content\"],\n                \"topic\": dominant_topic,\n            }\n        else:\n            current_segment[\"end\"] = sentence[\"end\"]\n            current_segment[\"content\"] += \" \" + sentence[\"content\"]\n\n    if current_segment[\"content\"]:\n        segments.append(current_segment)\n\n    print(f\"Identified {len(segments)} segments.\")\n    return segments\n\n\ndef generate_metadata(segments, lda_model):\n    print(\"Generating metadata for segments...\")\n    metadata = []\n    for i, segment in enumerate(progressbar.progressbar(segments)):\n        segment_metadata = {\n            \"segment_id\": i + 1,\n            \"start_time\": segment[\"start\"],\n            \"end_time\": segment[\"end\"],\n            \"duration\": segment[\"end\"] - segment[\"start\"],\n            \"dominant_topic\": segment[\"topic\"],\n            \"top_keywords\": [\n                word for word, _ in lda_model.show_topic(segment[\"topic\"], topn=5)\n            ],\n            \"transcript\": segment[\"content\"],\n        }\n        metadata.append(segment_metadata)\n    print(\"Metadata generation complete.\")\n    return metadata\n\n\ndef process_transcript(transcript, project_path, num_topics=5):\n    full_text = \" \".join([sentence[\"content\"] for sentence in transcript])\n    preprocessed_subjects = preprocess_text(full_text)\n    lda_model, corpus, dictionary = perform_topic_modeling(\n        preprocessed_subjects, num_topics\n    )\n\n    segments = identify_segments(transcript, lda_model, dictionary, num_topics)\n    metadata = generate_metadata(segments, lda_model)\n\n    results = {\n        \"topics\": [\n            {\n                \"topic_id\": topic_id,\n                \"words\": [word for word, _ in lda_model.show_topic(topic_id, topn=10)],\n            }\n            for topic_id in range(num_topics)\n        ],\n        \"segments\": metadata,\n    }\n\n    results_path = os.path.join(project_path, \"results.json\")\n    with open(results_path, \"w\") as f:\n        json.dump(results, f, indent=2)\n    print(f\"Results saved to: {results_path}\")\n\n    return results\n\n\ndef analyze_segment_with_gemini(segment_path, transcript):\n    print(f\"Analyzing segment: {segment_path}\")\n    # Set up the Gemini model\n    genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n\n    # Load the video segment as an image (first frame)\n    video = VideoFileClip(segment_path)\n    frame = video.get_frame(0)\n    image = Image.fromarray(frame)\n    video.close()\n\n    # Prepare the prompt\n    prompt = f\"Analyze this video segment. The transcript for this segment is: '{transcript}'. Describe the main subject matter, key visual elements, and how they relate to the transcript.\"\n\n    # Generate content\n    response = model.generate_content([prompt, image])\n\n    return response.text\n\n\ndef split_and_analyze_video(input_video, segments, output_dir):\n    print(\"Splitting video into segments and analyzing...\")\n    try:\n        video = VideoFileClip(input_video)\n        analyzed_segments = []\n\n        for i, segment in enumerate(progressbar.progressbar(segments)):\n            start_time = segment[\"start_time\"]\n            end_time = segment[\"end_time\"]\n            segment_clip = video.subclip(start_time, end_time)\n            output_path = os.path.join(output_dir, f\"segment_{i+1}.mp4\")\n            segment_clip.write_videofile(\n                output_path, codec=\"libx264\", audio_codec=\"aac\"\n            )\n\n            # Analyze the segment with Gemini\n            gemini_analysis = analyze_segment_with_gemini(\n                output_path, segment[\"transcript\"]\n            )\n\n            analyzed_segments.append(\n                {\n                    \"segment_id\": i + 1,\n                    \"start_time\": start_time,\n                    \"end_time\": end_time,\n                    \"transcript\": segment[\"transcript\"],\n                    \"topic\": segment[\"dominant_topic\"],\n                    \"keywords\": segment[\"top_keywords\"],\n                    \"gemini_analysis\": gemini_analysis,\n                }\n            )\n\n        video.close()\n        print(\"Video splitting and analysis complete.\")\n        return analyzed_segments\n    except Exception as e:\n        print(f\"Error during video splitting and analysis: {str(e)}\")\n        raise\n\n\ndef handle_audio_video(video_path, project_path):\n    audio_dir = os.path.join(project_path, \"audio\")\n    os.makedirs(audio_dir, exist_ok=True)\n\n    normalized_video_path = os.path.join(project_path, \"normalized_video.mkv\")\n    normalize_result = normalize_audio(video_path, normalized_video_path)\n    if normalize_result[\"status\"] == \"error\":\n        print(f\"Error during audio normalization: {normalize_result['message']}\")\n        # Handle the error (e.g., exit or continue without normalization)\n    else:\n        print(normalize_result[\"message\"])\n\n    # Remove silence\n    unsilenced_video_path = os.path.join(project_path, \"unsilenced_video.mkv\")\n    silence_removal_result = remove_silence(\n        normalized_video_path, unsilenced_video_path\n    )\n    if silence_removal_result[\"status\"] == \"error\":\n        print(f\"Error during silence removal: {silence_removal_result['message']}\")\n        # Handle the error (e.g., exit or continue without silence removal)\n    else:\n        print(silence_removal_result[\"message\"])\n\n    # Extract audio from unsilenced video\n    raw_audio_path = os.path.join(audio_dir, \"extracted_audio.wav\")\n    extract_audio(unsilenced_video_path, raw_audio_path)\n\n    # Convert to mono and resample for transcription\n    mono_resampled_audio_path = os.path.join(audio_dir, \"mono_resampled_audio.m4a\")\n    conversion_result = convert_to_mono_and_resample(\n        raw_audio_path, mono_resampled_audio_path\n    )\n    if conversion_result[\"status\"] == \"error\":\n        print(f\"Error during audio conversion: {conversion_result['message']}\")\n        # Handle the error (e.g., exit or continue without conversion)\n    else:\n        print(conversion_result[\"message\"])\n\n    return unsilenced_video_path, mono_resampled_audio_path\n\n\ndef handle_transcription(\n    video_path, audio_path, project_path, api=\"deepgram\", num_topics=2, groq_prompt=None\n):\n    segments_dir = os.path.join(project_path, \"segments\")\n    os.makedirs(segments_dir, exist_ok=True)\n\n    print(\"Parsing transcript with Videogrep...\")\n    transcript = videogrep.parse_transcript(video_path)\n    print(\"Transcript parsing complete.\")\n\n    if not transcript:\n        print(\"No transcript found. Transcribing audio...\")\n        deepgram_key = os.getenv(\"DG_API_KEY\")\n        groq_key = os.getenv(\"GROQ_API_KEY\")\n\n        if not deepgram_key:\n            raise ValueError(\"DG_API_KEY environment variable is not set\")\n        if not groq_key and api == \"groq\":\n            raise ValueError(\"GROQ_API_KEY environment variable is not set\")\n\n        if api == \"deepgram\":\n            deepgram_client = DeepgramClient(deepgram_key)\n            deepgram_options = PrerecordedOptions(\n                model=\"nova-2\",\n                language=\"en\",\n                topics=True,\n                intents=True,\n                smart_format=True,\n                punctuate=True,\n                paragraphs=True,\n                utterances=True,\n                diarize=True,\n                filler_words=True,\n                sentiment=True,\n            )\n            # Transcribe the normalized audio\n            transcription = transcribe_file_deepgram(\n                deepgram_client, audio_path, deepgram_options\n            )\n            transcript = [\n                {\n                    \"content\": utterance[\"transcript\"],\n                    \"start\": utterance[\"start\"],\n                    \"end\": utterance[\"end\"],\n                }\n                for utterance in transcription[\"results\"][\"utterances\"]\n            ]\n        else:  # Groq\n            groq_client = Groq(api_key=groq_key)\n            # Transcribe the normalized audio\n            transcription = transcribe_file_groq(\n                groq_client, audio_path, prompt=groq_prompt\n            )\n            transcript = [\n                {\n                    \"content\": segment[\"text\"],\n                    \"start\": segment[\"start\"],\n                    \"end\": segment[\"end\"],\n                }\n                for segment in transcription[\"segments\"]\n            ]\n\n    save_transcription(transcription, project_path)\n\n    save_transcript(transcript, project_path)\n    results = process_transcript(transcript, project_path, num_topics)\n\n    # Split the video and analyze segments\n    analyzed_segments = split_and_analyze_video(\n        video_path, results[\"segments\"], segments_dir\n    )\n\n    # Update results with analyzed segments\n    results[\"analyzed_segments\"] = analyzed_segments\n\n    # Save updated results\n    results_path = os.path.join(project_path, \"results.json\")\n    with open(results_path, \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # print(\"Cleaning up temporary files...\")\n    # os.remove(raw_audio_path)\n    # os.remove(normalized_audio_path) # Uncomment to remove normalized audio\n\n    return results\n\n\ndef process_video(\n    video_path, project_path, api=\"deepgram\", num_topics=2, groq_prompt=None\n):\n\n    unsilenced_video_path, mono_resampled_audio_path = handle_audio_video(\n        video_path, project_path\n    )\n    results = handle_transcription(\n        unsilenced_video_path, mono_resampled_audio_path, project_path, api, num_topics, groq_prompt\n    )\n\n    return results\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Process video or transcript for topic-based segmentation and multi-modal analysis\"\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--input\",\n        required=True,\n        help=\"Path to the input video file or transcript JSON\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output\",\n        default=os.getcwd(),\n        help=\"Base output directory for project folders\",\n    )\n    parser.add_argument(\n        \"--api\",\n        choices=[\"deepgram\", \"groq\"],\n        default=\"deepgram\",\n        help=\"Choose API: deepgram or groq\",\n    )\n    parser.add_argument(\n        \"--topics\", type=int, default=5, help=\"Number of topics for LDA model\"\n    )\n    parser.add_argument(\"--groq-prompt\", help=\"Optional prompt for Groq transcription\")\n    args = parser.parse_args()\n\n    project_path = create_project_folder(args.input, args.output)\n    print(f\"Created project folder: {project_path}\")\n\n    try:\n        if args.input.endswith(\".json\"):\n            transcript = load_transcript(args.input)\n            results = process_transcript(transcript, project_path, args.topics)\n            print(\n                \"Note: Video splitting and Gemini analysis are not performed when processing a transcript file.\"\n            )\n        else:\n            results = process_video(\n                args.input, project_path, args.api, args.topics, args.groq_prompt\n            )\n\n        print(f\"\\nProcessing complete. Project folder: {project_path}\")\n        print(f\"Results saved in: {os.path.join(project_path, 'results.json')}\")\n        print(\"\\nTop words for each topic:\")\n        for topic in results[\"topics\"]:\n            print(f\"Topic {topic['topic_id'] + 1}: {', '.join(topic['words'])}\")\n        print(f\"\\nGenerated and analyzed {len(results['analyzed_segments'])} segments\")\n\n        if not args.input.endswith(\".json\"):\n            print(f\"Video segments saved in: {os.path.join(project_path, 'segments')}\")\n    except Exception as e:\n        print(f\"\\nAn error occurred during processing: {str(e)}\")\n        print(\"Please check the project folder for any partial results or logs.\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "filename": "notebook_export.py",
      "path": "/bin/",
      "content": "import os\nimport shutil\nimport re\nfrom pathlib import Path\nimport argparse\nimport urllib.parse\n\ndef get_asset_subfolder(file_path):\n    \"\"\"Determine the appropriate assets subfolder based on file extension\"\"\"\n    extension = file_path.lower().split('.')[-1]\n    \n    if extension in ['png', 'jpg', 'jpeg', 'gif', 'bmp', 'svg', 'webp']:\n        return 'img'\n    elif extension in ['mp3', 'wav', 'ogg', 'm4a', 'flac']:\n        return 'audio'\n    elif extension in ['mp4', 'mov', 'avi', 'mkv', 'webm']:\n        return 'video'\n    elif extension == 'pdf':\n        return 'pdf'\n    else:\n        return 'other'\n\ndef extract_title_from_path(path):\n    \"\"\"Extract the title from a markdown file path\"\"\"\n    # Remove the .md extension if present\n    base_name = os.path.basename(path)\n    if base_name.lower().endswith('.md'):\n        base_name = base_name[:-3]\n    \n    # URL decode the name\n    decoded_name = urllib.parse.unquote(base_name)\n    return decoded_name\n\ndef copy_markdown_files(source, destination_folder):\n    source = os.path.expanduser(source)\n    destination_folder = os.path.expanduser(destination_folder)\n    \n    if os.path.isdir(source):\n        base_folder_name = os.path.basename(os.path.normpath(source))\n        notes_folder = os.path.join(destination_folder, base_folder_name)\n    else:\n        notes_folder = destination_folder\n    \n    os.makedirs(notes_folder, exist_ok=True)\n    \n    root_destination = os.path.dirname(destination_folder.rstrip(os.path.sep))\n    assets_root = os.path.join(root_destination, 'assets')\n    \n    asset_subfolders = ['img', 'audio', 'video', 'pdf', 'other']\n    for subfolder in asset_subfolders:\n        os.makedirs(os.path.join(assets_root, subfolder), exist_ok=True)\n    \n    def process_file(file_path):\n        source_folder = os.path.dirname(file_path)\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n        \n        # Process existing double-bracketed links first\n        backlinks = re.findall(r'\\[\\[(.*?)\\]\\]', content)\n        for backlink in backlinks:\n            backlink_path = os.path.join(source_folder, f\"{backlink}.md\")\n            if os.path.exists(backlink_path):\n                process_file(backlink_path)\n        \n        # Convert markdown links to double brackets if they point to .md files\n        def convert_markdown_link(match):\n            text = match.group(1)\n            link = match.group(2)\n            \n            # Check if it's a markdown file link\n            if link.lower().endswith('.md'):\n                # Extract the title from the path\n                title = extract_title_from_path(link)\n                # Process the linked file if it exists\n                full_link_path = os.path.join(source_folder, link)\n                if os.path.exists(full_link_path):\n                    process_file(full_link_path)\n                return f\"[[{title}]]\"\n            \n            # Return original link if it's not a markdown file\n            return match.group(0)\n        \n        # Convert markdown links to double brackets\n        content = re.sub(r'\\[(.*?)\\]\\((.*?)\\)', convert_markdown_link, content)\n        \n        # Process attachments and wrap images with liquid tags\n        def replace_attachment(match):\n            alt_text, attachment_path = match.groups()\n            full_attachment_path = os.path.join(source_folder, attachment_path)\n            \n            if os.path.exists(full_attachment_path):\n                asset_subfolder = get_asset_subfolder(attachment_path)\n                new_attachment_name = os.path.basename(attachment_path)\n                new_attachment_path = f'/assets/{asset_subfolder}/{new_attachment_name}'\n                dest_asset_path = os.path.join(root_destination, new_attachment_path.lstrip('/'))\n                shutil.copy2(full_attachment_path, dest_asset_path)\n                \n                if asset_subfolder == 'img':\n                    return f'{{% picture {new_attachment_name} --alt {alt_text} %}}'\n                else:\n                    return f'![{alt_text}]({new_attachment_path})'\n            return match.group(0)\n\n        # Only process image/attachment links (not already processed markdown links)\n        content = re.sub(r'!\\[(.*?)\\]\\((.*?)\\)', replace_attachment, content)\n        \n        # Process Mermaid and PlantUML code blocks\n        def process_code_blocks(match):\n            code_type = match.group(1).lower()\n            code_content = match.group(2)\n            \n            if code_type == 'mermaid':\n                return f'```mermaid!\\n{code_content}\\n```\\n\\nMermaid diagram detected. Consider rendering this diagram.'\n            elif code_type == 'plantuml':\n                return f'```plantuml!\\n{code_content}\\n```\\n\\nPlantUML diagram detected. Consider rendering this diagram.'\n            else:\n                return match.group(0)\n\n        content = re.sub(r'```(\\w+)\\n(.*?)```', process_code_blocks, content, flags=re.DOTALL)\n        \n        if os.path.isdir(source):\n            rel_path = os.path.relpath(file_path, source)\n            dest_path = os.path.join(notes_folder, rel_path)\n        else:\n            dest_path = os.path.join(notes_folder, os.path.basename(file_path))\n            \n        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n        \n        with open(dest_path, 'w', encoding='utf-8') as file:\n            file.write(content)\n    \n    def list_markdown_files(folder):\n        markdown_files = []\n        for root, _, files in os.walk(folder):\n            for file in files:\n                if file.endswith('.md'):\n                    markdown_files.append(os.path.join(root, file))\n        return markdown_files\n    \n    if os.path.isfile(source):\n        process_file(source)\n    elif os.path.isdir(source):\n        markdown_files = list_markdown_files(source)\n        for file_path in markdown_files:\n            process_file(file_path)\n    elif isinstance(source, list):\n        for file_path in source:\n            if os.path.isfile(file_path) and file_path.endswith('.md'):\n                process_file(file_path)\n    else:\n        raise ValueError(\"Invalid source. Must be a file, folder, or list of files.\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Copy Markdown files with double bracket link conversion.\")\n    parser.add_argument(\"source\", nargs='+', help=\"Source file(s) or folder\")\n    parser.add_argument(\"destination\", help=\"Destination folder\")\n    args = parser.parse_args()\n\n    source = args.source[0] if len(args.source) == 1 else args.source\n    copy_markdown_files(source, args.destination)\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
      "filename": "tmp-screenshot.sh",
      "path": "/bin/",
      "content": "#!/bin/bash\n\nset -e\n\nmkdir -p \"/tmp/screenshots\"\n\nFILE=\"/tmp/screenshots/$(date +%s).png\"\n\nscreenshot-and-open \"$FILE\"\n"
    },
    {
      "filename": "tmp-screenshot-clip.sh",
      "path": "/bin/",
      "content": "#!/bin/bash\n\nset -e\n\nmaim -u -s | xclip -selection clipboard -t image/png\n"
    },
    {
      "filename": "autorandr.sh",
      "path": "/defunct/",
      "content": "#!/usr/bin/env bash\n\ndeclare -rx BACKGROUNDS=\"/usr/share/backgrounds/syncopated\"\n\nif [ -x \"$(command -v autorandr)\" ]; then\n  profile=$(autorandr --detected)\n  autorandr -l \"$profile\"\nelse\n  echo \"autorandr not found\"\nfi\n\nsleep 1\n\n#note: tilda does not expand in quotes\nif [ -f ~/.fehbg ]; then\n  ~/.fehbg\nelse\n  #feh --no-fehbg --bg-scale $BACKGROUNDS/crescendo_01.png $BACKGROUNDS/str_01_03_bw_01.png \n  feh --recursive --bg-fill --randomize $BACKGROUNDS/*\nfi\n"
    },
    {
      "filename": "jack-time-machine.sh",
      "path": "/defunct/",
      "content": "#!/usr/bin/env bash\n\n# Set your desired options\nBITDEPTH=\"16\"\nRATE=16000\nCHANNELS=1\nPORT=\"REAPER:out\"\nFILENAME_PREFIX=\"jack_capture_\"\nFORMAT=\"wav\"\nTIMEMACHINE_PREBUFFER=10\n\n# Specify the output directory with a timestamp\nOUTPUT_DIRECTORY=\"$HOME/Desktop/Dictations\"\nTIMESTAMP=$(date +\"%Y%m%d_%H%M%S\")\n\n# Function to stop recording via OSC\nstop_recording() {\n    oscsend localhost 7777 /jack_capture/tm/stop\n}\n\ncleanup() {\n  echo \"all set!\"\n  # Stop recording via OSC\n  stop_recording\n}\n\n# Function to handle XF86AudioRecord events\nhandle_record_key() {\n  # Get current recording state from jack_capture (you might need to adjust this command)\n  # For example, you could use `jack_lsp` to check if the client is running\n  is_recording=$(jack_lsp | grep jack_capture)\n\n  if [[ -n \"$is_recording\" ]]; then\n    # Recording is running, so pause or stop it\n    echo \"XF86AudioRecord key pressed. Pausing/Stopping recording...\"\n    stop_recording  # Call your existing stop_recording function\n  else\n    echo \"XF86AudioRecord key pressed, but recording is not running.\"\n  fi\n}\n\n\n\ntrap cleanup SIGINT SIGTERM ERR EXIT\n# Start jack_capture as a daemon with specified options and output directory\njack_capture --bitdepth $BITDEPTH --channels $CHANNELS  --filename-prefix $FILENAME_PREFIX --format $FORMAT --port \"${PORT}1\" --port \"${PORT}2\" --timemachine --timemachine-prebuffer $TIMEMACHINE_PREBUFFER --filename \"$OUTPUT_DIRECTORY/$FILENAME_PREFIX$TIMESTAMP.$FORMAT\" --osc 7777 --daemon &\n\n\n# Get the process ID of the jack_capture daemon\nPID=$!\n\n# Wait for user input to stop recording (you can customize this part)\n# Use `xev` to determine the keycode for XF86AudioRecord\n# Run `xev` in a terminal and press the XF86AudioRecord key.\n# Note the keycode from the output (e.g., 173).\nrecord_keycode=175  # Replace with the actual keycode\n\n# Set up an event listener using `xinput`\nxinput test-xi2 --root | while read -r line; do\n  if [[ \"$line\" =~ \"keycode\\ $record_keycode\\ \" ]]; then\n    handle_record_key\n  fi\ndone &\n\n# Wait for jack_capture to finish and close gracefully\nwait $PID\n"
    },
    {
      "filename": "search_devdocs.sh",
      "path": "/defunct/",
      "content": "#!/bin/bash\n# A quick documentation finder based on rofi and devdocs\n# Requires: rofi, devdocs, i3-sensible-terminal, qutebrowser nerdfonts\nfiles=~/.cache/search_devdocs_history\n\ndir=\"$HOME/.config/rofi/launchers/type-1\"\ntheme=\"style-10\"\n\n## Run\nrofi=$()\n\nappend_new_term() {\n\t# Delete term. Append on the first line.\n\tsed -i \"/$input/d\" $files\n\tsed -i \"1i $input\" \"$files\"\n\t# Max cache limited to 20 entries: https://github.com/Zeioth/rofi-devdocs/issues/3\n\tsed -i 20d \"$files\"\n}\n\n\nif [ -e $files ]; then\n\t# If file list exist, use it\n\tinput=$(cat $files | rofi -dmenu -theme ${dir}/${theme}.rasi -p \"query:> \")\n\telse\n\n\t# There is no file list, create it and show menu only after that\n\ttouch $files\n\tinput=$(cat $files | rofi -dmenu -theme ${dir}/${theme}.rasi -p \"query:> \")\n\n\t#\tThe file if empty, initialize it, so we can insert on the top later\n  if [ ! -s \"$_file\" ]\n  then\n    echo \" \" > \"$files\"\n  fi\nfi\n\n# docs=(\"Ansible\" \"Bash\" \"Bootstrap\" \"CSS\" \"Docker\" \"HTML\" \"i3\" \"JavaScript\" \"Jekyll\" \"Jinja\" \"Ruby\" \"Sass\" \"Web APIs\")\n# doc=\"$(echo $input | cut -d \" \" -f -1|xargs -0)\"\n# query=\"$(echo $input | cut -d \" \" -f 2-)\"\n\nappend_new_term\n\n# exec devdocs-desktop \"$(echo ${doc})\" \"$(echo ${query})\" &> /dev/null &\n\nexec devdocs-desktop \"$(echo -e ${input})\" &> /dev/null &\n"
    },
    {
      "filename": "process_videov2.py",
      "path": "/defunct/",
      "content": "#!/usr/bin/env python\nimport os\nimport shutil\nimport subprocess\nimport argparse\nimport psutil\nimport tempfile\nimport time\nfrom unsilence import Unsilence\nfrom pydub import AudioSegment  # Import AudioSegment\n\n# Configuration constants\nMAX_RETRIES = 3  # Maximum number of retry attempts for Whisper transcription\nRETRY_DELAY = 5  # Delay in seconds between retry attempts\n\ndef run_whisper(input_file, output_file, verbose=False):\n    \"\"\"\n    Transcribe audio using the Whisper model.\n    \n    Args:\n    input_file (str): Path to the input audio file.\n    output_file (str): Path to save the transcription output.\n    verbose (bool): If True, print detailed progress information.\n    \n    Returns:\n    None\n    \"\"\"\n    for attempt in range(MAX_RETRIES):\n        try:\n            if verbose:\n                print(f\"Attempt {attempt + 1} to transcribe using Whisper\")\n            subprocess.run([\"whisper.cpp\", input_file, \"-o\", output_file], check=True)\n            return  # Success, exit the function\n        except subprocess.CalledProcessError as e:\n            if verbose:\n                print(f\"Whisper transcription failed: {e}\")\n            if attempt < MAX_RETRIES - 1:\n                if verbose:\n                    print(f\"Retrying in {RETRY_DELAY} seconds...\")\n                time.sleep(RETRY_DELAY)\n            else:\n                print(f\"Failed to transcribe after {MAX_RETRIES} attempts. Skipping transcription.\")\n                # Create an empty file to indicate transcription was attempted but failed\n                with open(output_file, 'w') as f:\n                    f.write(\"Transcription failed after multiple attempts.\")\n\ndef run_sonic_annotator(input_file, output_dir):\n    \"\"\"\n    Analyze audio features using Sonic Annotator.\n    \n    Args:\n    input_file (str): Path to the input audio file.\n    output_dir (str): Directory to save the analysis output.\n    \n    Returns:\n    None\n    \"\"\"\n    subprocess.run([\"sonic-annotator\", \"-d\", \"vamp:qm-vamp-plugins:qm-tempotracker:tempo\", \n                    \"-d\", \"vamp:qm-vamp-plugins:qm-keydetector:key\", \n                    \"-w\", \"csv\", \"--csv-force\", \n                    \"-o\", output_dir, input_file], check=True)\n\ndef transcode(input_file, output_file):\n    \"\"\"\n    Transcode video to MP4 format with specific encoding options.\n    \n    Args:\n    input_file (str): Path to the input video file.\n    output_file (str): Path to save the transcoded video.\n    \n    Returns:\n    None\n    \"\"\"\n    subprocess.run([\"ffmpeg\", \"-i\", input_file, \"-c:v\", \"libx264\", \"-crf\", \"23\", \n                    \"-c:a\", \"aac\", \"-b:a\", \"128k\", output_file], check=True)\n\ndef extract_audio(input_file, output_file):\n    \"\"\"\n    Extract audio from a video file as WAV.\n    \n    Args:\n    input_file (str): Path to the input video file.\n    output_file (str): Path to save the extracted audio.\n    \n    Returns:\n    None\n    \"\"\"\n    subprocess.run([\"ffmpeg\", \"-i\", input_file, \"-vn\", \"-acodec\", \"pcm_s16le\", \n                    \"-ar\", \"44100\", \"-ac\", \"2\", output_file], check=True)\n\ndef estimate_silence(input_file):\n    \"\"\"\n    Estimate the percentage of silence in an audio file.\n    \n    Args:\n    input_file (str): Path to the input audio file.\n    \n    Returns:\n    float: Percentage of silence in the audio.\n    \"\"\"\n    u = Unsilence(input_file)\n    u.detect_silence()\n    \n    audio = AudioSegment.from_file(input_file)  # Load audio with pydub\n    total_duration = audio.duration_seconds  # Get duration in seconds\n    \n    silent_duration = sum(interval[1] - interval[0] for interval in u.silent_intervals)\n    silence_percentage = (silent_duration / total_duration) * 100\n    return silence_percentage\n\ndef unsilence_audio(input_file, output_file, audible_speed=1, silent_speed=8):\n    \"\"\"\n    Remove silence from an audio/video file if silence percentage is above 10%.\n    \n    Args:\n    input_file (str): Path to the input audio/video file.\n    output_file (str): Path to save the unsilenced audio/video.\n    audible_speed (int): Speed for audible segments (default: 1).\n    silent_speed (int): Speed for silent segments (default: 8).\n    \n    Returns:\n    None\n    \"\"\"\n    if input_file.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n        # Extract audio from video\n        temp_dir = tempfile.mkdtemp()\n        audio_file = os.path.join(temp_dir, \"temp_audio.wav\")\n        extract_audio(input_file, audio_file)\n\n        silence_percentage = estimate_silence(audio_file)\n        if silence_percentage > 10:\n            print(f\"Estimated silence: {silence_percentage:.2f}%. Applying unsilence.\")\n            unsilenced_audio_file = os.path.join(temp_dir, \"temp_unsilenced_audio.wav\")\n            subprocess.run([\"unsilence\", audio_file, unsilenced_audio_file, \n                            \"-as\", str(audible_speed), \"-ss\", str(silent_speed)], check=True)\n\n            # Replace audio in the video with the unsilenced audio\n            subprocess.run([\"ffmpeg\", \"-i\", input_file, \"-i\", unsilenced_audio_file, \n                            \"-map\", \"0:v\", \"-map\", \"1:a\", \"-c:v\", \"copy\", \"-c:a\", \"aac\", \n                            \"-b:a\", \"128k\", output_file], check=True)\n        else:\n            print(f\"Estimated silence: {silence_percentage:.2f}%. Skipping unsilence.\")\n            shutil.copy(input_file, output_file)\n\n        shutil.rmtree(temp_dir)\n    else:\n        # Process as audio file\n        silence_percentage = estimate_silence(input_file)\n        if silence_percentage > 10:\n            print(f\"Estimated silence: {silence_percentage:.2f}%. Applying unsilence.\")\n            subprocess.run([\"unsilence\", input_file, output_file, \n                            \"-as\", str(audible_speed), \"-ss\", str(silent_speed)], check=True)\n        else:\n            print(f\"Estimated silence: {silence_percentage:.2f}%. Skipping unsilence.\")\n            shutil.copy(input_file, output_file)\n\ndef normalize(input_file, output_file):\n    \"\"\"\n    Normalize audio levels using ffmpeg-normalize for audio/video files.\n    \n    Args:\n    input_file (str): Path to the input audio/video file.\n    output_file (str): Path to save the normalized audio/video.\n    \n    Returns:\n    None\n    \"\"\"\n    if input_file.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n        # Extract audio from video\n        temp_dir = tempfile.mkdtemp()\n        audio_file = os.path.join(temp_dir, \"temp_audio.wav\")\n        extract_audio(input_file, audio_file)\n\n        normalized_audio_file = os.path.join(temp_dir, \"temp_normalized_audio.wav\")\n        subprocess.run([\n            \"ffmpeg-normalize\", \"-pr\", \"-nt\", \"rms\", audio_file, \n            \"-prf\", \"highpass=f=200\", \"-prf\", \"dynaudnorm=p=0.4:s=15\", \"-pof\", \"lowpass=f=7000\", \n            \"-ar\", \"48000\", \"-c:a\", \"pcm_s16le\", \"--keep-loudness-range-target\", \n            \"-o\", normalized_audio_file\n        ], check=True)\n\n        # Replace audio in the video with the normalized audio\n        subprocess.run([\"ffmpeg\", \"-i\", input_file, \"-i\", normalized_audio_file, \n                        \"-map\", \"0:v\", \"-map\", \"1:a\", \"-c:v\", \"copy\", \"-c:a\", \"aac\", \n                        \"-b:a\", \"128k\", output_file], check=True)\n\n        shutil.rmtree(temp_dir)\n    else:\n        # Process as audio file\n        subprocess.run([\n            \"ffmpeg-normalize\", \"-pr\", \"-nt\", \"rms\", input_file, \n            \"-prf\", \"highpass=f=200\", \"-prf\", \"dynaudnorm=p=0.4:s=15\", \"-pof\", \"lowpass=f=7000\", \n            \"-ar\", \"48000\", \"-c:a\", \"pcm_s16le\", \"--keep-loudness-range-target\", \n            \"-o\", output_file\n        ], check=True)\n\ndef pipeline(input_file, output_dir, verbose=False):\n    \"\"\"\n    Execute the complete processing pipeline: normalization, silence removal,\n    audio extraction, transcription, and audio analysis.\n    \n    Args:\n    input_file (str): Path to the input video or audio file.\n    output_dir (str): Directory to save all output files.\n    verbose (bool): If True, print detailed progress information.\n    \n    Returns:\n    None\n    \"\"\"\n    base_name = os.path.splitext(os.path.basename(input_file))[0]\n    \n    # Normalize\n    normalized_file = os.path.join(output_dir, f\"{base_name}_normalized.wav\")\n    normalize(input_file, normalized_file)\n    \n    # Unsilence\n    unsilenced_file = os.path.join(output_dir, f\"{base_name}_unsilenced.wav\")\n    unsilence_audio(normalized_file, unsilenced_file)\n    \n    # Extract audio (if input is video)\n    if input_file.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n        audio_file = os.path.join(output_dir, f\"{base_name}.wav\")\n        extract_audio(input_file, audio_file)\n    else:\n        audio_file = unsilenced_file\n    \n    # Transcribe\n    transcription_file = os.path.join(output_dir, f\"{base_name}_transcription.txt\")\n    run_whisper(audio_file, transcription_file, verbose)\n    \n    # Analyze audio\n    run_sonic_annotator(audio_file, output_dir)\n\ndef move_files(source_dir, dest_dir):\n    \"\"\"\n    Move all files from source directory to destination directory.\n    \n    Args:\n    source_dir (str): Source directory containing files to move.\n    dest_dir (str): Destination directory to move files to.\n    \n    Returns:\n    None\n    \"\"\"\n    for file in os.listdir(source_dir):\n        shutil.move(os.path.join(source_dir, file), dest_dir)\n\ndef check_ram(file_size):\n    \"\"\"\n    Determine whether to use /tmp or /var/tmp based on available RAM.\n    \n    Args:\n    file_size (int): Size of the file to be processed in bytes.\n    \n    Returns:\n    str: Path to the appropriate temporary directory.\n    \"\"\"\n    available_ram = psutil.virtual_memory().available\n    return \"/tmp\" if available_ram > file_size * 2 else \"/var/tmp\"\n\ndef main():\n    \"\"\"\n    Main function to parse command-line arguments and execute the chosen action.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Video and Audio Processing Script\")\n    parser.add_argument(\"input_file\", help=\"Input video or audio file\")\n    parser.add_argument(\"-o\", \"--output_dir\", help=\"Output directory\", default=\".\")\n    parser.add_argument(\"-a\", \"--action\", choices=[\"transcode\", \"extract_audio\", \"normalize\", \n                                                   \"unsilence\", \"pipeline\"], required=True,\n                        help=\"Action to perform on the input file\")\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Enable verbose output\")\n    args = parser.parse_args()\n\n    file_size = os.path.getsize(args.input_file)\n    temp_dir = check_ram(file_size)\n    \n    with tempfile.TemporaryDirectory(dir=temp_dir) as temp_output_dir:\n        base_name = os.path.splitext(os.path.basename(args.input_file))[0]\n        \n        if args.verbose:\n            print(f\"Processing {args.input_file}\")\n            print(f\"Using temporary directory: {temp_dir}\")\n        \n        if args.action == \"transcode\":\n            output_file = os.path.join(temp_output_dir, f\"{base_name}_transcoded.mp4\")\n            transcode(args.input_file, output_file)\n        elif args.action == \"extract_audio\":\n            output_file = os.path.join(temp_output_dir, f\"{base_name}.wav\")\n            extract_audio(args.input_file, output_file)\n        elif args.action == \"normalize\":\n            output_file = os.path.join(temp_output_dir, f\"{base_name}_normalized.wav\")\n            normalize(args.input_file, output_file)\n        elif args.action == \"unsilence\":\n            output_file = os.path.join(temp_output_dir, f\"{base_name}_unsilenced.wav\")\n            unsilence_audio(args.input_file, output_file)\n        elif args.action == \"pipeline\":\n            pipeline(args.input_file, temp_output_dir, args.verbose)\n        \n        move_files(temp_output_dir, args.output_dir)\n    \n    if args.verbose:\n        print(f\"Processing complete. Output files are in {args.output_dir}\")\n    else:\n        print(\"Processing complete.\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n# The code you provided defines a Python script named process_videov2.py designed for audio and video processing tasks. It leverages various external tools like ffmpeg, whisper.cpp, sonic-annotator, unsilence, and ffmpeg-normalize to perform actions such as:\n\n# Transcoding: Converting video files to MP4 format with specific encoding settings.\n# Audio Extraction: Extracting audio tracks from video files and saving them as WAV files.\n# Normalization: Adjusting audio levels to a consistent target.\n# Silence Removal: Detecting and optionally removing silent segments from audio or video.\n# Transcription: Generating text transcripts from audio using the Whisper speech recognition model.\n# Audio Analysis: Analyzing audio features like tempo and key using Sonic Annotator.\n# The script is structured into several functions, each responsible for a specific task:\n\n# run_whisper: Executes the Whisper model for audio transcription, handling potential errors and retries.\n# run_sonic_annotator: Runs Sonic Annotator to analyze audio features and save the results.\n# transcode: Uses ffmpeg to transcode video files to MP4 format.\n# extract_audio: Extracts audio from video files using ffmpeg.\n# estimate_silence: Calculates the percentage of silence in an audio file using the unsilence library and pydub.\n# unsilence_audio: Removes silence from audio or video files if the silence percentage exceeds a threshold, utilizing the unsilence command.\n# normalize: Normalizes audio levels in audio or video files using ffmpeg-normalize.\n# pipeline: Executes a sequence of processing steps: normalization, silence removal, audio extraction (if applicable), transcription, and audio analysis.\n# move_files: Moves files from a source directory to a destination directory.\n# check_ram: Determines the appropriate temporary directory (/tmp or /var/tmp) based on available RAM and file size.\n# main: Parses command-line arguments, determines the appropriate temporary directory, and calls the requested processing function.\n# The script can be invoked from the command line with various options to specify the input file, output directory, desired action (transcode, extract_audio, normalize, unsilence, or pipeline), and verbosity level.\n\n# In essence, this script provides a convenient way to automate common audio and video processing tasks, making it a useful tool for tasks like media preparation, analysis, and transcription"
    },
    {
      "filename": "process_video.py",
      "path": "/defunct/",
      "content": "#!/usr/bin/env python\nimport os\nimport shutil\nimport subprocess\nimport argparse\nimport psutil\nimport tempfile\nimport time\nfrom unsilence import Unsilence\n\n# Configuration constants\nMAX_RETRIES = 3  # Maximum number of retry attempts for Whisper transcription\nRETRY_DELAY = 5  # Delay in seconds between retry attempts\n\ndef run_whisper(input_file, output_file, verbose=False):\n    \"\"\"\n    Transcribe audio using the Whisper model.\n    \n    Args:\n    input_file (str): Path to the input audio file.\n    output_file (str): Path to save the transcription output.\n    verbose (bool): If True, print detailed progress information.\n    \n    Returns:\n    None\n    \"\"\"\n    for attempt in range(MAX_RETRIES):\n        try:\n            if verbose:\n                print(f\"Attempt {attempt + 1} to transcribe using Whisper\")\n            subprocess.run([\"whisper.cpp\", input_file, \"-o\", output_file], check=True)\n            return  # Success, exit the function\n        except subprocess.CalledProcessError as e:\n            if verbose:\n                print(f\"Whisper transcription failed: {e}\")\n            if attempt < MAX_RETRIES - 1:\n                if verbose:\n                    print(f\"Retrying in {RETRY_DELAY} seconds...\")\n                time.sleep(RETRY_DELAY)\n            else:\n                print(f\"Failed to transcribe after {MAX_RETRIES} attempts. Skipping transcription.\")\n                # Create an empty file to indicate transcription was attempted but failed\n                with open(output_file, 'w') as f:\n                    f.write(\"Transcription failed after multiple attempts.\")\n\ndef run_sonic_annotator(input_file, output_dir):\n    \"\"\"\n    Analyze audio features using Sonic Annotator.\n    \n    Args:\n    input_file (str): Path to the input audio file.\n    output_dir (str): Directory to save the analysis output.\n    \n    Returns:\n    None\n    \"\"\"\n    subprocess.run([\"sonic-annotator\", \"-d\", \"vamp:qm-vamp-plugins:qm-tempotracker:tempo\", \n                    \"-d\", \"vamp:qm-vamp-plugins:qm-keydetector:key\", \n                    \"-w\", \"csv\", \"--csv-force\", \n                    \"-o\", output_dir, input_file], check=True)\n\ndef transcode(input_file, output_file):\n    \"\"\"\n    Transcode video to MP4 format with specific encoding options.\n    \n    Args:\n    input_file (str): Path to the input video file.\n    output_file (str): Path to save the transcoded video.\n    \n    Returns:\n    None\n    \"\"\"\n    subprocess.run([\"ffmpeg\", \"-i\", input_file, \"-c:v\", \"libx264\", \"-crf\", \"23\", \n                    \"-c:a\", \"aac\", \"-b:a\", \"128k\", output_file], check=True)\n\ndef extract_audio(input_file, output_file):\n    \"\"\"\n    Extract audio from a video file as WAV.\n    \n    Args:\n    input_file (str): Path to the input video file.\n    output_file (str): Path to save the extracted audio.\n    \n    Returns:\n    None\n    \"\"\"\n    subprocess.run([\"ffmpeg\", \"-i\", input_file, \"-vn\", \"-acodec\", \"pcm_s16le\", \n                    \"-ar\", \"44100\", \"-ac\", \"2\", output_file], check=True)\n\ndef estimate_silence(input_file):\n    \"\"\"\n    Estimate the percentage of silence in an audio file.\n    \n    Args:\n    input_file (str): Path to the input audio file.\n    \n    Returns:\n    float: Percentage of silence in the audio.\n    \"\"\"\n    u = Unsilence(input_file)\n    u.detect_silence()\n    total_duration = u.duration\n    silent_duration = sum(interval[1] - interval[0] for interval in u.silent_intervals)\n    silence_percentage = (silent_duration / total_duration) * 100\n    return silence_percentage\n\ndef unsilence_audio(input_file, output_file):\n    \"\"\"\n    Remove silence from an audio file if silence percentage is above 10%.\n    \n    Args:\n    input_file (str): Path to the input audio file.\n    output_file (str): Path to save the unsilenced audio.\n    \n    Returns:\n    None\n    \"\"\"\n    silence_percentage = estimate_silence(input_file)\n    if silence_percentage > 10:\n        print(f\"Estimated silence: {silence_percentage:.2f}%. Applying unsilence.\")\n        u = Unsilence(input_file)\n        u.detect_silence()\n        u.render_media(output_file, audible_speed=1, silent_speed=8)\n    else:\n        print(f\"Estimated silence: {silence_percentage:.2f}%. Skipping unsilence.\")\n        shutil.copy(input_file, output_file)\n\ndef normalize(input_file, output_file):\n    \"\"\"\n    Normalize audio levels using ffmpeg-normalize.\n    \n    Args:\n    input_file (str): Path to the input audio file.\n    output_file (str): Path to save the normalized audio.\n    \n    Returns:\n    None\n    \"\"\"\n    subprocess.run([\"ffmpeg-normalize\", input_file, \"-o\", output_file], check=True)\n\ndef pipeline(input_file, output_dir, verbose=False):\n    \"\"\"\n    Execute the complete processing pipeline: normalization, silence removal,\n    audio extraction, transcription, and audio analysis.\n    \n    Args:\n    input_file (str): Path to the input video or audio file.\n    output_dir (str): Directory to save all output files.\n    verbose (bool): If True, print detailed progress information.\n    \n    Returns:\n    None\n    \"\"\"\n    base_name = os.path.splitext(os.path.basename(input_file))[0]\n    \n    # Normalize\n    normalized_file = os.path.join(output_dir, f\"{base_name}_normalized.wav\")\n    normalize(input_file, normalized_file)\n    \n    # Unsilence\n    unsilenced_file = os.path.join(output_dir, f\"{base_name}_unsilenced.wav\")\n    unsilence_audio(normalized_file, unsilenced_file)\n    \n    # Extract audio (if input is video)\n    if input_file.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n        audio_file = os.path.join(output_dir, f\"{base_name}.wav\")\n        extract_audio(input_file, audio_file)\n    else:\n        audio_file = unsilenced_file\n    \n    # Transcribe\n    transcription_file = os.path.join(output_dir, f\"{base_name}_transcription.txt\")\n    run_whisper(audio_file, transcription_file, verbose)\n    \n    # Analyze audio\n    run_sonic_annotator(audio_file, output_dir)\n\ndef move_files(source_dir, dest_dir):\n    \"\"\"\n    Move all files from source directory to destination directory.\n    \n    Args:\n    source_dir (str): Source directory containing files to move.\n    dest_dir (str): Destination directory to move files to.\n    \n    Returns:\n    None\n    \"\"\"\n    for file in os.listdir(source_dir):\n        shutil.move(os.path.join(source_dir, file), dest_dir)\n\ndef check_ram(file_size):\n    \"\"\"\n    Determine whether to use /tmp or /var/tmp based on available RAM.\n    \n    Args:\n    file_size (int): Size of the file to be processed in bytes.\n    \n    Returns:\n    str: Path to the appropriate temporary directory.\n    \"\"\"\n    available_ram = psutil.virtual_memory().available\n    return \"/tmp\" if available_ram > file_size * 2 else \"/var/tmp\"\n\ndef main():\n    \"\"\"\n    Main function to parse command-line arguments and execute the chosen action.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Video and Audio Processing Script\")\n    parser.add_argument(\"input_file\", help=\"Input video or audio file\")\n    parser.add_argument(\"-o\", \"--output_dir\", help=\"Output directory\", default=\".\")\n    parser.add_argument(\"-a\", \"--action\", choices=[\"transcode\", \"extract_audio\", \"normalize\", \n                                                   \"unsilence\", \"pipeline\"], required=True,\n                        help=\"Action to perform on the input file\")\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Enable verbose output\")\n    args = parser.parse_args()\n\n    file_size = os.path.getsize(args.input_file)\n    temp_dir = check_ram(file_size)\n    \n    with tempfile.TemporaryDirectory(dir=temp_dir) as temp_output_dir:\n        base_name = os.path.splitext(os.path.basename(args.input_file))[0]\n        \n        if args.verbose:\n            print(f\"Processing {args.input_file}\")\n            print(f\"Using temporary directory: {temp_dir}\")\n        \n        if args.action == \"transcode\":\n            output_file = os.path.join(temp_output_dir, f\"{base_name}_transcoded.mp4\")\n            transcode(args.input_file, output_file)\n        elif args.action == \"extract_audio\":\n            output_file = os.path.join(temp_output_dir, f\"{base_name}.wav\")\n            extract_audio(args.input_file, output_file)\n        elif args.action == \"normalize\":\n            output_file = os.path.join(temp_output_dir, f\"{base_name}_normalized.wav\")\n            normalize(args.input_file, output_file)\n        elif args.action == \"unsilence\":\n            output_file = os.path.join(temp_output_dir, f\"{base_name}_unsilenced.wav\")\n            unsilence_audio(args.input_file, output_file)\n        elif args.action == \"pipeline\":\n            pipeline(args.input_file, temp_output_dir, args.verbose)\n        \n        move_files(temp_output_dir, args.output_dir)\n    \n    if args.verbose:\n        print(f\"Processing complete. Output files are in {args.output_dir}\")\n    else:\n        print(\"Processing complete.\")\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
      "filename": "process_video.sh",
      "path": "/defunct/",
      "content": "#!/usr/bin/env bash\nset -x\n\n# Create a temporary directory in /tmp\ntemp_dir=$(mktemp -d -p /tmp)\n\n# Function to run whisper\nrun_whisper() {\n  local wav_file=\"$1\"\n\n  WHISPER=\"$HOME/Workspace/Vox/whisper.cpp\"\n  MODEL=\"$WHISPER/models/ggml-base.en-q5_0.bin\"\n  # -owts -fp /usr/local/share/fonts/Hack/Hack\\ Bold\\ Nerd\\ Font\\ Complete.ttf\n\n  $WHISPER/build/bin/main -m $MODEL -pp -sow \\\n    -ojf -otxt -osrt -ocsv -ovtt \\\n    -of \"${dest_dir}/${filename}\" \\\n    -pc \"${wav_file}\"\n}\n\n# Function to run sonic-annotator\nrun_sonic_annotator() {\n  local wav_file=\"$1\"\n\n  sonic-annotator -d vamp:vamp-aubio:aubiomelenergy:mfcc \\\n    -d vamp:vamp-aubio:aubioonset:onsets \\\n    -d vamp:vamp-aubio:aubionotes:notes \\\n    -d vamp:mtg-melodia:melodia:melody \"${wav_file}\" \\\n    -w csv --csv-force --csv-basedir \"${dest_dir}\"\n}\n\n# Function to process video using ffmpeg\ntranscode() {\n  local infile=\"$1\"\n  local outfile=\"$2\"\n\n  # ffmpeg options\n  local crf=\"15.0\"\n  local vcodec=\"libx264\"\n  local acodec=\"copy\"\n  local coder=\"1\"\n  local me_method=\"hex\"\n  local subq=\"6\"\n  local me_range=\"16\"\n  local g=\"250\"\n  local keyint_min=\"25\"\n  local sc_threshold=\"40\"\n  local i_qfactor=\"0.71\"\n  local b_strategy=\"1\"\n  local strict=\"-2\"\n  local threads=\"19\"\n\n  /usr/bin/ffmpeg -i \"${infile}\" \\\n    -crf \"${crf}\" \\\n    -vcodec \"${vcodec}\" \\\n    -acodec \"${acodec}\" \\\n    -coder \"${coder}\" \\\n    -flags +loop -cmp +chroma -partitions +parti4x4+partp8x8+partb8x8 \\\n    -me_method \"${me_method}\" \\\n    -subq \"${subq}\" \\\n    -me_range \"${me_range}\" \\\n    -g \"${g}\" \\\n    -keyint_min \"${keyint_min}\" \\\n    -sc_threshold \"${sc_threshold}\" \\\n    -i_qfactor \"${i_qfactor}\" \\\n    -b_strategy \"${b_strategy}\" \\\n    -strict \"${strict}\" \\\n    -threads \"${threads}\" \\\n    -y \"${outfile}\"\n}\n\n# Function to extract audio from a video file\nextract_audio() {\n  local infile=\"$1\"\n  local outfile=\"$2\"\n  local compress=\"$3\"\n  # Extract audio using ffmpeg\n  ffmpeg -i \"${infile}\" -ar 16000 -acodec pcm_s16le -ac 1 \"${outfile}\"\n\n  if [[ \"$compress\" == \"mp3\" ]]; then\n    mp3file=\"${temp_dir}/${filename}.mp3\"\n    sox \"${outfile}\" -b 16\"${mp3file}.mp3\"\n  fi\n\n}\n\n# Function to run the unsilence command\nunsilence_audio() {\n  local infile=\"$1\"\n  local outfile=\"$2\"\n  local threshold=$(gum input --prompt \"Enter threshold: \" --placeholder=\"-30\" --value=\"-30\")\n\n  unsilence -d -ss 1.5 -sl \"${threshold}\" \"${infile}\" \"${outfile}\"\n\n}\n\nnormalize() {\n  local infile=\"$1\"\n\n  ffmpeg-normalize -pr -nt rms \"${infile}\" \\\n    -prf \"highpass=f=200\" -prf \"dynaudnorm=p=0.4:s=15\" -pof \"lowpass=f=7000\" \\\n    -ar 48000 -c:a pcm_s16le --keep-loudness-range-target \\\n    -o \"${normalized}\"\n}\n\npipeline() {\n  local infile=\"$1\"\n  local ext=\"${infile##*.}\"\n  local normalized=\"${temp_dir}/${filename}_normalized.${ext}\"\n  local wavfile=\"${temp_dir}/${filename}.wav\"\n\n  local outfile=\"${temp_dir}/${filename}.${ext}\"\n\n  normalize \"${infile}\" \"${normalized}\"\n\n  unsilence_audio \"${normalized}\" \"${outfile}\"\n\n  extract_audio \"${outfile}\" \"${wavfile}\"\n\n  run_whisper \"${wavfile}\" && run_sonic_annotator \"${wavfile}\" \"${dest_dir}\"\n\n}\n\n# Function to move files to a destination folder\nmove_files() {\n  local source_dir=\"$1\"\n  local dest_dir=\"$2\"\n\n  # Create the destination folder if it doesn't exist\n  mkdir -p \"${dest_dir}\"\n\n  # Move all files from the source directory to the destination directory\n  mv \"${source_dir}\"/* \"${dest_dir}\"\n}\n\n# Main script logic\ndeclare infile=\"$1\"\ndeclare fbasename=$(basename \"$infile\")\ndeclare filename=\"${fbasename%.*}\"\ndeclare ext=\"${infile##*.}\"\n\n# Get file size in bytes\nfile_size=$(stat -c%s \"$infile\")\n\n# Get available RAM in bytes\navailable_ram=$(free -m | awk '/Mem:/ {print $4}')\navailable_ram=$((available_ram * 1024 * 1024))\n\n# Determine temp directory based on available RAM\nif ((available_ram > (file_size * 2))); then\n  temp_dir=$(mktemp -d -p /tmp)\nelse\n  temp_dir=$(mktemp -d -p /var/tmp)\n  export TMPDIR=\"/var/tmp\"\nfi\n\nif [[ -d \"$2\" ]]; then\n  dest_dir=\"$2\"\nelse\n  dest_dir=$(yad --file --directory)\nfi\n\nCHOICE=$(gum choose \"Transcode\" \"Extract Audio\" \"Normalize Audio\" \"Unsilence Audio\" \"Pipeline\")\n\ncase \"$CHOICE\" in\n\"Pipeline\")\n  pipeline \"${infile}\"\n  ;;\n\"Transcode\")\n  outfile=\"${temp_dir}/${filename}.mp4\"\n  transcode \"${infile}\" \"${outfile}\"\n  ;;\n\"Extract Audio\")\n  outfile=\"${temp_dir}/${filename}\"\n  extract_audio \"${infile}\" \"${outfile}.wav\"\n  ;;\n\"Normalize Audio\")\n  outfile=\"${temp_dir}/${filename}\"\n  normalize \"${infile}\" \"${outfile}_normalized.${ext}\"\n  ;;\n\"Unsilence Audio\")\n  outfile=\"${temp_dir}/${filename}\"\n  unsilence_audio \"${infile}\" \"${outfile}\"\n  ;;\nesac\n\nmove_files \"${temp_dir}\" \"${dest_dir}\"\n\nrm -rf \"${temp_dir}\"\n\n# # Ask the user if they want to move the files to a permanent folder\n# MOVE_CHOICE=$(gum choose \"Move to permanent folder\" \"Keep in temporary folder\")\n\n# if [[ \"$MOVE_CHOICE\" == \"Move to permanent folder\" ]]; then\n#   # Ask for the destination folder\n#   dest_dir=$(yad --file --directory)\n\n#   # Check if the user canceled the folder selection\n#   if [[ $? -ne 0 ]]; then\n#     echo \"Folder selection canceled. Files will remain in the temporary directory.\"\n#   else\n#     # Move the files to the destination folder\n#     move_files \"${temp_dir}\" \"${dest_dir}\"\n\n#     # Ask the user if they want to remove the temporary directory\n#     rm_tmp=$(gum choose --timeout=6s --selected=\"yes\" \"yes\" \"no\")\n#     if [[ \"$rm_tmp\" == \"yes\" ]]; then\n#       rm -rf \"${temp_dir}\"\n#     else\n#       echo \"Files will remain in the temporary directory.\"\n#     fi\n\n#   fi\n# fi\n"
    },
    {
      "filename": "audsplitter.sh",
      "path": "/dev/",
      "content": "#!/bin/bash\n\n# Check for dependencies\nif ! command -v sox &>/dev/null; then\n    echo \"Error: sox is required for this script. Please install it.\"\n    exit 1\nfi\n\nif ! command -v fd &>/dev/null; then\n    echo \"Error: fd command is required for this script. Please install it.\"\n    exit 1\nfi\n\nsox /tmp/output.wav -r 16000 -b 16 -c 1 snotes_mono16kv23.wav highpass 220 vol 1.5 amplitude 0.3 \\\\nsilence 0 0.1 00:00:00.1 -60d : newfile : restart\n\n# Set the path to search for audio files\npath=\"/tmp\"\n\n# Use fd to find audio files (adjust extensions as needed)\nfd --extension mp3 --extension wav --extension flac -p \"$path\" | while read -r file; do\n    # Run sox and capture the output\n    output=$(sox -V2 \"$file\" -n stats 2>&1)\n\n    # Check if the output indicates no audio\n    if [[ \"$output\" == \"sox WARN stats: no audio\" ]]; then\n        echo \"Deleting file with no audio: $file\"\n        rm \"$file\"\n    fi\ndone\n\n"
    },
    {
      "filename": "scriber.rb",
      "path": "/dev/",
      "content": "#!/usr/bin/env ruby\n\nlib_dir = File.expand_path(File.join(__dir__, '..', 'lib'))\n$LOAD_PATH.unshift lib_dir unless $LOAD_PATH.include?(lib_dir)\n\nrequire 'openai'\nrequire 'securerandom'\nrequire 'tty-config'\nrequire 'tty-which'\nrequire 'tty-prompt'\n\nrequire 'loger'\nrequire 'audio_input'\nrequire 'file_select'\nrequire 'transcribe'\nrequire 'i3_nav'\n\n\nNOTEBOOK = File.join(Dir.home, \"Notebook\")\n\nAPP_ROOT = File.expand_path(\"../../\", __FILE__)\n\n# Method that initiates the transcription process\ndef main\n  config = TTY::Config.new\n  config.append_path APP_ROOT\n  config.read if config.exist?\n\n  # Select the input device\n  input_device = InputDevice.new(config)\n  selected_port = input_device.select_input_device\n\n  # Prompt for the markdown file to paste the transcription\n  markdown_file = MarkdownFile.new(config)\n  selected_file = if ARGV[0] == '--new-file'\n                    markdown_file.select_markdown_file\n                  else\n                    config.fetch(:selected_file) { markdown_file.select_markdown_file }\n                  end\n\n  $logger.debug(\"#{selected_file}\")\n\n  # Fetch the openai_access_token from the config file\n  openai_access_token = config.fetch(:openai_access_token)\n\n  # Execute the transcription command\n  command = TranscribeCommand.new(\n                                  selected_port,\n                                  selected_file,\n                                  openai_access_token\n                                  )\n  command.execute\nend\n\nmain\n"
    },
    {
      "filename": "splitter.py",
      "path": "/dev/",
      "content": "#!/usr/bin/env python\nimport argparse\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pydub import AudioSegment\nfrom pydub.silence import split_on_silence\nfrom pydub.effects import normalize\nfrom tqdm import tqdm\nfrom scipy.signal import butter, sosfiltfilt \n\n\n# def save_chunk(chunk, start_time, output_dir, output_format):\n#     # normalized_chunk = normalize(chunk)  \n#     # mono_chunk = normalized_chunk.set_channels(1)  # Convert to mono\n#     chunk.export(os.path.join(output_dir, f'chunk_{start_time}.{output_format}'), format=output_format)\n\ndef save_chunk(chunk  , start_time, output_dir, output_format):\n    # normalized_chunk = normalize(chunk)  \n    # mono_chunk =   normalized_chunk.set_channels(1)  # Convert to mono\n    chunk.export(os.path.join(output_dir, f'chunk_{start_time:08d}.{output_format}'), format=output_format)\n\n\ndef merge_short_chunks(chunks, min_chunk_length_ms):\n    merged_chunks = []\n    current_chunk = chunks[0]\n\n    for chunk in chunks[1:]:\n        if len(current_chunk) + len(chunk) < min_chunk_length_ms:\n            current_chunk += chunk\n        else:\n            merged_chunks.append(current_chunk)\n            current_chunk = chunk\n\n    merged_chunks.append(current_chunk)\n    return merged_chunks\n\ndef split_audio(input_file, output_dir, chunk_length_ms, output_format, silence_based):\n    # Load the input audio file using Pydub\n    audio = AudioSegment.from_file(input_file)\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    if silence_based:\n        # Split the audio file based on silence\n        min_silence_len = 300  # Minimum length of silence in milliseconds\n        silence_thresh = -60   # Silence threshold in dB\n        chunks = split_on_silence(audio, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n\n        # Merge adjacent chunks shorter than the specified length\n        chunks = merge_short_chunks(chunks, chunk_length_ms)\n\n        # Set up progress bar with tqdm\n        pbar = tqdm(total=len(chunks), desc=\"Processing chunks based on silence\")\n\n        # Save chunks in parallel using ThreadPoolExecutor\n        with ThreadPoolExecutor() as executor:\n            for i, chunk in enumerate(chunks):\n                executor.submit(save_chunk, chunk, i, output_dir, output_format).add_done_callback(lambda x: pbar.update(1))\n\n    else:\n        # Calculate the total length of the audio in milliseconds and the number of full chunks\n        audio_length_ms = len(audio)\n        num_chunks = audio_length_ms // chunk_length_ms\n\n        # Set up progress bar with tqdm\n        pbar = tqdm(total=num_chunks + (audio_length_ms % chunk_length_ms != 0), desc=\"Processing fixed-size chunks\")\n\n        # Split and save chunks in parallel using ThreadPoolExecutor\n        with ThreadPoolExecutor() as executor:\n            for i in range(num_chunks):\n                start_time = i * chunk_length_ms\n                end_time = (i + 1) * chunk_length_ms\n                chunk = audio[start_time:end_time]\n                executor.submit(save_chunk, chunk, start_time, output_dir, output_format).add_done_callback(lambda x: pbar.update(1))\n\n            # Handle the last chunk if there is any remainder\n            if audio_length_ms % chunk_length_ms != 0:\n                start_time = num_chunks * chunk_length_ms\n                end_time = audio_length_ms\n                chunk = audio[start_time:end_time]\n                executor.submit(save_chunk, chunk, start_time, output_dir, output_format).add_done_callback(lambda x: pbar.update(1))\n\n    # Close progress bar\n    pbar.close()\n    \n\ndef main():\n    # Set up argument parser for the CLI app\n    parser = argparse.ArgumentParser(description=\"Split an audio file into equally sized chunks.\")\n    parser.add_argument(\"input_file\", help=\"Path to the input audio file.\")\n    parser.add_argument(\"output_dir\", help=\"Path to the output directory where chunks will be saved.\")\n    parser.add_argument(\"--chunk_length\", type=int, default=300000, help=\"Length of each chunk in milliseconds (default: 300000 ms / 5 minutes).\")\n    parser.add_argument(\"--output_format\", type=str, default=\"wav\", help=\"Output format for the audio chunks (default: wav). Supported formats include wav, mp3, and ogg.\")\n    parser.add_argument(\"--silence_based\", action=\"store_true\", help=\"Split the audio based on silence instead of fixed-size chunks. If set, --chunk_length is ignored.\")\n\n    # Parse the arguments\n    args = parser.parse_args()\n\n    # Call the split_audio function with the provided arguments\n    split_audio(args.input_file, args.output_dir, args.chunk_length, args.output_format, args.silence_based)\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
      "filename": "transcribe.sh",
      "path": "/dev/",
      "content": "#!/usr/bin/env bash\nset -x\n\ndeclare -rx timestamp=$(date +'%Y-%m-%d_%H-%M-%S')\n\ndeclare -rx dictations=\"$HOME/Recordings/audio/dictations\"\n\n# declare -rx transcription=\"/tmp/transcription-${timestamp}.txt\"\n\ndeclare -rx capturefile=\"${dictations}/jack_capture_${timestamp}\"\n\ndeclare -rx wavfile=\"${capturefile}.wav\"\n\ndeclare xel_args=\"-ib\"\n\nfunction cleanup() {\n    cd /tmp && rm -rf *.mp3\n}\n\nfunction transcribe() {\n        local infile=$1\n        fbasename=$(basename \"$1\")\n        filename=\"${fbasename%.*}\"\n\n        local outfile=\"/tmp/${filename}.mp3\"\n\n        sox ${infile} ${outfile}\n\n        text=$(gum spin -s line --title \"transcribing\" --show-output -- curl https://api-inference.huggingface.co/models/distil-whisper/distil-large-v2 \\\n                        -X POST \\\n                        --data-binary \"@${outfile}\" \\\n                        -H \"Authorization: Bearer $HUGGINGFACE_API_KEY\"\n                      )\n\n        echo $text | jq '.text' | xargs -0 | ruby -pe 'gsub(/^\\s/, \"\")'\n}\n\n#########################################################################\n#                             Greetings                                 #\n#########################################################################\ntrap cleanup SIGINT SIGTERM ERR EXIT\n\ngum style \\\n        --foreground 014 --border-foreground 024 --border double \\\n        --align center --width 50 --margin \"1 2\" --padding \"2 4\" \\\n        'Hello.' && sleep 1\n\nxsel -cb\n\n# transcribe $1\n\nif [ $? = 0 ]; then\n        notify-send -t 5000 'transcribing....'\n        logger -t dictation --priority user.debug transcribing $1\n\n        text=\"\"\n        max_retries=5\n        for ((i=0; i<max_retries; i++)); do\n                text=$(transcribe $1)\n                if [[ \"$text\" != \"null\" ]]; then\n                        break\n                fi\n\n                echo $text\n\n                echo \"Transcription failed, retrying ($((max_retries-i)))...\"\n\n                gum spin -s line --title \"waiting 10 seconds...\" sleep 10\n\n        done\n\n        if [[ \"$text\" == \"null\" || \"$text\" == \"\" ]]; then\n                notify-send -t 5000 -u critical \"Transcription failed\"\n        else\n                text=$(echo $text | xargs -0 | ruby -pe 'gsub(/^\\s/, \"\")')\n                xsel -a -b <<< \"$text\"\n                notify-send -t 10000 \"transcription copied to clipboard\"\n                xsel -ob | gum format -t code\n        fi\n\nelse\n        notify-send -t 5000 -u critical \"capture failed\"\nfi\n"
    },
    {
      "filename": "vid_transcript.sh",
      "path": "/dev/",
      "content": "#!/usr/bin/env bash\nset -x\n\n\n#API_URL=\"https://api-inference.huggingface.co/models/distil-whisper/distil-large-v2\"\n\nAPI_URL=\"https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h\"\n\nfunction transcribe() {\n  local mp3file=$1\n  local max_retries=5\n  for ((i = 0; i < max_retries; i++)); do\n\n    text=$(gum spin -s line --title \"transcribing $mp3file\" --show-output -- curl $API_URL \\\n      -X POST \\\n      -H \"Content-Type: multipart/form-data\" \\\n      -H \"Authorization: Bearer $HUGGINGFACE_API_KEY\" \\\n      --data-binary \"@${mp3file}\" | jq '.text')\n\n    if [[ \"$text\" != \"null\" ]]; then\n      break\n    fi\n\n    notify-send -t 5000 -u critical \"Transcription failed, retrying ($((max_retries - i)))...\"\n\n    gum spin -s line --title \"waiting 10 seconds...\" sleep 10\n  done\n  echo $text | xargs -0 | ruby -pe 'gsub(/^\\s|!/, \"\")'\n}\n\n#########################################################################\n#                             Greetings                                 #\n#########################################################################\ntrap SIGINT SIGTERM ERR EXIT\n\ngum style \\\n  --foreground 014 --border-foreground 024 --border double \\\n  --align center --width 50 --margin \"1 2\" --padding \"2 4\" \\\n  'Hello.' && sleep 1\n\nxsel -cb\n\ncd /tmp/splits\n\nfor mp3file in $(fd -e mp3); do\n\n  text=$(transcribe $mp3file)\n\n  if [[ \"$text\" == \"null\" ]]; then\n    notify-send -t 5000 -u critical \"Transcription failed\"\n  else\n    text=$(echo $text | xargs -0 | ruby -pe 'gsub(/^\\s/, \"\")')\n    xsel -a -b <<<\"$text\"\n    notify-send -t 10000 \"transcription copied to clipboard\"\n    xsel -ob\n  fi\n\ndone\n"
    },
    {
      "filename": "video_process_utility.py",
      "path": "/dev/",
      "content": "#!/usr/bin/env python\n\nimport os\nimport argparse\nimport json\nimport sys\nfrom dotenv import load_dotenv\nfrom deepgram import DeepgramClient, PrerecordedOptions, FileSource, DeepgramError\nfrom groq import Groq\nimport time\nfrom pydub import AudioSegment\nimport spacy\nfrom gensim import corpora\nfrom gensim.models import LdaMulticore\nimport videogrep\nfrom moviepy.editor import VideoFileClip\nimport google.generativeai as genai\nfrom PIL import Image\nimport traceback\n\nload_dotenv()\n\n# Load spaCy model\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef extract_audio(video_path, output_path=None):\n    try:\n        audio = AudioSegment.from_file(video_path)\n        if output_path:\n            audio.export(output_path, format=\"wav\")\n            return f\"Audio extracted to {output_path}\"\n        else:\n            return audio.raw_data\n    except Exception as e:\n        return f\"Error during audio extraction: {str(e)}\"\n\ndef normalize_audio(input_file, output_file, lowpass_freq=1000, highpass_freq=100, norm_level=-3,\n                    compression_params=None):\n    \"\"\"\n    Normalize audio using PySox with optional low-pass and high-pass filtering and compression.\n    \"\"\"\n    try:\n        tfm = sox.Transformer()\n        \n        # Apply low-pass filter\n        if lowpass_freq:\n            tfm.lowpass(frequency=lowpass_freq)\n        \n        # Apply high-pass filter\n        if highpass_freq:\n            tfm.highpass(frequency=highpass_freq)\n        \n        # Normalize audio\n        tfm.norm(db_level=norm_level)\n        \n        # Apply compression if parameters are provided\n        if compression_params:\n            tfm.compand(**compression_params)\n        \n        # Build and apply the effects\n        tfm.build(input_file, output_file)\n        \n        return {\n            \"status\": \"success\",\n            \"message\": f\"Audio normalized and saved to {output_file}\",\n            \"lowpass_freq\": lowpass_freq,\n            \"highpass_freq\": highpass_freq,\n            \"norm_level\": norm_level,\n            \"compression_applied\": bool(compression_params)\n        }\n    except Exception as e:\n        return {\n            \"status\": \"error\",\n            \"message\": f\"Error during audio normalization: {str(e)}\"\n        }\n\ndef transcribe_file_deepgram(file_path, options=None):\n    try:\n        deepgram_key = os.getenv(\"DG_API_KEY\")\n        if not deepgram_key:\n            return \"DG_API_KEY environment variable is not set\"\n        \n        client = DeepgramClient(deepgram_key)\n        if options is None:\n            options = PrerecordedOptions(\n                model=\"nova-2\",\n                smart_format=True,\n                language=\"en\",\n                punctuate=True,\n                utterances=True,\n                diarize=True,\n            )\n        \n        with open(file_path, \"rb\") as audio:\n            buffer_data = audio.read()\n            payload = {\"buffer\": buffer_data, \"mimetype\": \"audio/wav\"}\n            response = client.listen.rest.v(\"1\").transcribe_file(payload, options)\n        return json.loads(response.to_json())\n    except Exception as e:\n        return f\"Error during Deepgram transcription: {str(e)}\"\n\ndef transcribe_file_groq(file_path, model=\"whisper-large-v3\", language=\"en\", prompt=None):\n    try:\n        groq_key = os.getenv(\"GROQ_API_KEY\")\n        if not groq_key:\n            return \"GROQ_API_KEY environment variable is not set\"\n        \n        client = Groq(api_key=groq_key)\n        with open(file_path, \"rb\") as file:\n            transcription = client.audio.transcriptions.create(\n                file=(file_path, file.read()),\n                model=model,\n                language=language,\n                response_format=\"json\",\n                prompt=prompt\n            )\n        return json.loads(transcription.text)\n    except Exception as e:\n        return f\"Error during Groq transcription: {str(e)}\"\n\ndef preprocess_text(text):\n    doc = nlp(text)\n    subjects = []\n    for sent in doc.sents:\n        for token in sent:\n            if \"subj\" in token.dep_:\n                if token.dep_ in [\"nsubj\", \"nsubjpass\", \"csubj\"]:\n                    subject = get_compound_subject(token)\n                    subjects.append(subject)\n    \n    cleaned_subjects = [\n        [token.lemma_.lower() for token in nlp(subject) \n         if not token.is_stop and not token.is_punct and token.is_alpha]\n        for subject in subjects\n    ]\n    cleaned_subjects = [list(s) for s in set(tuple(sub) for sub in cleaned_subjects) if s]\n    return cleaned_subjects\n\ndef get_compound_subject(token):\n    subject = [token.text]\n    for left_token in token.lefts:\n        if left_token.dep_ == \"compound\":\n            subject.insert(0, left_token.text)\n    for right_token in token.rights:\n        if right_token.dep_ == \"compound\":\n            subject.append(right_token.text)\n    return \" \".join(subject)\n\ndef perform_topic_modeling(subjects, num_topics=5):\n    dictionary = corpora.Dictionary(subjects)\n    corpus = [dictionary.doc2bow(subject) for subject in subjects]\n    lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100,\n                             chunksize=100, passes=10, per_word_topics=True)\n    topics = [{\"topic_id\": topic_id, \"words\": [word for word, _ in lda_model.show_topic(topic_id, topn=10)]} \n              for topic_id in range(num_topics)]\n    return topics\n\ndef identify_segments(transcript, lda_model, dictionary, num_topics):\n    segments = []\n    current_segment = {\"start\": 0, \"end\": 0, \"content\": \"\", \"topic\": None}\n    \n    for sentence in transcript:\n        subjects = preprocess_text(sentence[\"content\"])\n        if not subjects:\n            continue\n        \n        bow = dictionary.doc2bow([token for subject in subjects for token in subject])\n        topic_dist = lda_model.get_document_topics(bow)\n        dominant_topic = max(topic_dist, key=lambda x: x[1])[0] if topic_dist else None\n        \n        if dominant_topic != current_segment[\"topic\"]:\n            if current_segment[\"content\"]:\n                current_segment[\"end\"] = sentence[\"start\"]\n                segments.append(current_segment)\n            current_segment = {\n                \"start\": sentence[\"start\"],\n                \"end\": sentence[\"end\"],\n                \"content\": sentence[\"content\"],\n                \"topic\": dominant_topic\n            }\n        else:\n            current_segment[\"end\"] = sentence[\"end\"]\n            current_segment[\"content\"] += \" \" + sentence[\"content\"]\n    \n    if current_segment[\"content\"]:\n        segments.append(current_segment)\n    \n    return segments\n\ndef analyze_segment_with_gemini(segment_path, transcript):\n    try:\n        genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n        model = genai.GenerativeModel('gemini-pro-vision')\n\n        video = VideoFileClip(segment_path)\n        frame = video.get_frame(0)\n        image = Image.fromarray(frame)\n        video.close()\n\n        prompt = f\"Analyze this video segment. The transcript for this segment is: '{transcript}'. Describe the main subject matter, key visual elements, and how they relate to the transcript.\"\n\n        response = model.generate_content([prompt, image])\n        return response.text\n    except Exception as e:\n        return f\"Error during Gemini analysis: {str(e)}\"\n\ndef split_video(input_video, segments, output_dir):\n    try:\n        video = VideoFileClip(input_video)\n        split_info = []\n        for i, segment in enumerate(segments):\n            start_time = segment[\"start\"]\n            end_time = segment[\"end\"]\n            segment_clip = video.subclip(start_time, end_time)\n            output_path = os.path.join(output_dir, f\"segment_{i+1}.mp4\")\n            segment_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n            split_info.append({\n                \"segment_id\": i + 1,\n                \"start_time\": start_time,\n                \"end_time\": end_time,\n                \"output_path\": output_path\n            })\n        video.close()\n        return split_info\n    except Exception as e:\n        return f\"Error during video splitting: {str(e)}\"\n\ndef full_pipeline(input_video, output_dir, api=\"deepgram\", num_topics=5, groq_prompt=None, \n                  lowpass_freq=1000, highpass_freq=100, norm_level=-3, compress=False):\n    \"\"\"\n    Execute the full video processing pipeline.\n    \"\"\"\n    try:\n        # Create output directories\n        os.makedirs(output_dir, exist_ok=True)\n        audio_dir = os.path.join(output_dir, \"audio\")\n        segments_dir = os.path.join(output_dir, \"segments\")\n        os.makedirs(audio_dir, exist_ok=True)\n        os.makedirs(segments_dir, exist_ok=True)\n\n        # Extract audio\n        raw_audio_path = os.path.join(audio_dir, \"extracted_audio.wav\")\n        extract_audio(input_video, raw_audio_path)\n\n        # Normalize audio\n        normalized_audio_path = os.path.join(audio_dir, \"normalized_audio.wav\")\n        compression_params = {\n            \"attack_time\": 0.2,\n            \"decay_time\": 1,\n            \"soft_knee_db\": 2,\n            \"threshold\": -20,\n            \"db_from\": -20,\n            \"db_to\": -10\n        } if compress else None\n        normalize_audio(raw_audio_path, normalized_audio_path, lowpass_freq, highpass_freq, norm_level, compression_params)\n\n        # Transcribe audio\n        if api == \"deepgram\":\n            transcription = transcribe_file_deepgram(normalized_audio_path)\n            transcript = [\n                {\n                    \"content\": utterance[\"transcript\"],\n                    \"start\": utterance[\"start\"],\n                    \"end\": utterance[\"end\"]\n                }\n                for utterance in transcription.get('results', {}).get('utterances', [])\n            ]\n        else:  # Groq\n            transcription = transcribe_file_groq(normalized_audio_path, prompt=groq_prompt)\n            if isinstance(transcription, dict) and 'segments' in transcription:\n                transcript = [\n                    {\n                        \"content\": segment.get('text', ''),\n                        \"start\": segment.get('start', 0),\n                        \"end\": segment.get('end', 0)\n                    }\n                    for segment in transcription['segments']\n                ]\n            elif isinstance(transcription, str):\n                # If Groq returns a string, create a single segment\n                transcript = [{\n                    \"content\": transcription,\n                    \"start\": 0,\n                    \"end\": 0  # You might want to get the audio duration here\n                }]\n            else:\n                raise ValueError(f\"Unexpected transcription format from Groq: {type(transcription)}\")\n\n        # Save transcript\n        transcript_path = os.path.join(output_dir, \"transcript.json\")\n        with open(transcript_path, 'w') as f:\n            json.dump(transcript, f, indent=2)\n\n        # Preprocess text and perform topic modeling\n        full_text = \" \".join([sentence[\"content\"] for sentence in transcript])\n        preprocessed_subjects = preprocess_text(full_text)\n        topics = perform_topic_modeling(preprocessed_subjects, num_topics)\n\n        # Identify segments\n        segments = identify_segments(transcript, topics, preprocessed_subjects, num_topics)\n\n        # Split video and analyze segments\n        split_info = split_video(input_video, segments, segments_dir)\n\n        # Analyze segments with Gemini\n        analyzed_segments = []\n        for segment in split_info:\n            gemini_analysis = analyze_segment_with_gemini(segment['output_path'], segment['transcript'])\n            analyzed_segments.append({\n                **segment,\n                \"gemini_analysis\": gemini_analysis\n            })\n\n        # Prepare final results\n        results = {\n            \"input_video\": input_video,\n            \"normalized_audio\": normalized_audio_path,\n            \"transcription_api\": api,\n            \"transcript_path\": transcript_path,\n            \"topics\": topics,\n            \"segments\": analyzed_segments\n        }\n\n        # Save results\n        results_path = os.path.join(output_dir, \"results.json\")\n        with open(results_path, 'w') as f:\n            json.dump(results, f, indent=2)\n\n        return {\n            \"status\": \"success\",\n            \"message\": \"Full pipeline completed successfully\",\n            \"results_path\": results_path\n        }\n\n    except Exception as e:\n        return {\n            \"status\": \"error\",\n            \"message\": f\"Error during full pipeline execution: {str(e)}\",\n            \"traceback\": traceback.format_exc()\n        }\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Video processing utility functions\")\n    parser.add_argument(\"function\", nargs='?', choices=[\"extract_audio\", \"transcribe_deepgram\", \"transcribe_groq\", \n                                             \"preprocess_text\", \"topic_modeling\", \"identify_segments\", \n                                             \"analyze_segment\", \"split_video\", \"normalize_audio\"],\n                        help=\"Function to execute (if not specified, full pipeline will be executed)\")\n    parser.add_argument(\"-i\", \"--input\", required=True, help=\"Input file path\")\n    parser.add_argument(\"-o\", \"--output\", help=\"Output file or directory path\")\n    parser.add_argument(\"--topics\", type=int, default=5, help=\"Number of topics for LDA model\")\n    parser.add_argument(\"--api\", choices=[\"deepgram\", \"groq\"], default=\"deepgram\", help=\"Transcription API\")\n    parser.add_argument(\"--prompt\", help=\"Prompt for Groq transcription or Gemini analysis\")\n    parser.add_argument(\"--lowpass\", type=int, default=1000, help=\"Low-pass filter frequency\")\n    parser.add_argument(\"--highpass\", type=int, default=100, help=\"High-pass filter frequency\")\n    parser.add_argument(\"--norm-level\", type=float, default=-3, help=\"Normalization level in dB\")\n    parser.add_argument(\"--compress\", action=\"store_true\", help=\"Apply compression\")\n    args = parser.parse_args()\n\n    if args.function is None:\n        # Execute full pipeline\n        result = full_pipeline(args.input, args.output or os.getcwd(), args.api, args.topics, args.prompt,\n                               args.lowpass, args.highpass, args.norm_level, args.compress)\n    else:\n        # Execute specific function\n        result = None\n        if args.function == \"normalize_audio\":\n            compression_params = {\n                \"attack_time\": 0.2,\n                \"decay_time\": 1,\n                \"soft_knee_db\": 2,\n                \"threshold\": -20,\n                \"db_from\": -20,\n                \"db_to\": -10\n            } if args.compress else None\n            \n            result = normalize_audio(args.input, args.output, args.lowpass, args.highpass, args.norm_level, compression_params)\n        elif args.function == \"extract_audio\":\n            result = extract_audio(args.input, args.output)\n        elif args.function == \"transcribe_deepgram\":\n            result = transcribe_file_deepgram(args.input)\n        elif args.function == \"transcribe_groq\":\n            result = transcribe_file_groq(args.input, prompt=args.prompt)\n        elif args.function == \"preprocess_text\":\n            with open(args.input, 'r') as f:\n                text = f.read()\n            result = preprocess_text(text)\n        elif args.function == \"topic_modeling\":\n            with open(args.input, 'r') as f:\n                subjects = json.load(f)\n            result = perform_topic_modeling(subjects, args.topics)\n        elif args.function == \"identify_segments\":\n            # This requires more complex input, you might need to adjust based on your needs\n            pass\n        elif args.function == \"analyze_segment\":\n            with open(args.input, 'r') as f:\n                transcript = f.read()\n            result = analyze_segment_with_gemini(args.output, transcript)\n        elif args.function == \"split_video\":\n            with open(args.input, 'r') as f:\n                segments = json.load(f)\n            result = split_video(args.output, segments, os.path.dirname(args.output))\n\n    json.dump(result, sys.stdout, indent=2)\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
      "filename": "audio_input.rb",
      "path": "/lib/",
      "content": "#!/usr/bin/env ruby\n\n# Class responsible for selecting the input device\nclass InputDevice\n  def initialize(config)\n    @config = config\n    @prompt = TTY::Prompt.new\n  end\n\n  # Selects the input device based on user preference or default configuration\n  def select_input_device\n    input_source = @config.fetch(:input_source) { @prompt.select(\"Select the input source:\", %w[alsa jack]) }\n    @config.set(:input_source, value: input_source)\n    @config.write(force: true)\n\n    case input_source\n    when 'alsa'\n      select_alsa_device\n    when 'jack'\n      select_jack_port\n    else\n      puts \"Invalid input source. Exiting...\"\n      exit\n    end\n  end\n\n  private\n\n  # # Selects the input device in the alsa input source\n  # def select_alsa_device\n  #   device_list = `arecord -l`\n  #   devices = device_list.scan(/card \\d+: .*?\\n/)\n  #\n  #   selected_device = @config.fetch(:alsa_device) { @prompt.select(\"Select the input device:\", devices) }.strip\n  #   @config.set(:alsa_device, value: selected_device)\n  #   @config.write(force: true)\n  #\n  #   card_number = selected_device.match(/card (\\d+):/)[1]\n  #   device_number = selected_device.match(/device (\\d+):/)[1]\n  #   \"hw:#{card_number},#{device_number}\"\n  # end\n\n  # Selects the input device in the alsa input source\n  def select_alsa_device\n    device_list = `arecord -L`\n    devices = device_list.scan(/sysdefault:CARD=(.*?)\\n/).flatten\n\n    selected_device = @config.fetch(:alsa_device, nil)\n    if selected_device.nil?\n      selected_device = @prompt.select(\"Select the input device:\", devices).strip\n      @config.data['alsa_device'] = \"sysdefault:CARD=#{selected_device}\"\n      @config.write(force: true)\n    end\n\n    selected_device_match = selected_device.match(/sysdefault:CARD=(.*)/)\n    if selected_device_match\n      selected_device_match[1]\n    else\n      raise \"Failed to extract ALSA device name\"\n    end\n  end\n\n  # Selects the capture port in the jack input source\n  def select_jack_port\n    if TTY::Which.exist?('jack_lsp')\n      port_list = `jack_lsp`\n      ports = port_list.split(\"\\n\").select { |port| port.include?('capture') }\n\n      selected_port = @config.fetch(:jack_port) { @prompt.select(\"Select the capture port:\", ports) }\n      @config.set(:jack_port, value: selected_port)\n      @config.write(force: true)\n\n      selected_port\n    else\n      puts 'jack_lsp does not appear to be installed. Exiting...'\n      exit\n    end\n  end\nend\n"
    },
    {
      "filename": "99-local.rules",
      "path": "/lib/autosync/",
      "content": "#\nSUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"0bc2\", ATTRS{idProduct}==\"ac2d\", ACTION==\"add\", TAGS+=\"systemd\", ENV{SYSTEMD_WANTS}==\"autosync.service\"\n"
    },
    {
      "filename": "autosync.service",
      "path": "/lib/autosync/",
      "content": "# service to watch for usb presence\n[Unit]\nDescription=MediaSync trigger\nRequires=dev-disk-by\\x2dlabel-mediasync.device\nAfter=dev-disk-by\\x2dlabel-mediasync.device\n\n[Service]\nExecStart=/usr/local/bin/autosync.sh\n\n[Install]\nWantedBy=dev-disk-by\\x2dlabel-mediasync.device\n"
    },
    {
      "filename": "autosync.sh",
      "path": "/lib/autosync/",
      "content": "/mnt/autosync#!/bin/sh\n# Script to do a two-way sync with a USB drive\n\nif [ $(mount | grep -c /mnt/autosync) != 1 ]\nthen\n  /sbin/mount /dev/disk/by-uuid/5732DF544868E675 || exit 1\n  echo \"/mnt/autosync is now mounted\"\n\n  rsync -rtuv --modify-window=1 --size-only --exclude \".csync*\" --exclude \".owncloud*\" /mnt/autosync/ /srv/share/media/\n  rsync -rtuv --modify-window=1 --size-only --exclude \".csync*\" --exclude \".owncloud*\" --delete /srv/share/media/ /mnt/autosync/\n\n  sync\n\n  # sudo -u bob DISPLAY=:0 DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus notify-send \"SYNCSHARE Finished Synchronizing!\"\n\n  sleep 5\n\n  umount /mnt/autosync\nelse\n  rsync -rtuv --modify-window=1 --size-only --exclude \".csync*\" --exclude \".owncloud*\" /mnt/autosync/ /srv/share/media/\n  rsync -rtuv --modify-window=1 --size-only --exclude \".csync*\" --exclude \".owncloud*\" --delete-delay /srv/share/media/ /mnt/autosync/\n\n  sync\n\n  # sudo -u bob DISPLAY=:0 DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus notify-send \"SYNCSHARE Finished Synchronizing!\"\n\n  sleep 5\n\n  umount /mnt/autosync\nfi\n"
    },
    {
      "filename": "cli.rb",
      "path": "/lib/",
      "content": "#!/usr/bin/env ruby\n# frozen_string_literal: false\n\nrequire \"drydock\"\nrequire \"highline/import\"\n\nmodule RobotStuff\n  class CLI\n    extend Drydock\n\n    default :welcome\n    debug :on\n\n    before do\n      @hostname = `cat /etc/hostname`.strip\n    end\n\n    about \"about\"\n    command :about do |_obj|\n      puts \"hey...this is what this is about!\"\n    end\n\n    about \"transcribe\"\n    command :transcribe do |_obj|\n      #TOOD\n    end\n\n  end # end cli class\nend # end soundbot module\n"
    },
    {
      "filename": "command.rb",
      "path": "/lib/",
      "content": "#!/usr/bin/env ruby\n# frozen_string_literal: true\n\nrequire 'tty-command'\n\nmodule RobotStuff\n  module_function\n\n  module Command\n    module_function\n\n    def run(*args)\n      cmd = TTY::Command.new(printer: :pretty)\n      result = cmd.run(args.join(' '), only_output_on_error: true)\n      return result\n    end\n\n    # use this if captured output is desired...\n    def tty(*args)\n\n      cmd = TTY::Command.new(output: $logger, uuid: false, timeout: 15)\n\n      begin\n        out, err = cmd.run(args.join(' '), only_output_on_error: true)\n      rescue TTY::Command::ExitError => e\n        $logger.debug \"#{e} #{args}\"\n        exit\n      rescue TTY::Command::TimeoutExceeded => e\n        $logger.debug \"#{e} #{args}\"\n      ensure\n        results = out\n      end\n\n    end\n\n    # Execute system command with shell aliasing on\n    def zsh(*args)\n      zsh = \"zsh -lc 'source ~/.zshrc && setopt aliases'\"\n\n      tty(\"#{zsh} && #{args.join(' ')}\")\n    end\n\n    def open(*args)\n      require 'open3'\n      options = args[-1].is_a?(Hash) ? args.pop : {}\n\n      fork do\n        stdin, stdout, stderr = Open3.popen3(*args)\n        options[:error] ? stderr.read : stdout.read\n      end\n\n    end\n\n    def status(service)\n      `pgrep -l #{service}`.chomp\n    end\n\n    def forkoff(command)\n      fork do\n        exec(command)\n      end\n    end\n\n  end # end Command class\nend\n"
    },
    {
      "filename": "config.rb",
      "path": "/lib/",
      "content": "#!/usr/bin/env ruby\n# frozen_string_literal: true\n\nrequire \"fileutils\"\nrequire \"pathname\"\nrequire \"yaml\"\n\nmodule RobotStuff\n\n  class Config\n\n    def self.load_config\n\n      default_config = File.join(APP_ROOT, \"config.default.yml\")\n      conf_directory = Pathname.new(CONFIG_DIR)\n\n      unless Pathname.new(CONFIG_DIR).exist?\n        conf_directory.mkdir unless conf_directory.exist?\n        FileUtils.cp(default_config, CONFIG)\n      end\n\n      YAML.load_file(CONFIG)\n\n    end\n\n    def self.get(attribute)\n      return Config.load_config[attribute]\n    end\n\n    def self.set\n      File.open(CONFIG, \"w\") { |file| file.write $CONFIG_DIR.to_yaml }\n    end\n\n  end\n\nend\n\n$config = RobotStuff::Config.load_config\n"
    },
    {
      "filename": "easyssh.rb",
      "path": "/lib/",
      "content": "#!/usr/bin/env ruby\n# frozen_string_literal: true\n\nmodule EasySSH\n\tmodule_function\n\n\trequire 'net/ssh'\n\n  USER = ENV[\"USER\"]\n  PRIVATEKEY = File.join(ENV[\"HOME\"], \".ssh\", \"id_rsa\")\n\n\t#http://stackoverflow.com/a/3386375\n    def ssh_exec!(ssh, command)\n      stdout_data = \"\"\n      stderr_data = \"\"\n      exit_code = nil\n      exit_signal = nil\n      ssh.open_channel do |channel|\n      \t#to resolve \"sudo: sorry, you must have a tty to run sudo\" have the ssh connection request a psuedo terminal\n      \tchannel.request_pty do |channel, success|\n      \t\traise \"could not request pty\" unless success\n\n          channel.exec(command) do |ch, success|\n              unless success\n                abort \"FAILED: couldn't execute command (ssh.channel.exec)\"\n              end\n\n              channel.on_data do |ch,data|\n                stdout_data+=data\n              end\n\n              channel.on_extended_data do |ch,type,data|\n                stderr_data+=data\n              end\n\n              channel.on_request(\"exit-status\") do |ch,data|\n                exit_code = data.read_long\n              end\n\n              channel.on_request(\"exit-signal\") do |ch, data|\n                exit_signal = data.read_long\n              end\n          end\n      end\n    end\n      ssh.loop\n      [stdout_data, stderr_data, exit_code, exit_signal]\n    end\n\n\n    def run_remote_command(host, command)\n    \t# keep in mind that sudo will probably be required....\n    \t# if this script is run as a user with ssh-keys setup,\n      # you need not specify user/keys....\n\n    \t#start a session\n    \tsession = Net::SSH.start(host, USER, :keys => [ \"#{PRIVATEKEY}\"])\n\n      # returning an array of stdout, stderr, exit code|signal\n    \tssh_command_output = ssh_exec!(session, command).inspect\n\n      # when calling Net::SSH.start without a block, you must explictly close the session\n    \tsession.close\n\n    \treturn ssh_command_output\n\n    end\n\nend\n"
    },
    {
      "filename": "file_select.rb",
      "path": "/lib/",
      "content": "# Class responsible for selecting the markdown file\nclass MarkdownFile\n  def initialize(config)\n    @config = config\n  end\n\n  # Selects the markdown file to paste the transcription\n  def select_markdown_file\n    last_selected_file = @config.fetch(:last_selected_file, nil)\n\n    markdown_files = file_list(NOTEBOOK)\n\n    markdown_files.delete_if { |file| file.include?(\"chatGPTexports\")}\n\n    selected_file = @config.fetch(:selected_file) { prompt.select(\"Select the markdown file:\", markdown_files, default:\n    last_selected_file) }\n\n    @config.set(:selected_file, value: selected_file)\n    @config.set(:last_selected_file, value: selected_file)\n\n    @config.write(force: true)\n    selected_file\n  end\n\n  private\n\n  # Returns an array of markdown file paths\n  def file_list(path)\n    Dir.glob(File.join(path, \"**/*{,/*/**}.{md,markdown}\"))\n  end\n\n  # TTY::Prompt instance for user interaction\n  def prompt\n    @prompt ||= TTY::Prompt.new\n  end\nend\n"
    },
    {
      "filename": "glob.rb",
      "path": "/lib/",
      "content": "#!/usr/bin/env ruby\n# frozen_string_literal: true\n\nmodule Glob\n  module_function\n\n  FD = \"/usr/bin/fd\"\n  FD_OPTS = \"-a -L --show-errors\"\n\n  def cmd\n    TTY::Command.new(output: $logger, 2 => 1)\n  end\n\n  def fd(pattern,type,path)\n    cmd.run(\"#{FD} #{pattern} -t #{type} #{FD_OPTS} #{path}\", only_output_on_error: true)\n  end\n\n  # force an encoding and replace marks with nothing\n  def fix_encoding(input)\n    output = input.encode('UTF-8', 'binary', invalid: :replace, undef: :replace, replace: '')\n  end\n\n  def folders(path,regex=nil)\n    regex = \".\" if regex.nil?\n    results = fd(regex, \"d\", path)\n    folders = fix_encoding(results.out)\n    return folders.split(\"\\n\").sort\n  end\n\n  def files(path,regex=nil)\n    regex = \".\" if regex.nil?\n    results = cmd.run(\"#{FD} #{regex} -t f #{FD_OPTS} #{path}\", :err => :out, only_output_on_error: true)\n\n    files = fix_encoding(results.out)\n    return files.split(\"\\n\").sort\n  end\n\n  def sounds(path)\n    return Dir.glob(File.join(path, \"**/*{,/*/**}.{wav,flac,mp3,aiff,ogg,wma,opus,m4a}\"))\n  end\n\n  def videos(path)\n    return Dir.glob(File.join(path, \"**/*{,/*/**}.{mp4,mkv}\"))\n  end\n\n  def soundfonts(path)\n    return Dir.glob(File.join(path, \"**/*{,/*/**}.{sfz,sf2,gig}\"))\n  end\n\n  def midi(path)\n    return Dir.glob(File.join(path, \"**{,/*/**}/*.{mid,midi}\"))\n  end\n\nend\n"
    },
    {
      "filename": "i3_nav.rb",
      "path": "/lib/",
      "content": "#!/usr/bin/env ruby\n\nclass WorkspaceSwitcher\n  def initialize(file)\n    @file = file\n  end\n\n  # Opens the selected file in VSCode\n  def open_file_in_vscode\n    system(\"code --goto #{@file}\")\n  end\nend\n"
    },
    {
      "filename": "loger.rb",
      "path": "/lib/",
      "content": "#!/usr/bin/env ruby\n# frozen_string_literal: true\n\nrequire 'logging'\nrequire 'pathname'\nrequire 'pry-stack_explorer'\n\nLOG_DIR = File.join(File.expand_path(\"../../\", __FILE__), \"logs\")\nLOG = File.join(LOG_DIR, \"#{Time.now.strftime(\"%Y-%m-%d\")}.log\")\n\nLOG_MAX_SIZE = 6_145_728\nLOG_MAX_FILES = 10\n\nunless Pathname.new(LOG_DIR).exist?\n  Pathname.new(LOG_DIR).mkdir\n  puts \"creating log directory!\"\nend\n\nLogging.color_scheme( 'bright',\n  :levels => {\n    :info  => :green,\n    :warn  => :yellow,\n    :error => :red,\n    :fatal => [:white, :on_red]\n  },\n  :date => :blue,\n  :logger => :cyan,\n  :message => :magenta\n)\n\nLogging.appenders.stdout(\n  'stdout',\n  :layout => Logging.layouts.pattern(\n    :pattern => '[%d] %-5l %c: %m\\n',\n    :color_scheme => 'bright'\n  ),\n  :level => :debug\n)\n\nLogging.appenders.file(\n  LOG,\n  :layout => Logging.layouts.pattern(\n    :pattern => '[%d] %-5l %c: %m\\n',\n    :color_scheme => 'bright'\n  ),\n  :level => :debug\n)\n\n$logger = Logging.logger['Happy::Colors']\n\n$logger.add_appenders(\n        Logging.appenders.stdout,\n        Logging.appenders.file(LOG))\n\n$logger.debug\n\ndef test\n  $logger.debug \"this is what is happening...all of it\"\n  $logger.info \"basically what is happening\"\n  $logger.warn \"Hey, watch out\"\n  $logger.error StandardError.new(\"an error has occurred, continue on to next task\")\n  $logger.fatal \"an error has occured. nothing more can happen.\"\nend\n\n$daemon_options = {\n  :log_output => true,\n  :backtrace => true,\n  :output_logfilename => LOG,\n  :log_dir =>  LOG_DIR,\n  :ontop => false\n}\n"
    },
    {
      "filename": "lsmi.tar.gz",
      "path": "/lib/",
      "content": null
    },
    {
      "filename": "transcribe.rb",
      "path": "/lib/",
      "content": "#!/usr/bin/env ruby\n\nrequire 'shellwords'\nrequire 'clipboard'\nrequire 'tty-command'\nrequire 'tty-reader'\n\n\n# Class that represents the transcription command\nclass TranscribeCommand\n  TEMP_FILE_PATH = \"/tmp/#{SecureRandom.uuid}.wav\".freeze\n  JACK_CAPTURE = TTY::Which.which('jack_capture')\n  ARECORD = TTY::Which.which('arecord')\n\n  def initialize(selected_port, selected_file, openai_access_token)\n    @selected_port = selected_port\n    @selected_file = selected_file\n    @openai_access_token = openai_access_token\n    @cmd = TTY::Command.new(pty: true)\n    @reader = TTY::Reader.new(interrupt: :signal)\n  end\n\n  # Executes the transcription command\n  def execute\n    if alsa_device?\n      record_audio_with_alsa(480, @selected_port)\n    else\n      record_audio_with_jack(480, @selected_port)\n    end\n\n    # Transcribe audio\n    transcription = transcribe_audio(TEMP_FILE_PATH)\n\n    #Clipboard.copy(\"#{transcription}\")\n\n    system(\"echo #{transcription} | xclip -selection clipboard\")\n    #\n    # # # puts \"#{clipb}\"\n    # fork do\n    #   exec(\"xclip -o | xclip -selection clipboard\")\n    # end\n\n    # Print transcription\n    puts 'Transcription:'\n    puts transcription\n\n    # Write transcription to the selected file\n    write_transcription_to_file(transcription, @selected_file)\n\n    # Delete temporary WAV file\n    delete_temp_file\n    exit\n  end\n\n  private\n\n  # Checks if the selected port is an ALSA device\n  def alsa_device?\n    @selected_port.start_with?('sysdefault:')\n  end\n\n\n  # Records audio using arecord\n  def record_audio_with_alsa(duration, selected_port)\n    stop_recording = false\n\n    recording_pid = Process.spawn(\"#{ARECORD} -D 'sysdefault:CARD=#{selected_port}' -v -V mono -d #{duration} -f S16_LE -r 48000 #{TEMP_FILE_PATH}\")\n\n    input_thread = Thread.new do\n      print \"Press '8' to stop recording early: \"\n      puts \"\\n\\n\"\n      loop do\n\n        user_input = getkeyinput\n        puts \"Key Pressed: #{user_input.inspect}\"\n\n        break if user_input == '8'\n      end\n      stop_recording = true\n      Process.kill('TERM', recording_pid) # Send command to stop recording\n    end\n\n    input_thread.join\n    Process.wait(recording_pid)\n\n    puts '\\n\\nRecording stopped early\\n' if stop_recording\n  end\n\n  # Records audio using jack_capture\n  def record_audio_with_jack(duration, selected_port)\n    stop_recording = false\n\n    recording_pid = Process.spawn(\"#{JACK_CAPTURE} -dc -hbu -p '#{selected_port}' -d #{duration} #{TEMP_FILE_PATH}\")\n\n    input_thread = Thread.new do\n      print \"Press '8' to stop recording early: \"\n\n      loop do\n        x = @reader.read_keypress(nonblock: true)\n\n        if x == \"9\"\n          @copy_to_clipboard = true\n          break\n        elsif x == \"8\"\n          break\n        end\n\n      end\n\n      stop_recording = true\n      Process.kill('TERM', recording_pid) # Send command to stop recording\n    end\n\n    input_thread.join\n    Process.wait(recording_pid)\n\n    puts 'Recording stopped early' if stop_recording\n  end\n\n  # Transcribes the audio file\n  def transcribe_audio(filename)\n\n    begin\n      client = OpenAI::Client.new(access_token: @openai_access_token)\n\n      response = client.transcribe(\n        parameters: {\n          model: 'whisper-1',\n          file: File.open(filename, 'rb')\n        }\n      )\n\n      if response[\"error\"]\n        $logger.fatal(\"#{response[\"error\"][\"message\"]}\")\n        exit\n      else\n        response['text']\n      end\n\n    rescue StandardError => e\n      $logger.fatal(\"something failed #{e}\")\n    end\n\n  end\n\n  # Writes the transcription to the selected file\n  def write_transcription_to_file(transcription, file)\n    File.open(file.shellescape, 'a') { |f| f.write(\"\\n#{transcription}\\n\") }\n    $logger.debug(\"Transcription written to file. #{file}\")\n    #FIXME: record_audio_with_jack doesn't exit gracefully,\n    # so as a work-around we exit here\n    exit\n  end\n\n  # Deletes the temporary WAV file\n  def delete_temp_file\n    File.delete(TEMP_FILE_PATH)\n  end\nend\n"
    },
    {
      "filename": "add_bridge.sh",
      "path": "/staging/",
      "content": "#!/usr/bin/env bash\n\necho \"enter bridge interface name\"\nbridge_name=$(gum input --placeholder=\"br0\" --value=\"br0\")\n\necho \"choose interface to use as a bridge \"\nbridge_slave=$(ifconfig | awk '{print $1}'|grep en| sed 's/://' |gum choose)\n\necho \"enter ip address for bridge interface\"\ncurrent_ip=$(ip address | grep ${bridge_slave} | grep inet | awk '{print $2}')\nipaddr=$(gum input --placeholder=\"${current_ip}\" --value=\"${current_ip}\")\n\necho \"enter gateway\"\ngateway=$(gum input --placeholder=\"192.168.41.1\" --value=\"192.168.41.1\")\n\necho \"enter dns\"\ndns=$(gum input --placeholder=\"192.168.41.1\" --value=\"192.168.41.1\")\n\necho \"enter dns search\"\nsearch=$(gum input --placeholder=\"syncopated.net\" --value=\"syncopated.net\")\n\nclear\n\nprintf \"bridge name: $bridge_name \\nbridge slave: $bridge_slave\\n\"\nprintf \"bridge address: $ipaddr\\nbridge gateway: $gateway\\n\"\nprintf \"bridge dns: $dns\\nbridge dns search: $search\\n\"\necho -e \"----------------------\"\n\ngum confirm \"does this all look correct?\"\n\nif [ $? = 0 ]; then\n  nmcli connection add type bridge autoconnect yes con-name ${bridge_name} ifname ${bridge_name}\n\n  nmcli connection modify br0 ipv4.addresses ${ipaddr} ipv4.method manual\n\n  nmcli connection modify br0 ipv4.gateway ${gateway}\n\n  nmcli connection modify br0 ipv4.dns ${dns}\n\n  nmcli connection modify br0 ipv4.dns-search ${search}\n\n  nmconnection=$(nmcli connection show | grep ${bridge_slave} | awk -F '  ' '{print $1}')\n\n  nmcli connection delete \"${nmconnection}\"\n\n  nmcli connection add type bridge-slave autoconnect yes con-name ${bridge_slave} ifname ${bridge_slave} master ${bridge_name}\n\n  gum confirm \"reboot?\" && sudo shutdown -r now\n\nelse\n  echo \"ok then, try again. exiting\"\n  exit\nfi\n"
    },
    {
      "filename": "audiosplitter.rb",
      "path": "/staging/",
      "content": "#!/usr/bin/env ruby\n# frozen_string_literal: true\n\n# #! /bin/bash\n# ffmpeg -i Long_Audio_File.mp3 -af silencedetect=d=0.1 -f null - |& awk '/silence_end/ {print $4,$5}' | awk '{S=$2;printf \"%d:%02d:%02d\\n\",S/(60*60),S%(60*60)/60,S%60}'\n#\n# #! /bin/bash\n# x=\"00:00:00\"\n# z=0\n# filename=$(basename -- \"$2\")\n# ext=\"${filename##*.}\"\n# filename=\"${filename%.*}\"\n# initcmd=\"ffmpeg  -nostdin -hide_banner -loglevel error -i $2\"\n# while read y\n# do\n# initcmd+=\" -ss $x -to $y -c copy $filename$z.$ext\"\n# let \"z=z+1\"\n# x=$y\n# done < $1\n# $initcmd\n\n\n\nrequire 'open3'\nrequire 'fileutils'\n\nfilename = ARGV[1]\next = File.extname(filename)\nbasename = File.basename(filename, ext)\n\n# Run the ffmpeg command to detect silence and parse the output\noutput, _error, _status = Open3.capture3(\n  \"ffmpeg -i #{filename} -af silencedetect=d=0.1 -f null -\"\n)\ntimechunks = output.scan(/silence_end: ([\\d\\.]+)/).map { |match| match[0] }\n\n# Convert the timechunks into HH:MM:SS format\ntimechunks.map! do |chunk|\n  seconds = chunk.to_i\n  hours = seconds / (60 * 60)\n  seconds %= (60 * 60)\n  minutes = seconds / 60\n  seconds %= 60\n  sprintf(\"%02d:%02d:%02d\", hours, minutes, seconds)\nend\n\n# Process the chunks\ninitcmd = \"ffmpeg -nostdin -hide_banner -loglevel error -i #{filename}\"\ntimechunks.each_with_index do |chunk, index|\n  initcmd += \" -ss #{index.zero? ? '00:00:00' : timechunks[index-1]} -to #{chunk} -c copy #{basename}#{index}#{ext}\"\nend\n\n# Run the final ffmpeg command\nsystem(initcmd)\n"
    },
    {
      "filename": "errlog.sh",
      "path": "/staging/",
      "content": "#!/usr/bin/env bash\n\ndeclare -rx ERRLOG=\"$HOME/errlog.md\"\n\ncat <<EOF >$ERRLOG\n# Good news!\n\n> The system made it this far.\n> You have something to live for.\n> You're doing great.\n\n# log entries of note for $(date)\n\\`\\`\\`bash\n$(journalctl -p 3 -b )\n\\`\\`\\`\n\n# services that failed to start:\n\n\\`\\`\\`ruby\n$(systemctl --failed)\n\\`\\`\\`\n\n\nEOF\n\n# i3-msg \"exec /usr/bin/kitty --hold --class 'changelog' sh -c 'cat ~/changelog.md | gum format -t markdown | gum style --border thick --margin '10 10' --padding '1 1'  --border-foreground 198 --background 34 --foreground 12'\"\n\ni3-msg \"exec --no-startup-id /usr/bin/kitty --hold --class 'changelog' sh -c 'cat ~/errlog.md | gum format -t markdown | gum style --border-foreground 12 --background 0 --foreground 58 --border normal'\"\n\n# # /usr/bin/kitty --hold --class 'changelog' sh -c 'gum style --border normal --margin \"10 10\" --padding \"1 2\" --border-foreground 212 < $ERRLOG | gum format -t code'\n"
    },
    {
      "filename": "gcommit.rb",
      "path": "/staging/",
      "content": "#!/usr/bin/env ruby\nrequire 'ruby/openai'\nrequire 'logging'\nrequire 'clipboard'\n\n# Set up logging configuration\nLogging.color_scheme('bright',\n  :levels => {\n    :info  => :green,\n    :warn  => :yellow,\n    :error => :red,\n    :fatal => [:white, :on_red]\n  },\n  :date => :blue,\n  :logger => :cyan,\n  :message => :magenta\n)\n\n# Configure appenders\nstdout_appender = Logging.appenders.stdout(\n  'stdout',\n  :layout => Logging.layouts.pattern(\n    :pattern => '[%d] %-5l %c: %m\\n',\n    :color_scheme => 'bright'\n  ),\n  :level => :debug\n)\n\n# Create a logger with appenders\n$logger = Logging.logger['gdiffsum']\n$logger.add_appenders(stdout_appender)\n\n# Set the log level\n$logger.level = :debug\n\n# Configure OpenAI with your API key\nOpenAI.configure do |config|\n  config.access_token = ENV['OPENAI_API_KEY']\n  config.request_timeout = 60 # Optional\nend\n\n# Create a new client\nclient = OpenAI::Client.new\n\n# Read the piped input\ndiff = ARGF.read\n\n# Best practices for commit messages\nbest_practices = \"\"\"\nAnalyze the git diff and summarize the changes for a git commit message.\nUse past tense.\nDo not end a line with a period.\nSeparate subject from body with a blank line.\nLimit the subject line to 50 characters.\nUse the imperative mood in the subject line.\nTry to ascertain why each change as made.\nWrap the body at 72 characters.\n\"\"\"\n\nSTREAMING_TIMEOUT = -0.1150\n\ndef time_check(elapsed_time)\n  #p elapsed_time\n  if elapsed_time < STREAMING_TIMEOUT\n    error_message = \"Error: No new response received within 5 seconds after the last response has been processed.\"\n    res = { \"type\" => \"error\", \"content\" => \"ERROR: #{error_message}\" }\n    #pp res\n    $logger.debug(res)\n    sleep 1\n    return res\n  end\nend\n\n\nputs \"-----------------\"\n\n# Initialize the JSON variable and the last processed time.\njson = nil\nend_time = Time.now\n\nbegin\n  # Generate a summary of the diff using OpenAI\n  response = client.chat(\n    parameters: {\n      model: \"gpt-3.5-turbo-16k-0613\",\n      messages: [\n        {role: \"system\", content: best_practices},\n        {role: \"user\", content: \"###\\n#{diff}\\n###\"}\n      ],\n      temperature: 0.8,\n      top_p: 0.5,\n      max_tokens: 2000,\n      frequency_penalty: -0.3,\n      presence_penalty: 0.4,\n      stream: proc do |chunk, _bytesize|\n        # $logger.debug(chunk)\n        current_time = Time.now\n        content = chunk.dig(\"choices\", 0, \"delta\", \"content\")\n        #p content\n        print content.gsub(\"`\", \"'\") unless content.nil?\n        sleep 0.1125\n        end_time = Time.now\n        elapsed_time = (current_time - end_time).to_f\n        time_check(elapsed_time)\n      end\n    }\n  )\n  # $logger.debug(response)\n  #\n  # # Get the summary and escape it for use in a shell command\n  # summary = response['choices'][0]['message']['content'].strip\n  #\n  # puts \"#{summary}\"\nrescue StandardError => e\n  $logger.fatal(\"#{e}\")\nend\n"
    },
    {
      "filename": "jmeters_pulse.rb",
      "path": "/staging/",
      "content": "#!/usr/bin/env ruby\n# frozen_string_literal: true\n\nrequire 'tty-cursor'\nrequire 'tty-prompt'\nrequire 'yaml'\n\ndef forkoff(command)\n  fork do\n    exec(command.to_s)\n  end\nend\n\n\nmeterAliases = File.join(ENV['HOME'], \"Studio\", \"projects\", \"meters.yml\")\n\nmonitor_ports = YAML.load_file(meterAliases).map { |ports| ports.map { |port| \"'#{port}'\"} }\n\n\nforkoff(\"jmeters -t vu -c 4 #{monitor_ports.flatten.join(' ')}\")\n"
    },
    {
      "filename": "kill-midi.sh",
      "path": "/staging/",
      "content": "#!/usr/bin/env bash\n\n# ensure midi through port-0 gets connected to objects that play sound.\n# this will ensure the `sendmidi panic` binding functions as it should.\n\n\nMIDI_IN=(\"REAPER:MIDI Input 1\"\n         \"LinuxSampler:midi_in_0\"\n         \"Helm:lv2_events_in\"\n        )\n\nfor midi_in in \"${MIDI_IN[@]}\"; do\n  jack_connect \"a2j:Midi Through [14] (capture): Midi Through Port-0\" \"$midi_in\" || continue\ndone\n"
    },
    {
      "filename": "lsmi.sh",
      "path": "/staging/",
      "content": "#!/usr/bin/env bash\n\n# utilize usb keyboard as a midi controller\n\nlsmi=\"/usr/local/bin/lsmi-keyhack\"\nkeydb=\"$HOME/Library/configs/lsmi/keydb_01\"\n\nkeyboard_id=$(xinput list | grep 'Dell KB216 Wired Keyboard' |\n                            awk -F ' ' '{ print $6 }' |\n                            grep id |\n                            awk -F '=' '{ print $2 }')\n\nif [[ -z \"$keyboard_id\" ]];\nthen\n  systemd-cat -t \"lsmi\" echo \"this keyboard is not connected\"\nelse\n  if ! pgrep -x \"lsmi-keyhack\" > /dev/null\n  then\n    keyboard_event=$(xinput --list-props $keyboard_id | grep event |\n                                                        awk -F \" \" '{ print $4 }')\n\n    i3-msg \"exec --no-startup-id $lsmi -k $keydb -d $keyboard_event &!\"\n  fi\nfi\n"
    },
    {
      "filename": "makeh2kit",
      "path": "/staging/",
      "content": "#!/bin/bash\n#\n# makeh2kit by Andrew M Taylor 2013\n#\n# This is a script to auto-generate h2drumkit files. Requires bash v4 and gawk\n#\n# For a full list of options, run 'makeh2kit -h'\n#\n# This is free and unencumbered software released into the public domain.\n# Please read UNLICENSE for more details, or refer to <http://unlicense.org/>\n\n\ndeclare -ra ARGS=(\"$@\")\ndeclare -ri NUM_ARGS=$#\n\ndeclare -ri MAX_LAYERS=16\n\ndeclare -ri CYAN=3\ndeclare -ri RED=4\ndeclare -ri YELLOW=6\ndeclare -ri WHITE=7\n\ndeclare -ri ERROR=-1\n\ndeclare -a instrumentNames\ndeclare -ia instrNumLayers\ndeclare -ia instrFilesIndex\ndeclare -ia instrMidiNotes\ndeclare -a instrFilePaths\ndeclare -a origFileNames\ndeclare -a newFileNames\ndeclare -a fileSearchPaths\ndeclare -a dirSearchPaths\n\ndeclare kitName=\"${PWD##*/}\" # Kit name defaults to present working directory\ndeclare kitAuthor=\"\"\ndeclare kitInfo=\"\"\ndeclare kitLicense=\"\"\ndeclare fileType=\"flac\"\ndeclare layerMarker=\"\"\ndeclare -r infoHtmlTagOpen=\"&lt;!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.0//EN\\\" \\\"http://www.w3.org/TR/REC-html40/strict.dtd\\\">\n&lt;html>&lt;head>&lt;meta name=\\\"qrichtext\\\" content=\\\"1\\\" />&lt;style type=\\\"text/css\\\">\np, li { white-space: pre-wrap; }\n&lt;/style>&lt;/head>&lt;body style=\\\" font-family:'Lucida Grande'; font-size:10pt; font-weight:400; font-style:normal;\\\">\n&lt;p style=\\\" margin-top:0px; margin-bottom:0px; margin-left:0px; margin-right:0px; -qt-block-indent:0; text-indent:0px;\\\">\"\ndeclare -r infoHtmlTagClose=\"&lt;/p>&lt;/body>&lt;/html>\"\n\ndeclare isReverseSortOn=false\ndeclare isInteractiveModeOn=false\ndeclare isServiceMenuModeOn=false\n\n\nfunction setTextColour() { tput setf $1; }\n\nfunction startUnderline() { tput smul; }\n\nfunction stopUnderline() { tput rmul; }\n\nfunction moveCursorUpOneLine() { tput cuu1; }\n\nfunction resetText() { echo -en \"\\e[0m\"; }\n\n\nfunction exitWithError() \n{\n    declare errorMessage=\"$1\"\n    \n    setTextColour $RED\n    \n    # 1>&2 means redirect text headed for stdout to stderr\n    echo -e \"\\n$errorMessage\\n\" 1>&2\n    \n    resetText\n    \n    exit 1\n}\n\n\nfunction showHelp() \n{\n    cat << EOF\n\nUsage:  makeh2kit [options]\n        makeh2kit [options] --files 'file name' ...\n\nRunning makeh2kit with no arguments will scan the current directory \nfor flac files and create a h2drumkit file named after the current \ndirectory.\n\nAudio files can be treated as indiviual instruments or as instrument \nlayers grouped either by sub-directory or by file name with numeric \nprefixes/suffixes indicating the layers.\n\nHydrogen kits that \"map\" SFZ drum kits can also be created. The \nresulting kit won't contain any audio files and will instead trigger \nthe MIDI note of each instrument in the SFZ kit.\n\nOptions:\n  -n, --name 'KIT NAME'         Drumkit name\n  -a, --author 'AUTHOR'         Name of author\n  -l, --license 'LICENSE'       License to use for the drumkit\n  -i, --info 'INFO'             Additional info; accepts HTML-\n                                formatting tags\n  -f, --format FORMAT           File format; FORMAT may be \n                                flac (default), wav, au, aiff, or sfz\n  -L, --layers LAYER_MARKER     Files are grouped into instruments\n                                according to a \"layer marker\"\n                                LAYER_MARKER may be dirs, prefix, or\n                                suffix (or d, p, or s)\n  -r, --reverse                 File names are sorted in reverse order\n  -I, --interactive             Run in \"interactive\" mode, where user\n                                is prompted for input\n  -h, --help                    Display this help and exit\n\nN.B. When supplying arguments to options -n, -a, -l, and -i use\nsingle rather than double quotes to allow for use of special\ncharacters\n\nEOF\n}\n\n\nfunction setFileType() \n{\n    declare input=$1\n\n    # check if user input is a valid file type\n    case \"$input\" in\n        [aA][iI][fF][fF] )  fileType=\"aiff\";;\n        [aA][uU] )          fileType=\"au\";;\n        [fF][lL][aA][cC] )  fileType=\"flac\";;\n        [wW][aA][vV] )      fileType=\"wav\";;\n        [sS][fF][zZ] )      fileType=\"sfz\";;\n        * )                 exitWithError \"File type must be flac, wav, au, aiff, or sfz\";;\n    esac\n}\n\n\nfunction setLayerMarker()\n{\n    declare input=$1\n\n    # check if user input is valid\n    case \"$input\" in\n        [dD][iI][rR][sS] )          layerMarker=\"dirs\";;\n        [dD] )                      layerMarker=\"dirs\";;\n        [pP][rR][eE][fF][iI][xX] )  layerMarker=\"prefix\";;\n        [pP] )                      layerMarker=\"prefix\";;\n        [sS][uU][fF][fF][iI][xX] )  layerMarker=\"suffix\";;\n        [sS] )                      layerMarker=\"suffix\";;\n        * )                         exitWithError \"Layer marker must be dirs, prefix, or suffix (or d, p, s)\";;\n    esac\n}\n\n\nfunction setSearchPaths()\n{\n    declare -i param=$1\n\n    while (( param < NUM_ARGS )); do\n\n        if [ -f \"${ARGS[$param]}\" ]; then\n            fileSearchPaths+=(\"${ARGS[$param]}\")\n        elif [ -d \"${ARGS[$param]}\" ]; then\n            dirSearchPaths+=(\"${ARGS[$param]}\")\n        fi\n\n        (( param++ ))\n    done\n}\n\n\nfunction processArgs()\n{\n    declare -i i=0\n\n    if (( NUM_ARGS > 0 )); then\n        while (( i < NUM_ARGS )); do\n            case ${ARGS[$i]} in\n                -n )                (( i++ )); kitName=${ARGS[$i]};;\n                --name )            (( i++ )); kitName=${ARGS[$i]};;\n                -a )                (( i++ )); kitAuthor=${ARGS[$i]};;\n                --author )          (( i++ )); kitAuthor=${ARGS[$i]};;\n                -i )                (( i++ )); kitInfo=${ARGS[$i]};;\n                --info )            (( i++ )); kitInfo=${ARGS[$i]};;\n                -l )                (( i++ )); kitLicense=${ARGS[$i]};;\n                --license )         (( i++ )); kitLicense=${ARGS[$i]};;\n                -f )                (( i++ )); setFileType ${ARGS[$i]};;\n                --format )          (( i++ )); setFileType ${ARGS[$i]};;\n                -L )                (( i++ )); setLayerMarker ${ARGS[$i]};;\n                --layers )          (( i++ )); setLayerMarker ${ARGS[$i]};;\n                -r )                isReverseSortOn=true;;\n                --reverse )         isReverseSortOn=true;;\n                -I )                isInteractiveModeOn=true;;\n                --interactive )     isInteractiveModeOn=true;;\n                --service-menu )    isServiceMenuModeOn=true;;\n                -h )                showHelp; exit 0;;\n                --help )            showHelp; exit 0;;\n                --files )           (( i++ )); setSearchPaths $i; break;;\n                * )                 exitWithError \"Invalid option: \\\"${ARGS[$i]}\\\". See 'makeh2kit -h' for usage\";;\n            esac\n            (( i++ ))\n        done\n    fi\n}\n\n\nfunction guessFileTypeFromFileList()\n{\n    declare filePath=\"\"\n    declare prevFileType=\"\"\n    \n    fileType=\"\"\n    \n    for filePath in \"${fileSearchPaths[@]}\"; do\n        fileType=\"${filePath##*.}\"\n\n        if [[ $fileType != [fF][lL][aA][cC] ]] && [[ $fileType != [wW][aA][vV] ]] && \\\n           [[ $fileType != [aA][uU] ]] && [[ $fileType != [aA][iI][fF][fF] ]] && [[ $fileType != [sS][fF][zZ] ]]; then\n            fileType=\"\"\n        fi\n\n        if [ \"$prevFileType\" != \"\" ] && [ \"$fileType\" != \"$prevFileType\" ]; then\n            fileType=\"\"\n            break\n        fi\n        prevFileType=\"$fileType\"\n    done\n}\n\n\nfunction showWelcomeMessage()\n{\n    setTextColour $YELLOW; startUnderline; echo -e \"\\nMake h2drumkit\"; stopUnderline\n    \n    setTextColour $WHITE; cat << EOF\n\nThis script will auto-generate a h2drumkit file from the selected file(s).\nIf no files are selected it will scan the current directory for compatible\nfiles (flac, wav, au, aiff, or sfz).\n\nAudio files can be treated as indiviual instruments or as instrument layers\ngrouped either by sub-directory or by file name with numeric prefixes/suffixes\nindicating the layers.\n\nHydrogen kits that \"map\" SFZ drum kits can also be created. The resulting kit\nwill not contain any audio files and will instead trigger the MIDI note of\neach instrument in the SFZ kit.\n\nEOF\n}\n\n\n# This function only gets called if makeh2kit was launched from a KDE service menu\nfunction setWorkingDir()\n{    \n    # Try to work out present working directory from list of files passed on the command line\n    if (( ${#fileSearchPaths[@]} > 0 )); then\n        cd \"${fileSearchPaths[0]%/*}\"\n    elif (( ${#dirSearchPaths[@]} > 1 )); then\n        cd \"${dirSearchPaths[0]%/*}\"\n    elif (( ${#dirSearchPaths[@]} == 1 )); then\n        # Ask user if this is the PWD or a sub-dir\n        setTextColour $YELLOW; echo \"Selected directory: ${dirSearchPaths[0]}\"\n        setTextColour $WHITE; echo \"Should makeh2kit set this as the current directory?\"\n        echo -e \"yes - create files in ${dirSearchPaths[0]} (default)\\nno  - create files in ${dirSearchPaths[0]%/*}\"\n        setTextColour $CYAN; read -e input\n        \n        if [[ $input == \"\" ]]; then \n            input=\"yes\"; moveCursorUpOneLine; echo \"$input\"\n        fi\n\n        if [[ $input == [yY] ]] || [[ $input == [yY][eE][sS] ]]; then\n            cd \"${dirSearchPaths[0]}\"\n            unset dirSearchPaths\n        elif [[ $input == [nN] ]] || [[ $input == [nN][oO] ]]; then\n            cd \"${dirSearchPaths[0]%/*}\"\n        else\n            exitWithError \"Invalid value: \\\"$input\\\"\"\n        fi\n    else\n        exitWithError \"Error: no files or dirs passed on command line. \\nThe programmer has messed up!\"\n    fi\n}\n\n\nfunction getValuesFromUser()\n{\n    declare input=\"\"\n\n    guessFileTypeFromFileList\n    \n    showWelcomeMessage\n\n    setTextColour $YELLOW; echo -e \"Please enter the following details or simply press enter to accept the\\ndefault values...\\n\"\n    \n    if $isServiceMenuModeOn; then\n        setWorkingDir\n    fi\n    \n    # Set file type\n    if [[ $fileType == \"\" ]]; then\n        setTextColour $WHITE; echo \"File format (flac (default), wav, au, aiff, or sfz):\"\n        setTextColour $CYAN; read -e input\n        \n        if [[ $input == \"\" ]]; then \n            input=\"flac\"; moveCursorUpOneLine; echo \"$input\"\n        fi\n        \n        setFileType \"$input\"\n    fi\n\n    # Set kit name\n    if [[ $fileType != [sS][fF][zZ] ]]; then\n        setTextColour $WHITE; echo \"Drumkit name (default is name of current directory, \\\"${PWD##*/}\\\"):\"\n        setTextColour $CYAN; read -e kitName\n        \n        if [[ $kitName == \"\" ]]; then \n            kitName=\"${PWD##*/}\"; moveCursorUpOneLine; echo $kitName\n        fi\n    fi\n\n    # Set author\n    setTextColour $WHITE; echo \"Author:\"\n    setTextColour $CYAN; read -e kitAuthor\n\n    # Set license\n    setTextColour $WHITE; echo \"License:\"\n    setTextColour $CYAN; read -e kitLicense\n\n    # Set info\n    setTextColour $WHITE; echo \"Info (HTML formatting tags allowed):\"\n    setTextColour $CYAN; read -e kitInfo\n\n    # Set \"layer marker\"\n    if [[ $fileType != [sS][fF][zZ] ]]; then\n        setTextColour $WHITE; echo \"Set \\\"layer marker\\\" (none (default), dirs, prefix, or suffix (or d, p, s)):\"\n        setTextColour $CYAN; read -e input\n        \n        if [[ $input == \"\" ]]; then \n            input=\"none\"; moveCursorUpOneLine; echo \"$input\"\n        fi\n        \n        if [[ $input != \"none\" ]]; then \n            setLayerMarker \"$input\"\n        fi\n    fi\n\n    # Reverse sort order?\n    if [[ $fileType != [sS][fF][zZ] ]]; then\n        setTextColour $WHITE; echo \"Reverse sort order? (default is no):\"\n        setTextColour $CYAN; read -e input\n        \n        if [[ $input == \"\" ]]; then \n            input=\"no\"; moveCursorUpOneLine; echo \"$input\"\n        fi\n\n        if [[ $input == [yY] ]] || [[ $input == [yY][eE][sS] ]]; then\n            isReverseSortOn=true\n        elif [[ $input == [nN] ]] || [[ $input == [nN][oO] ]]; then\n            isReverseSortOn=false\n        else\n            exitWithError \"Invalid value: \\\"$input\\\"\"\n        fi\n    fi\n\n    setTextColour $WHITE\n}\n\n\nfunction makeInputXmlCompatible()\n{\n    # ${variable//</&lt;} - Every occurrence of \"<\" is replaced with \"&lt;\"\n\n    kitName=\"${kitName[$i]//</&lt;}\"\n    kitAuthor=\"${kitAuthor[$i]//</&lt;}\"\n    kitInfo=\"${kitInfo[$i]//</&lt;}\"\n    kitLicense=\"${kitLicense[$i]//</&lt;}\"\n}\n\n\nfunction makePathsRelative()\n{\n    declare -i i=0\n    \n    # Remove PWD from start of files and dirs so paths are relative rather than absolute\n    for (( i = 0; i < ${#fileSearchPaths[@]}; i++ )); do\n        fileSearchPaths[i]=\"${fileSearchPaths[i]#$PWD/}\"\n    done\n    \n    for (( i = 0; i < ${#dirSearchPaths[@]}; i++ )); do\n        if [[ ${dirSearchPaths[i]} == \"$PWD\" ]] || [[ ${dirSearchPaths[i]} == \"$PWD/\" ]]; then\n            dirSearchPaths[i]=\".\"\n        else\n            dirSearchPaths[i]=\"${dirSearchPaths[i]#$PWD/}\"\n        fi\n    done\n}\n\n\n# File names may contain any character except for \"\\0\" and \"/\" so care needs to be taken\n# to prevent file names containing spaces or new lines from being split\n\n\nfunction getNumMatchingFiles()\n{\n    declare -a searchPaths=(\"$@\")\n\n    if (( ${#searchPaths[@]} == 0 )); then\n        echo 0; return\n    fi\n\n    # RS (Record Separator) is set to \"/\" as \"\\0\" would simply read the entire\n    # input stream as one record. Setting FS (Field Separator) to \"\\0\" is no good\n    # as that would cause individual characters to be treated as fields\n\n    find \"${searchPaths[@]}\" -maxdepth 1 -type f -iname \"*.$fileType\" -printf \"%f/\" | gawk '\n    BEGIN { RS = \"/\" }  # RS - Record Separator\n    END { print NR }    # NR - No. of Records'\n\n    unset searchPaths\n}\n\n\nfunction getSortedFileNames()\n{\n    declare -a searchPaths=(\"$@\")\n\n    declare key1=\"\"\n    declare key2=\"\"\n    declare key3=\"\"\n    declare sortOrder=\"\"\n\n    # File names are sorted by leading digits, trailing digits, and by any characters in between.\n    # The order of the sort keys is dependant on the \"layer marker\"\n\n    if $isReverseSortOn; then\n        sortOrder=\"r\"\n    fi\n\n    case $layerMarker in\n        dirs )      key1=\"1,1n$sortOrder\";  key2=\"2,2d$sortOrder\";  key3=\"3,3n\";;\n        prefix )    key1=\"2,2d\";            key2=\"3,3n\";            key3=\"1,1n$sortOrder\";;\n        suffix )    key1=\"1,1n\";            key2=\"2,2d\";            key3=\"3,3n$sortOrder\";;\n        * )         key1=\"1,1n$sortOrder\";  key2=\"2,2d$sortOrder\";  key3=\"3,3n\";;\n    esac\n\n    # sort -z appends null terminator instead of a new line\n    # GNU tr won't delete \"\\0\" unless explicitly told to\n\n    find \"${searchPaths[@]}\" -maxdepth 1 -type f -iname \"*.$fileType\" -printf \"%f/\" | gawk '\n    BEGIN {\n        RS = \"/\"\n        IGNORECASE = 1\n    }\n    {\n        delimFileName = gensub(/(^[0-9]*)(.*[^0-9])([0-9]*[.]'$fileType'$)/, \"\\\\1/\\\\2/\\\\3\", 1)\n        printf(\"%s\\0\", delimFileName)\n    }\n    ' | sort -z -t/ -k $key1 -k $key2 -k $key3 | tr -d '/'\n\n    unset searchPaths\n}\n\n\nfunction renameDuplicateFileNames()\n{\n    declare -iA fileNameCounters\n    declare -i count=0\n    declare newName=\"\"\n\n    for fileName in \"${origFileNames[@]}\"; do\n        fileNameCounters[\"$fileName\"]+=1\n\n        # If file name is a duplicate\n        if (( ${fileNameCounters[\"$fileName\"]} > 1 )); then\n            count=1\n            newName=\"${fileName%.*}_$count.$fileType\"\n\n            # While the new name is not unique\n            while [ \"${fileNameCounters[$newName]}\" != \"\" ]; do\n                (( count++ ))\n                newName=\"${fileName%.*}_$count.$fileType\"\n            done\n\n            fileNameCounters[\"$newName\"]+=1\n            newFileNames+=(\"$newName\")\n        else\n            newFileNames+=(\"$fileName\")\n        fi\n    done\n}\n\n\nfunction findFilesInSubDirs()\n{\n    if (( ${#dirSearchPaths[@]} == 0 )); then\n        return\n    fi\n\n    # IFS (Internal Field Separator) is set to null for duration of read operation to prevent\n    # file names containing spaces or new lines from being split\n\n    # Find sub-directories containing audio files \n    while IFS= read -r -d '' dirName; do\n        numFiles=$( getNumMatchingFiles \"$dirName\" )\n\n        if (( numFiles > 0 )); then\n            if (( numFiles > MAX_LAYERS )); then\n                echo -e \"Warning: Instruments can only have $MAX_LAYERS layers \\n$numFiles matching files found in $PWD/$dirName \\nOnly the first $MAX_LAYERS will be included\\n\"\n                numFiles=$MAX_LAYERS\n            fi\n            instrumentNames+=(\"$dirName\")\n            instrNumLayers+=($numFiles)\n            instrFilePaths+=(\"$PWD/$dirName\")\n            instrFilesIndex+=($filesIndex)\n            (( filesIndex += numFiles ))\n\n            # Find audio files in sub-directory\n            layerNum=1\n            while IFS= read -r -d '' fileName; do\n                if (( layerNum <= MAX_LAYERS )); then\n                    origFileNames+=(\"$fileName\")\n                    (( layerNum++ ))\n                fi\n            done < <(getSortedFileNames \"$dirName\")\n        fi\n    done < <(find \"${dirSearchPaths[@]}\" -maxdepth 1 -type d ! -wholename '[.]' -printf \"%f\\0\" | sort -z)\n}\n\n\nfunction findFilesWithDigits()\n{\n    declare -i instrID=-1\n    declare -iA instrLayerCounters\n    declare instrName=\"\"\n    declare regEx=\"\"\n\n    if [ \"$layerMarker\" == \"prefix\" ]; then\n        # Strip leading digits from file name\n        regEx='/^[0-9]*([^0-9].*)$/'\n    elif [ \"$layerMarker\" == \"suffix\" ]; then\n        # Strip trailing digits from file name\n        regEx='/^(.*[^0-9])[0-9]*$/'\n    else\n        exitWithError \"Error: \\$layerMarker == $layerMarker \\nThe programmer has messed up!\"\n    fi\n\n    numFiles=$( getNumMatchingFiles \"${fileSearchPaths[@]}\" )\n\n    if (( numFiles > 0 )); then\n        while IFS= read -r -d '' fileName; do\n            # Strip file extension and layer marker digits from file name\n            instrName=$( printf '%s/' \"${fileName%.*}\" | gawk 'BEGIN { RS = \"/\" }\n            {\n                instrName = gensub( '\"$regEx\"', \"\\\\1\", 1 )\n                printf(\"%s\", instrName)\n            }')\n\n            # If file belongs to a new instrument rather than a layer of an existing instrument...\n            if [ \"${instrLayerCounters[$instrName]}\" == \"\" ]; then\n                (( instrID++ ))\n                instrumentNames+=(\"$instrName\")\n                instrFilePaths+=(\"$PWD\")\n                instrFilesIndex+=($filesIndex)\n            fi\n\n            instrLayerCounters[\"$instrName\"]+=1\n            layerNum=${instrLayerCounters[\"$instrName\"]}\n            if (( layerNum <= MAX_LAYERS )); then\n                instrNumLayers[$instrID]=$layerNum\n                origFileNames+=(\"$fileName\")\n                (( filesIndex++ ))\n            else\n                echo -e \"Warning: Instruments can only have $MAX_LAYERS layers \\nInstrument \\\"\"$( formatInstrName \"$instrName\" )\"\\\" already has $MAX_LAYERS layers \\n$fileName will not be included\\n\"\n            fi\n\n        done < <(getSortedFileNames \"${fileSearchPaths[@]}\")\n    fi\n}\n\n\nfunction findFilesSingleLayer()\n{\n    numFiles=$( getNumMatchingFiles \"${fileSearchPaths[@]}\" )\n\n    if (( numFiles > 0 )); then\n        while IFS= read -r -d '' fileName; do\n            origFileNames+=(\"$fileName\")\n            instrumentNames+=(\"${fileName%.*}\")\n            instrNumLayers+=(1)\n            instrFilePaths+=(\"$PWD\")\n            instrFilesIndex+=($filesIndex)\n            (( filesIndex++ ))\n        done < <(getSortedFileNames \"${fileSearchPaths[@]}\")\n    fi\n}\n\n\nfunction findFilesSFZ()\n{\n    numFiles=$( getNumMatchingFiles \"${fileSearchPaths[@]}\" )\n\n    if (( numFiles > 0 )); then\n        while IFS= read -r -d '' fileName; do\n            origFileNames+=(\"$fileName\")\n        done < <(getSortedFileNames \"${fileSearchPaths[@]}\")\n    fi\n}\n\n\nfunction formatInstrName()\n{\n    # Replace \"_\" and \"\\\" characters with spaces and trim leading and trailing white space\n    printf '%s' \"$1\" | tr '\\\\_' ' ' | xargs\n}\n\n\nfunction formatInstrNames()\n{\n    declare -ri numInstruments=${#instrumentNames[@]}\n    declare -i instrID=0\n\n    for (( instrID=0; instrID < numInstruments; instrID++ )); do\n        instrumentNames[$instrID]=$( formatInstrName \"${instrumentNames[$instrID]}\" )\n    done\n}\n\n\nfunction findFiles()\n{\n    declare -i numFiles=0\n    declare -i filesIndex=0\n    declare -i layerNum=0\n\n    # If no file or dir names have been passed on the command line then set file search path to PWD\n    if (( ${#fileSearchPaths[@]} == 0 )) && (( ${#dirSearchPaths[@]} == 0 )); then\n        fileSearchPaths=(\".\")\n        dirSearchPaths=(\".\")\n    fi\n\n    if (( ${#fileSearchPaths[@]} == 0 )) && (( ${#dirSearchPaths[@]} == 1 )) && [[ ${dirSearchPaths[0]} == \".\" ]]; then\n        fileSearchPaths=(\".\")\n    fi\n\n    if [ \"$fileType\" == \"sfz\" ]; then\n        findFilesSFZ\n    else\n        case $layerMarker in\n            dirs )      findFilesInSubDirs; findFilesSingleLayer; renameDuplicateFileNames;;\n            prefix )    findFilesWithDigits;;\n            suffix )    findFilesWithDigits;;\n            * )         findFilesSingleLayer;;\n        esac\n    fi\n\n    formatInstrNames\n}\n\n\nfunction calculateF()\n{\n    declare expression=$1\n\n    # scale=4         : round to 4 fractional digits\n    # if(x<1) print 0 : if answer is less than 1 print leading 0\n    # 2>/dev/null     : don't print error messages\n    echo $( echo \"scale=4; x=$expression; if(x<1) print 0; x\" | bc -q 2>/dev/null )\n}\n\n\nfunction printIndent() \n{\n    declare -i i=0\n\n    for (( i=0; i < currentIndentSize; i++ )); do\n        echo -n \" \"\n    done\n}\n\n\nfunction addLayerXML() \n{\n    xmlLines+=(\"$indent<layer>\")\n\n    (( currentIndentSize += tabSize ))\n    indent=$(printIndent)\n\n        if [ \"$layerMarker\" == \"dirs\" ]; then\n            xmlLines+=(\"$indent<filename>${newFileNames[$fileID]}</filename>\")\n        else\n            xmlLines+=(\"$indent<filename>${origFileNames[$fileID]}</filename>\")\n        fi\n        xmlLines+=(\"$indent<min>$velLo</min>\")\n        xmlLines+=(\"$indent<max>$velHi</max>\")\n        xmlLines+=(\"$indent<gain>1</gain>\")\n        xmlLines+=(\"$indent<pitch>0</pitch>\")\n\n    (( currentIndentSize -= tabSize ))\n    indent=$(printIndent)\n\n    xmlLines+=(\"$indent</layer>\")\n}\n\n\nfunction addInstrumentXML() \n{\n    velIncrement=$( calculateF \"1 / $numLayers\" )\n    velHi=1\n    velLo=$( calculateF \"1 - $velIncrement\" )\n\n    xmlLines+=(\"$indent<instrument>\")\n\n    (( currentIndentSize += tabSize ))\n    indent=$(printIndent)\n\n        xmlLines+=(\"$indent<id>$instrID</id>\")\n        xmlLines+=(\"$indent<name>$instrName</name>\")\n        xmlLines+=(\"$indent<volume>1</volume>\")\n        xmlLines+=(\"$indent<isMuted>false</isMuted>\")\n        xmlLines+=(\"$indent<pan_L>1</pan_L>\")\n        xmlLines+=(\"$indent<pan_R>1</pan_R>\")\n        xmlLines+=(\"$indent<randomPitchFactor>0</randomPitchFactor>\")\n        xmlLines+=(\"$indent<gain>1</gain>\")\n        xmlLines+=(\"$indent<filterActive>false</filterActive>\")\n        xmlLines+=(\"$indent<Attack>0</Attack>\")\n        xmlLines+=(\"$indent<Decay>0</Decay>\")\n        xmlLines+=(\"$indent<Sustain>1</Sustain>\")\n        xmlLines+=(\"$indent<Release>1000</Release>\")\n        xmlLines+=(\"$indent<muteGroup>-1</muteGroup>\")\n        if [ \"$fileType\" == \"sfz\" ]; then\n            xmlLines+=(\"$indent<midiOutChannel>0</midiOutChannel>\")\n            xmlLines+=(\"$indent<midiOutNote>${instrMidiNotes[$instrID]}</midiOutNote>\")\n        fi\n        xmlLines+=(\"$indent<isStopNote>false</isStopNote>\")\n\n        while (( fileID < filesIndex + numLayers )); do\n            if (( fileID == filesIndex + numLayers - 1 )); then\n                velLo=0\n            fi\n\n            addLayerXML\n\n            velHi=$( calculateF \"$velHi - $velIncrement\" )\n            velLo=$( calculateF \"$velLo - $velIncrement\" )\n            (( fileID++ ))\n        done\n\n    (( currentIndentSize -= tabSize ))\n    indent=$(printIndent)\n\n    xmlLines+=(\"$indent</instrument>\")\n}\n\n\nfunction addInstrumentListXML() \n{\n    declare -ri numInstruments=${#instrumentNames[@]}\n    declare -i instrID=0\n    declare instrName=\"\"\n    declare -i numLayers=0\n    declare -i fileID=0\n    declare -i filesIndex=0\n    declare velHi=1\n    declare velLo=0\n    declare velIncrement=0.0\n\n    xmlLines+=(\"$indent<instrumentList>\")\n\n    (( currentIndentSize += tabSize ))\n    indent=$(printIndent)\n\n        for (( instrID=0; instrID < numInstruments; instrID++ )); do\n            instrName=\"${instrumentNames[$instrID]}\"\n            numLayers=${instrNumLayers[$instrID]}\n            filesIndex=${instrFilesIndex[$instrID]}\n            fileID=$filesIndex\n\n            addInstrumentXML\n        done\n\n    (( currentIndentSize -= tabSize ))\n    indent=$(printIndent)\n\n    xmlLines+=(\"$indent</instrumentList>\")\n}\n\n\nfunction createDrumkitXmlFile()\n{\n    declare -ri tabSize=4\n    declare -i currentIndentSize=0\n    declare indent=\"\"\n    declare -a xmlLines\n\n    xmlLines+=(\"$indent<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\")\n    xmlLines+=(\"$indent<drumkit_info xmlns:xsi=\\\"http://www.w3.org/2001/XMLSchema-instance\\\" xmlns=\\\"http://www.hydrogen-music.org/drumkit\\\">\")\n\n    (( currentIndentSize += tabSize ))\n    indent=$(printIndent)\n\n        xmlLines+=(\"$indent<name>$kitName</name>\")\n        xmlLines+=(\"$indent<author>$kitAuthor</author>\")\n        xmlLines+=(\"$indent<info>$infoHtmlTagOpen$kitInfo$infoHtmlTagClose</info>\")\n        xmlLines+=(\"$indent<license>$kitLicense</license>\")\n        addInstrumentListXML\n\n    (( currentIndentSize -= tabSize ))\n    indent=$(printIndent)\n\n    xmlLines+=(\"$indent</drumkit_info>\")\n\n    touch \"$kitName\"/drumkit.xml\n\n    for line in \"${xmlLines[@]}\"; do\n        printf '%s\\n' \"$line\" >> \"$kitName\"/drumkit.xml\n    done\n}\n\n\nfunction createH2DrumkitFile()\n{\n    declare -ri numInstruments=${#instrumentNames[@]}\n    declare -i instrID=0\n    declare -i numLayers=0\n    declare filePath=\"\"\n    declare -i fileID=0\n    declare -i filesIndex=0\n\n    for (( instrID=0; instrID < numInstruments; instrID++ )); do\n        numLayers=${instrNumLayers[$instrID]}\n        filePath=\"${instrFilePaths[$instrID]}\"\n        filesIndex=${instrFilesIndex[$instrID]}\n        fileID=$filesIndex\n\n        while (( fileID < filesIndex + numLayers )); do\n            if [ \"$layerMarker\" == \"dirs\" ]; then\n                ln -s \"$filePath/${origFileNames[$fileID]}\" \"$kitName/${newFileNames[$fileID]}\"\n            else\n                ln -s \"$filePath/${origFileNames[$fileID]}\" \"$kitName/${origFileNames[$fileID]}\"\n            fi\n            (( fileID++ ))\n        done\n    done\n\n    tar --create --dereference --gzip --file \"$kitName\".h2drumkit \"$kitName\"\n}\n\n\nfunction createDir()\n{\n    declare dirName=\"$1\"\n\n    # If a directory with this name already exists then bail out\n    if [ -d \"$dirName\" ]; then\n        exitWithError \"Can't create \\\"$dirName\\\", dir already exists\"\n    else\n        mkdir \"$dirName\"\n    fi\n}\n\n\n# This function returns a list of instrument/MIDI-note pairs. Records are separated by \"\\0\"\n# and fields are separated by \"/\". Field 1 is the instrument name, field 2 is the MIDI note\nfunction parseSfzFile()\n{\n    declare fileName=\"$1\"\n\n    gawk '\n    BEGIN {\n        RS = \"<group>\"\n        FS = \"<region>\"\n    }\n\n    function getKeyValue( input,    octave, result )\n    {\n        switch (substr( input, 1, 1 ))\n        {\n            case /[cC]/:\n                result = 0\n                break\n            case /[dD]/:\n                result = 2\n                break\n            case /[eE]/:\n                result = 4\n                break\n            case /[fF]/:\n                result = 5\n                break\n            case /[gG]/:\n                result = 7\n                break\n            case /[aA]/:\n                result = 9\n                break\n            case /[bB]/:\n                result = 11\n                break\n        }\n        \n        if ( input ~ /[[:alpha:]]#/ )\n        {\n            result++\n        }\n        if ( input ~ /[[:alpha:]]b/ )\n        {\n            result--\n        }\n        if ( input ~ /-1/ )\n        {\n            result -= 24\n        }\n\n        octave = substr( input, length(input), 1 )\n        result += 12 * (octave + 1)\n\n        return result\n    }\n\n    {\n        # If group has a \"key=\" attribute or if group has \"lokey=\" and \"hikey=\" attributes\n        # which both have the same values then treat the group as a single instrument\n\n        if ( match( $1, /.*\\<key=[0-9]+.*/ ) )\n        {\n            key = gensub( /.*\\<key=([0-9]+).*/, \"\\\\1\", 1, $1 )\n            key += 24\n            instrName = gensub( /.*sample=(.+)[.]([wW][aA][vV]|[fF][lL][aA][cC]|[oO][gG][gG]).*/, \"\\\\1\", 1 )\n            keys[key] = instrName\n        }\n        else if ( match( $1, /.*\\<key=[a-gA-G](#|b)?(-1|[0-9]).*/ ) )\n        {\n            key = gensub( /.*\\<key=([a-gA-G](#|b)?(-1|[0-9])).*/, \"\\\\1\", 1, $1 )\n            key = getKeyValue( key )\n            key += 24\n            instrName = gensub( /.*sample=(.+)[.]([wW][aA][vV]|[fF][lL][aA][cC]|[oO][gG][gG]).*/, \"\\\\1\", 1 )\n            keys[key] = instrName\n        }\n        else if ( match( $1, /.*(lo|hi)key=[0-9]+.*(lo|hi)key=[0-9]+.*/ ) )\n        {\n            lokey = gensub( /.*lokey=([0-9]+).*/, \"\\\\1\", 1, $1 )\n            hikey = gensub( /.*hikey=([0-9]+).*/, \"\\\\1\", 1, $1 )\n\n            if ( lokey == hikey )\n            {\n                key = lokey + 24\n                instrName = gensub( /.*sample=(.+)[.]([wW][aA][vV]|[fF][lL][aA][cC]|[oO][gG][gG]).*/, \"\\\\1\", 1 )\n                keys[key] = instrName\n            }\n        }\n        else if ( match( $1, /.*(lo|hi)key=[a-gA-G](#|b)?(-1|[0-9]).*(lo|hi)key=[a-gA-G](#|b)?(-1|[0-9]).*/ ) )\n        {\n            lokey = gensub( /.*lokey=([a-gA-G](#|b)?(-1|[0-9])).*/, \"\\\\1\", 1, $1 )\n            hikey = gensub( /.*hikey=([a-gA-G](#|b)?(-1|[0-9])).*/, \"\\\\1\", 1, $1 )\n\n            lokey = getKeyValue( lokey )\n            hikey = getKeyValue( hikey )\n\n            if ( lokey == hikey )\n            {\n                key = lokey + 24\n                instrName = gensub( /.*sample=(.+)[.]([wW][aA][vV]|[fF][lL][aA][cC]|[oO][gG][gG]).*/, \"\\\\1\", 1 )\n                keys[key] = instrName\n            }\n        }\n        else\n        {\n            # If a region has a key attribute or matching lokey/hikey attributes then treat it as an individual instrument\n\n            for ( field = 2; field <= NF; field++ )\n            {\n                if ( match( $field, /.*\\<key=[0-9]+.*/ ) )\n                {\n                    key = gensub( /.*\\<key=([0-9]+).*/, \"\\\\1\", 1, $field )\n                    key += 24\n                    instrName = gensub( /.*sample=(.+)[.]([wW][aA][vV]|[fF][lL][aA][cC]|[oO][gG][gG]).*/, \"\\\\1\", 1, $field )\n                    keys[key] = instrName\n                }\n                else if ( match( $field, /.*\\<key=[a-gA-G](#|b)?(-1|[0-9]).*/ ) )\n                {\n                    key = gensub( /.*\\<key=([a-gA-G](#|b)?(-1|[0-9])).*/, \"\\\\1\", 1, $field )\n                    key = getKeyValue( key )\n                    key += 24\n                    instrName = gensub( /.*sample=(.+)[.]([wW][aA][vV]|[fF][lL][aA][cC]|[oO][gG][gG]).*/, \"\\\\1\", 1, $field )\n                    keys[key] = instrName\n                }\n                else if ( match( $field, /.*(lo|hi)key=[0-9]+.*(lo|hi)key=[0-9]+.*/ ) )\n                {\n                    lokey = gensub( /.*lokey=([0-9]+).*/, \"\\\\1\", 1, $field )\n                    hikey = gensub( /.*hikey=([0-9]+).*/, \"\\\\1\", 1, $field )\n\n                    if ( lokey == hikey )\n                    {\n                        key = lokey + 24\n                        instrName = gensub( /.*sample=(.+)[.]([wW][aA][vV]|[fF][lL][aA][cC]|[oO][gG][gG]).*/, \"\\\\1\", 1, $field )\n                        keys[key] = instrName\n                    }\n                }\n                else if ( match( $field, /.*(lo|hi)key=[a-gA-G](#|b)?(-1|[0-9]).*(lo|hi)key=[a-gA-G](#|b)?(-1|[0-9]).*/ ) )\n                {\n                    lokey = gensub( /.*lokey=([a-gA-G](#|b)?(-1|[0-9])).*/, \"\\\\1\", 1, $field )\n                    hikey = gensub( /.*hikey=([a-gA-G](#|b)?(-1|[0-9])).*/, \"\\\\1\", 1, $field )\n\n                    lokey = getKeyValue( lokey )\n                    hikey = getKeyValue( hikey )\n\n                    if ( lokey == hikey )\n                    {\n                        key = lokey + 24\n                        instrName = gensub( /.*sample=(.+)[.]([wW][aA][vV]|[fF][lL][aA][cC]|[oO][gG][gG]).*/, \"\\\\1\", 1, $field )\n                        keys[key] = instrName\n                    }\n                }\n            }\n        }\n    }\n\n    END {\n        for (k in keys)\n        {\n            printf( \"%s/%d\\0\", keys[k], k )\n        }\n    }\n    ' \"$fileName\" | sort -z -t/ -k 2,2n\n}\n\n\n# Main body of script\n\nprocessArgs\n\nif $isInteractiveModeOn; then\n    getValuesFromUser\nfi\n\nmakeInputXmlCompatible\nmakePathsRelative\n\nif $isInteractiveModeOn; then\n    echo\nfi\n\nfindFiles\n\nif (( ${#origFileNames[@]} == 0 )); then\n    exitWithError \"No $fileType files found!\"\nfi\n\nif [[ $fileType == \"sfz\" ]]; then\n    for fileName in \"${origFileNames[@]}\"; do\n        kitName=\"${fileName%.*}\"\n        unset instrumentNames\n        unset instrMidiNotes\n        unset instrNumLayers\n\n        while IFS= read -r -d '' instrData; do\n            instrumentNames+=(\"${instrData%/*}\")    # Discard text after the \"/\" field separator\n            instrMidiNotes+=(${instrData#*/})       # Discard text before the \"/\" field separator\n            instrNumLayers+=(0)\n        done < <(parseSfzFile \"$fileName\")\n\n        formatInstrNames\n        \n        createDir \"$kitName\"\n        createDrumkitXmlFile\n        createH2DrumkitFile\n        rm -rf \"$kitName\"\n    done\nelse\n    createDir \"$kitName\"\n    createDrumkitXmlFile\n    createH2DrumkitFile\n    rm -rf \"$kitName\"\nfi\n\nif $isInteractiveModeOn; then\n    setTextColour $YELLOW; echo -e \"All done!\\n\"\nfi\n\nresetText\n"
    },
    {
      "filename": "makesfz",
      "path": "/staging/",
      "content": "#!/bin/bash\n#\n# makesfz by Andrew M Taylor 2013. Inspired by makesfz.sh by Dan MacDonald\n#\n# A script to auto-generate SFZ files. Requires BASH v4\n#\n# This is free and unencumbered software released into the public domain.\n# Please read UNLICENSE for more details, or refer to <http://unlicense.org/>\n\n\ndeclare -ra ARGS=(\"$@\")\ndeclare -ri NUM_ARGS=$#\n\ndeclare -ri KEY_MAX=127\ndeclare -ri KEY_MIN=0\ndeclare -ri VEL_MAX=127\n\ndeclare -ri CYAN=3\ndeclare -ri RED=4\ndeclare -ri YELLOW=6\ndeclare -ri WHITE=7\n\ndeclare -ri ERROR=-1\n\ndeclare -a instrumentNames\ndeclare -ia instrNumLayers\ndeclare -ia instrFilesIndex\ndeclare -a sampleFilePaths\ndeclare -a fileSearchPaths\ndeclare -a dirSearchPaths\n\ndeclare isKeySet=false\ndeclare isSingleFile=false\ndeclare isReverseSortOn=false\ndeclare isInteractiveModeOn=false\ndeclare isServiceMenuModeOn=false\n\ndeclare loopMode=\"no_loop\"\ndeclare fileType=\"wav\"\ndeclare sfzFileName=\"${PWD##*/}\" # SFZ file name defaults to present working directory\ndeclare layerMarker=\"\"\ndeclare randMarker=\"\"\n\ndeclare -i lochan=1\ndeclare -i hichan=16\ndeclare -i lokey=0\ndeclare -i hikey=127\ndeclare -i keycenter=60\ndeclare -i key=60\n\n\nfunction setTextColour() { tput setf $1; }\n\nfunction startUnderline() { tput smul; }\n\nfunction stopUnderline() { tput rmul; }\n\nfunction moveCursorUpOneLine() { tput cuu1; }\n\nfunction resetText() { echo -en \"\\e[0m\"; }\n\n\nfunction exitWithError() \n{\n    declare errorMessage=\"$1\"\n    \n    setTextColour $RED\n    \n    # 1>&2 means redirect text headed for stdout to stderr\n    echo -e \"\\n$errorMessage\\n\" 1>&2\n    \n    resetText\n    \n    exit 1\n}\n\n\nfunction showHelp() \n{\n    cat << EOF\n\nUsage:  makesfz [options]\n        makesfz [options] --files 'file_name' ...\n\nBy default, running makesfz with no arguments creates individual\nsfz files for every wav file found in the current directory.\nSamples are mapped across the entire keyboard with pitch_keycenter\nset to MIDI key C4 (middle C).\n\nAudio files can be treated as indiviual instruments or as instrument \nlayers grouped either by sub-directory or by file name with numeric \nprefixes/suffixes indicating the layers.\n\nTo scan for files inside a directory, pass the dir name to makesfz\nlike so: makesfz --files 'dir_name'. To scan for files grouped by \nsub-dir inside a directory use: makesfz -L dirs --files 'dir_name'.\n    \nOptions:\n  -s, --single          Creates a single sfz file named after the\n                        current directory. Each wav file found is\n                        mapped to its own MIDI key (starting at C4\n                        by default)\n  -n, --name 'NAME'     Output file name. Ignored unless used with \n                        -s option\n      --lk KEY,         Set lokey. KEY must be 0 - 127 or C-1 - G9\n      --lokey KEY       (# or b allowed). Ignored when used with\n                        -s option\n      --hk KEY,         Set hikey. KEY must be 0 - 127 or C-1 - G9\n      --hikey KEY       (# or b allowed). Ignored when used with\n                        -s option\n      --kc KEY,         Set pitch_keycenter. KEY must be 0 - 127\n      --keycenter KEY   or C-1 - G9 (# or b allowed). Ignored when\n                        used with -s option\n  -k, --key KEY         Overrides --lk, --hk, and --kc, setting lokey,\n                        hikey, and pitch_keycenter to KEY.\n                        When used with -s option, each audio file is\n                        mapped to its own MIDI key starting at KEY\n  -1, --one-shot        Set loop_mode to one_shot\n  -m, --midi CHAN       Set MIDI channel. CHAN must be 1 - 16 or\n                        all (default)\n  -f, --format FORMAT   Format of audio files to scan for:\n                        wav (default), flac, or ogg\n  -L, --layers MARKER   Files are grouped into instruments\n                        according to a \"layer marker\".\n                        MARKER may be dirs, prefix, or suffix\n                        (or d, p, or s)\n  -r, --reverse         File names are sorted in reverse order\n  -I, --interactive     Run in \"interactive\" mode, where user\n                        is prompted for input\n  -h, --help            Display this help and exit\n\nEOF\n}\n\n\nfunction checkKeyIsValid() \n{\n    declare input=$1\n    \n    declare -i key=0\n    declare -i numChars=0\n    declare -i i=0\n    declare -i octave=0\n\n    # check if user input is a valid key name\n    if [[ $input =~ ^[a-gA-G](#|b)?(-1|[0-9])$ ]]; then\n        case \"${input:0:1}\" in\n            [cC] )  key=0;;\n            [dD] )  key=2;;\n            [eE] )  key=4;;\n            [fF] )  key=5;;\n            [gG] )  key=7;;\n            [aA] )  key=9;;\n            [bB] )  key=11;;\n        esac\n\n        numChars=`expr length $input`\n        i=0\n        for (( i = 1; i < numChars; i++ )); do\n            if [ \"${input:$i:1}\" == \"#\" ]; then\n                (( key++ ))\n            fi\n            if [ \"${input:$i:1}\" == \"b\" ]; then\n                (( key-- ))\n            fi\n            if [ \"${input:$i:1}\" == \"-\" ]; then\n                (( key -= 24 ))\n            fi\n        done\n\n        octave=${input:(-1)}\n        (( key += 12 * (octave + 1) ))\n\n        if [ $key -lt $KEY_MIN ] || [ $key -gt $KEY_MAX ]; then\n            key=$ERROR\n        fi\n    # check if user input is a valid key number\n    elif [[ $input =~ ^([0-9]|[1-9][0-9]|1[0-2][0-9])$ ]]; then\n        key=$input\n        if [ $key -gt $KEY_MAX ]; then\n            key=$ERROR\n        fi\n    else\n        key=$ERROR\n    fi\n    echo $key\n}\n\n\nfunction setLoKey() \n{\n    lokey=$( checkKeyIsValid \"$1\" )\n    if (( lokey == ERROR )); then\n        exitWithError \"Key must be in the range C-1 to G9\"\n    fi\n}\n\n\nfunction setHiKey() \n{\n    hikey=$( checkKeyIsValid \"$1\" )\n    if (( hikey == ERROR )); then\n        exitWithError \"Key must be in the range C-1 to G9\"\n    fi\n}\n\n\nfunction setKeyCenter() \n{\n    keycenter=$( checkKeyIsValid \"$1\" )\n    if (( keycenter == ERROR )); then\n        exitWithError \"Key must be in the range C-1 to G9\"\n    fi\n}\n\n\nfunction setKey() \n{\n    key=$( checkKeyIsValid \"$1\" )\n    if (( key == ERROR )); then\n        exitWithError \"Key must be in the range C-1 to G9\"\n    fi\n    isKeySet=true\n}\n\n\nfunction setMidiChan() \n{\n    declare input=$1\n\n    # check user input is valid (1 to 16)\n    if [[ $input =~ ^([1-9]|1[0-6])$ ]]; then\n        lochan=$input\n        hichan=$input\n    elif [[ $input =~ ^(all|ALL)$ ]]; then\n        lochan=1\n        hichan=16\n    else\n        exitWithError \"MIDI channel must be '1 - 16' or 'all'\"\n    fi\n}\n\n\nfunction setFileType() \n{\n    declare input=$1\n\n    # check if user input is a valid file type\n    case \"$input\" in\n        [wW][aA][vV] )      fileType=\"wav\";;\n        [fF][lL][aA][cC] )  fileType=\"flac\";;\n        [oO][gG][gG] )      fileType=\"ogg\";;\n        * )                 exitWithError \"File type must be wav, flac, or ogg\";;\n    esac\n}\n\n\nfunction setLayerMarker()\n{\n    declare input=$1\n\n    # check if user input is valid\n    case \"$input\" in\n        [dD][iI][rR][sS] )          layerMarker=\"dirs\";;\n        [dD] )                      layerMarker=\"dirs\";;\n        [pP][rR][eE][fF][iI][xX] )  layerMarker=\"prefix\";;\n        [pP] )                      layerMarker=\"prefix\";;\n        [sS][uU][fF][fF][iI][xX] )  layerMarker=\"suffix\";;\n        [sS] )                      layerMarker=\"suffix\";;\n        * )                         exitWithError \"Layer marker must be dirs, prefix, or suffix (or d, p, s)\";;\n    esac\n}\n\n\nfunction setSearchPaths()\n{\n    declare -i param=$1\n\n    while (( param < NUM_ARGS )); do\n\n        if [ -f \"${ARGS[$param]}\" ]; then\n            fileSearchPaths+=(\"${ARGS[$param]}\")\n        elif [ -d \"${ARGS[$param]}\" ] && [ \"${ARGS[$param]}\" != \".\" ] && [ \"${ARGS[$param]}\" != \"./\" ]; then\n            dirSearchPaths+=(\"${ARGS[$param]}\")\n        fi\n\n        (( param++ ))\n    done\n}\n\n\nfunction processArgs()\n{\n    declare -i i=0\n\n    if (( NUM_ARGS > 0 )); then\n        while (( i < NUM_ARGS )); do\n            case ${ARGS[$i]} in\n                -s )                isSingleFile=true; isKeySet=true;;\n                --single )          isSingleFile=true; isKeySet=true;;\n                -n )                (( i++ )); sfzFileName=${ARGS[$i]};;\n                --name )            (( i++ )); sfzFileName=${ARGS[$i]};;\n                --lk )              (( i++ )); setLoKey ${ARGS[$i]};;\n                --lokey )           (( i++ )); setLoKey ${ARGS[$i]};;\n                --hk )              (( i++ )); setHiKey ${ARGS[$i]};;\n                --hikey )           (( i++ )); setHiKey ${ARGS[$i]};;\n                --kc )              (( i++ )); setKeyCenter ${ARGS[$i]};;\n                --keycenter )       (( i++ )); setKeyCenter ${ARGS[$i]};;\n                -k )                (( i++ )); setKey ${ARGS[$i]};;\n                --key )             (( i++ )); setKey ${ARGS[$i]};;\n                -1 )                loopMode=\"one_shot\";;\n                --oneshot )         loopMode=\"one_shot\";;\n                -m )                (( i++ )); setMidiChan ${ARGS[$i]};;\n                --midi )            (( i++ )); setMidiChan ${ARGS[$i]};;\n                -f )                (( i++ )); setFileType ${ARGS[$i]};;\n                --format )          (( i++ )); setFileType ${ARGS[$i]};;\n                -r )                isReverseSortOn=true;;\n                --reverse )         isReverseSortOn=true;;\n                -L )                (( i++ )); setLayerMarker ${ARGS[$i]};;\n                --layers )          (( i++ )); setLayerMarker ${ARGS[$i]};;\n                -I )                isInteractiveModeOn=true;;\n                --interactive )     isInteractiveModeOn=true;;\n                --service-menu )    isServiceMenuModeOn=true;;\n                -h )                showHelp; exit 0;;\n                --help )            showHelp; exit 0;;\n                --files )           (( i++ )); setSearchPaths $i; break;;\n                * )                 exitWithError \"Invalid option: \\\"${ARGS[$i]}\\\". See 'makesfz -h' for usage\";;\n            esac\n            (( i++ ))\n        done\n    fi\n}\n\n\nfunction guessFileTypeFromFileList()\n{\n    declare filePath=\"\"\n    declare prevFileType=\"\"\n    \n    fileType=\"\"\n    \n    for filePath in \"${fileSearchPaths[@]}\"; do\n        fileType=\"${filePath##*.}\"\n\n        if [[ $fileType != [wW][aA][vV] ]] && [[ $fileType != [fF][lL][aA][cC] ]] && [[ $fileType != [oO][gG][gG] ]]; then\n            fileType=\"\"\n        fi\n\n        if [ \"$prevFileType\" != \"\" ] && [ \"$fileType\" != \"$prevFileType\" ]; then\n            fileType=\"\"\n            break\n        fi\n        prevFileType=\"$fileType\"\n    done\n}\n\n\nfunction showWelcomeMessage()\n{\n    setTextColour $YELLOW; startUnderline; echo -e \"\\nMake SFZ\"; stopUnderline\n    \n    setTextColour $WHITE; cat << EOF\n\nThis script will auto-generate one or more SFZ files from the selected \nfile(s) and/or dir(s). If no files are selected it will scan the current \ndirectory for compatible audio files (wav, flac, or ogg).\n\nMake SFZ can create individual SFZ files for each audio file or, \nalternatively, it can create a single SFZ file in which every audio \nfile is mapped to its own key.\n\nAudio files can be treated as indiviual instruments or as instrument \nlayers grouped either by sub-directory or by file name with numeric \nprefixes/suffixes indicating the layers.\n\nEOF\n}\n\n\n# This function only gets called if makesfz was launched from a KDE service menu\nfunction setWorkingDir()\n{\n    # Try to work out present working directory from list of files passed on the command line\n    if (( ${#fileSearchPaths[@]} > 0 )); then\n        cd \"${fileSearchPaths[0]%/*}\"\n    elif (( ${#dirSearchPaths[@]} > 1 )); then\n        cd \"${dirSearchPaths[0]%/*}\"\n    elif (( ${#dirSearchPaths[@]} == 1 )); then\n        # Ask user if this is the PWD or a sub-dir\n        setTextColour $YELLOW; echo \"Selected directory: ${dirSearchPaths[0]}\"\n        setTextColour $WHITE; echo \"Should makeh2kit set this as the current directory?\"\n        echo -e \"yes - create files in ${dirSearchPaths[0]} (default)\\nno  - create files in ${dirSearchPaths[0]%/*}\"\n        setTextColour $CYAN; read -e input\n        \n        if [[ $input == \"\" ]]; then \n            input=\"yes\"; moveCursorUpOneLine; echo \"$input\"\n        fi\n\n        if [[ $input == [yY] ]] || [[ $input == [yY][eE][sS] ]]; then\n            cd \"${dirSearchPaths[0]}\"\n            unset dirSearchPaths\n        elif [[ $input == [nN] ]] || [[ $input == [nN][oO] ]]; then\n            cd \"${dirSearchPaths[0]%/*}\"\n        else\n            exitWithError \"Invalid value: \\\"$input\\\"\"\n        fi\n    else\n        exitWithError \"Error: no files or dirs passed on command line. \\nThe programmer has messed up!\"\n    fi\n}\n\n\nfunction getValuesFromUser()\n{\n    declare input=\"\"\n\n    guessFileTypeFromFileList\n    \n    showWelcomeMessage\n    \n    setTextColour $YELLOW; echo -e \"Please enter the following details or simply press enter to accept the\\ndefault values...\\n\"\n\n    if $isServiceMenuModeOn; then\n        setWorkingDir\n    fi\n    \n    # Create a single SFZ file?\n    setTextColour $WHITE; echo \"Do you want to create a single SFZ file? (default is no):\"\n    setTextColour $CYAN; read -e input\n    \n    if [[ $input == \"\" ]]; then \n        input=\"no\"; moveCursorUpOneLine; echo \"$input\"\n    fi\n\n    if [[ $input == [yY] ]] || [[ $input == [yY][eE][sS] ]]; then\n\n        isSingleFile=true\n        \n        # Set SFZ file name\n        setTextColour $WHITE; echo \"Name of SFZ file (default is name of current directory, \\\"${PWD##*/}\\\"):\"\n        setTextColour $CYAN; read -e sfzFileName\n        \n        if [[ $sfzFileName == \"\" ]]; then \n            sfzFileName=\"${PWD##*/}\"; moveCursorUpOneLine; echo \"$sfzFileName\"\n        fi\n\n        # Set key\n        setTextColour $WHITE; echo \"Start key (0 to 127, or C-1 to G9, # or b allowed) (default is C4):\"\n        setTextColour $CYAN; read -e input\n        \n        if [[ $input == \"\" ]]; then \n            input=\"C4\"; moveCursorUpOneLine; echo \"$input\"\n        fi\n        \n        setKey \"$input\"\n        \n    elif [[ $input == [nN] ]] || [[ $input == [nN][oO] ]]; then\n\n        # Set lokey\n        setTextColour $WHITE; echo \"Lowest key (0 to 127, or C-1 to G9, # or b allowed) (default is C-1):\"\n        setTextColour $CYAN; read -e input\n        \n        if [[ $input == \"\" ]]; then \n            input=\"C-1\"; moveCursorUpOneLine; echo \"$input\"\n        fi\n        \n        setLoKey \"$input\"\n\n        # Set hikey\n        setTextColour $WHITE; echo \"Highest key (0 to 127, or C-1 to G9, # or b allowed) (default is G9):\"\n        setTextColour $CYAN; read -e input\n        \n        if [[ $input == \"\" ]]; then\n            input=\"G9\"; moveCursorUpOneLine; echo \"$input\"\n        fi\n        \n        setHiKey \"$input\"\n\n        # Set pitch key center\n        setTextColour $WHITE; echo \"Pitch key center (0 to 127, or C-1 to G9, # or b allowed) (default is C4):\"\n        setTextColour $CYAN; read -e input\n        \n        if [[ $input == \"\" ]]; then \n            input=\"C4\"; moveCursorUpOneLine; echo \"$input\"\n        fi\n        \n        setKeyCenter \"$input\"\n\n    else \n        exitWithError \"Invalid value: \\\"$input\\\"\"\n    fi\n\n    # Set audio file format\n    if [[ $fileType == \"\" ]]; then\n        setTextColour $WHITE; echo \"Audio file format (wav (default), flac, or ogg):\"\n        setTextColour $CYAN; read -e input\n        \n        if [[ $input == \"\" ]]; then \n            input=\"wav\"; moveCursorUpOneLine; echo \"$input\"\n        fi\n        \n        setFileType \"$input\"\n    fi\n\n    # Enable one-shot?\n    setTextColour $WHITE; echo \"Enable one-shot? (default is no):\"\n    setTextColour $CYAN; read -e input\n    \n    if [[ $input == \"\" ]]; then \n        input=\"no\"; moveCursorUpOneLine; echo \"$input\"\n    fi\n    \n    if [[ $input == [yY] ]] || [[ $input == [yY][eE][sS] ]]; then\n        loopMode=\"one_shot\"\n    elif [[ $input == [nN] ]] || [[ $input == [nN][oO] ]]; then\n        loopMode=\"no_loop\"\n    else \n        exitWithError \"Invalid value: \\\"$input\\\"\"\n    fi\n\n    # Set MIDI channel\n    setTextColour $WHITE; echo \"Set MIDI channel (1 - 16, or all) (default is \\\"all\\\"):\"\n    setTextColour $CYAN; read -e input\n    \n    if [[ $input == \"\" ]]; then \n        input=\"all\"; moveCursorUpOneLine; echo \"$input\"\n    fi\n    \n    setMidiChan \"$input\"\n\n    # Set \"layer marker\"\n    setTextColour $WHITE; echo \"Set \\\"layer marker\\\" (none (default), dirs, prefix, or suffix (or d, p, s)):\"\n    setTextColour $CYAN; read -e input\n    \n    if [[ $input == \"\" ]]; then \n        input=\"none\"; moveCursorUpOneLine; echo \"$input\"\n    fi\n    \n    if [[ $input != \"none\" ]]; then \n        setLayerMarker \"$input\"\n    fi\n\n    # Reverse sort order?\n    setTextColour $WHITE; echo \"Reverse sort order? (default is no):\"\n    setTextColour $CYAN; read -e input\n    \n    if [[ $input == \"\" ]]; then \n        input=\"no\"; moveCursorUpOneLine; echo \"$input\"\n    fi\n\n    if [[ $input == [yY] ]] || [[ $input == [yY][eE][sS] ]]; then\n        isReverseSortOn=true\n    elif [[ $input == [nN] ]] || [[ $input == [nN][oO] ]]; then\n        isReverseSortOn=false\n    else\n        exitWithError \"Invalid value: \\\"$isReverseSortOn\\\"\"\n    fi\n    \n    setTextColour $WHITE\n}\n\n\nfunction makePathsRelative()\n{\n    declare -i i=0\n    \n    # Remove PWD from start of files and dirs so paths are relative rather than absolute\n    for (( i = 0; i < ${#fileSearchPaths[@]}; i++ )); do\n        fileSearchPaths[i]=\"${fileSearchPaths[i]#$PWD/}\"\n    done\n    \n    for (( i = 0; i < ${#dirSearchPaths[@]}; i++ )); do\n        if [[ ${dirSearchPaths[i]} == \"$PWD\" ]] || [[ ${dirSearchPaths[i]} == \"$PWD/\" ]]; then\n            dirSearchPaths[i]=\".\"\n        else\n            dirSearchPaths[i]=\"${dirSearchPaths[i]#$PWD/}\"\n        fi\n    done\n}\n\n\n# File names may contain any character except for \"\\0\" and \"/\" so care needs to be taken\n# to prevent file names containing spaces or new lines from being split\n\n\nfunction getNumMatchingFiles()\n{\n    declare -a searchPaths=(\"$@\")\n\n    # RS (Record Separator) is set to \"/\" as \"\\0\" would simply read the entire\n    # input stream as one record. Setting FS (Field Separator) to \"\\0\" is no good\n    # as that would cause individual characters to be treated as fields\n\n    find \"${searchPaths[@]}\" -maxdepth 1 -type f -iname \"*.$fileType\" -printf \"%f/\" | gawk '\n    BEGIN { RS = \"/\" }  # RS - Record Separator\n    END { print NR }    # NR - No. of Records'\n\n    unset searchPaths\n}\n\n\nfunction getSortedFileNames()\n{\n    declare -a searchPaths=(\"$@\")\n\n    declare key1=\"\"\n    declare key2=\"\"\n    declare key3=\"\"\n    declare sortOrder=\"\"\n\n    # File names are sorted by leading digits, trailing digits, and by any characters in between.\n    # The order of the sort keys is dependant on the \"layer marker\"\n\n    if $isReverseSortOn; then\n        sortOrder=\"r\"\n    fi\n\n    case $layerMarker in\n        dirs )      key1=\"1,1n$sortOrder\";  key2=\"2,2d$sortOrder\";  key3=\"3,3n\";;\n        prefix )    key1=\"2,2d\";            key2=\"3,3n\";            key3=\"1,1n$sortOrder\";;\n        suffix )    key1=\"1,1n\";            key2=\"2,2d\";            key3=\"3,3n$sortOrder\";;\n        * )         key1=\"1,1n$sortOrder\";  key2=\"2,2d$sortOrder\";  key3=\"3,3n\";;\n    esac\n\n    # sort -z appends null terminator instead of a new line\n    # GNU tr won't delete \"\\0\" unless explicitly told to\n\n    find \"${searchPaths[@]}\" -maxdepth 1 -type f -iname \"*.$fileType\" -printf \"%f/\" | gawk '\n    BEGIN {\n        RS = \"/\"\n        IGNORECASE = 1\n    }\n    {\n        delimFileName = gensub( /(^[0-9]*)(.*[^0-9])([0-9]*[.]'$fileType'$)/, \"\\\\1/\\\\2/\\\\3\", 1 )\n        printf( \"%s\\0\", delimFileName )\n    }\n    ' | sort -z -t/ -k $key1 -k $key2 -k $key3 | tr -d '/'\n\n    unset searchPaths\n}\n\n\nfunction findFilesInSubDirs()\n{\n    # IFS (Internal Field Separator) is set to null for duration of read operation to prevent\n    # file names containing spaces or new lines from being split\n\n    # Find sub-directories containing audio files \n    while IFS= read -r -d '' subDir; do\n        numFiles=$( getNumMatchingFiles \"$subDir\" )\n\n        if (( numFiles > 0 )); then\n            if (( numFiles > VEL_MAX )); then\n                echo -e \"Warning: Instruments can only have $VEL_MAX layers \\n$numFiles matching files found in $subDir \\nOnly the first $VEL_MAX will be included\\n\"\n                numFiles=$VEL_MAX\n            fi\n            instrumentNames+=(\"${subDir##*/}\")\n            instrNumLayers+=($numFiles)\n            instrFilesIndex+=($filesIndex)\n            (( filesIndex += numFiles ))\n\n            # Find audio files in sub-directory\n            layerNum=1\n            while IFS= read -r -d '' fileName; do\n                if (( layerNum <= VEL_MAX )); then\n                    subDir=\"${subDir#./}\"\n                    filePath=\"$subDir/$fileName\"\n                    sampleFilePaths+=(\"${filePath//'/'/\\\\}\")\n                    (( layerNum++ ))\n                fi\n            done < <(getSortedFileNames \"$subDir\")\n        fi\n    done < <(find \"$directory\" -maxdepth 1 -type d ! -wholename \"$directory\" -print0 | sort -z)\n}\n\n\nfunction findFilesWithDigits()\n{\n    declare -a searchPaths=(\"$@\")\n\n    declare -iA instrLayerCounters\n    declare instrName=\"\"\n    declare regEx=\"\"\n\n    if [ \"$layerMarker\" == \"prefix\" ]; then\n        # Strip leading digits from file name\n        regEx='/^[0-9]*([^0-9].*)$/'\n    elif [ \"$layerMarker\" == \"suffix\" ]; then\n        # Strip trailing digits from file name\n        regEx='/^(.*[^0-9])[0-9]*$/'\n    else\n        exitWithError \"Error: \\$layerMarker == $layerMarker \\nThe programmer has messed up!\"\n    fi\n\n    numFiles=$( getNumMatchingFiles \"${searchPaths[@]}\" )\n\n    if (( numFiles > 0 )); then\n        while IFS= read -r -d '' fileName; do\n            # Strip file extension and layer marker digits from file name\n            instrName=$( printf '%s/' \"${fileName%.*}\" | gawk 'BEGIN { RS = \"/\" }\n            {\n                instrName = gensub( '\"$regEx\"', \"\\\\1\", 1 )\n                printf(\"%s\", instrName)\n            }')\n\n            # If file belongs to a new instrument rather than a layer of an existing instrument...\n            if [ \"${instrLayerCounters[$instrName]}\" == \"\" ]; then\n                (( instrID++ ))\n                instrumentNames+=(\"$instrName\")\n                instrFilesIndex+=($filesIndex)\n            fi\n\n            instrLayerCounters[\"$instrName\"]+=1\n            layerNum=${instrLayerCounters[\"$instrName\"]}\n\n            if (( layerNum <= VEL_MAX )); then\n                instrNumLayers[$instrID]=$layerNum\n\n                if [ \"$directory\" == \"\" ] || [ \"$directory\" == \".\" ] || [ \"$directory\" == \"./\" ]; then\n                    filePath=\"$fileName\"\n                else\n                    filePath=\"$directory/$fileName\"\n                fi\n                sampleFilePaths+=(\"${filePath//'/'/\\\\}\")\n                (( filesIndex++ ))\n            else\n                echo -e \"Warning: Instruments can only have $VEL_MAX layers \\nInstrument \\\"\"$( formatInstrName \"$instrName\" )\"\\\" already has $VEL_MAX layers \\n$fileName will not be included\\n\"\n            fi\n        done < <(getSortedFileNames \"${searchPaths[@]}\")\n    fi\n}\n\n\nfunction findFilesSingleLayer()\n{\n    declare -a searchPaths=(\"$@\")\n\n    numFiles=$( getNumMatchingFiles \"${searchPaths[@]}\" )\n\n    if (( numFiles > 0 )); then\n        while IFS= read -r -d '' fileName; do\n            instrumentNames+=(\"${fileName%.*}\")\n            instrNumLayers+=(1)\n\n            if [ \"$directory\" == \"\" ] || [ \"$directory\" == \".\" ] || [ \"$directory\" == \"./\" ]; then\n                filePath=\"$fileName\"\n            else\n                filePath=\"$directory/$fileName\"\n            fi\n            sampleFilePaths+=(\"${filePath//'/'/\\\\}\")\n            instrFilesIndex+=($filesIndex)\n\n            (( filesIndex++ ))\n        done < <(getSortedFileNames \"${searchPaths[@]}\")\n    fi\n}\n\n\nfunction formatInstrName()\n{\n    # Replace underscores with spaces and trim leading and trailing white space\n    printf '%s' \"$1\" | tr '_' ' ' | xargs\n}\n\n\nfunction formatInstrNames()\n{\n    declare -ri numInstruments=${#instrumentNames[@]}\n    declare -i instrID=0\n\n    for (( instrID=0; instrID < numInstruments; instrID++ )); do\n        instrumentNames[$instrID]=$( formatInstrName \"${instrumentNames[$instrID]}\" )\n    done\n}\n\n\nfunction findFiles()\n{\n    declare -i numFiles=0\n    declare -i filesIndex=0\n    declare -i layerNum=0\n    declare filePath=\"\"\n    declare -i instrID=-1\n\n    # If no file or dir names have been passed on the command line then set dir search path to PWD\n    if (( ${#dirSearchPaths[@]} == 0 )) && (( ${#fileSearchPaths[@]} == 0 )); then\n        dirSearchPaths=(\".\")\n    fi\n\n    for directory in \"${dirSearchPaths[@]}\"; do\n        case $layerMarker in\n            dirs )      findFilesInSubDirs \"$directory\"; findFilesSingleLayer \"$directory\";;\n            prefix )    findFilesWithDigits \"$directory\";;\n            suffix )    findFilesWithDigits \"$directory\";;\n            * )         findFilesSingleLayer \"$directory\";;\n        esac\n    done\n\n    unset directory\n\n    if (( ${#fileSearchPaths[@]} > 0 )); then\n        case $layerMarker in\n            dirs )      findFilesSingleLayer \"${fileSearchPaths[@]}\";;\n            prefix )    findFilesWithDigits \"${fileSearchPaths[@]}\";;\n            suffix )    findFilesWithDigits \"${fileSearchPaths[@]}\";;\n            * )         findFilesSingleLayer \"${fileSearchPaths[@]}\";;\n        esac\n    fi\n\n    formatInstrNames\n}\n\n\nfunction addGroup() \n{\n    (( velIncrement = VEL_MAX / numLayers + 1 ))\n    (( numBigIncrements = VEL_MAX % numLayers ))\n\n    if (( numLayers == VEL_MAX )); then\n        velIncrement=1\n    fi\n    lovel=1\n    hivel=$velIncrement\n    layerNum=1\n\n    textLines+=(\"// $instrName\")\n    if $isKeySet; then\n        textLines+=(\"<group> key=$key loop_mode=$loopMode lochan=$lochan hichan=$hichan\")\n    else\n        textLines+=(\"<group> lokey=$lokey hikey=$hikey pitch_keycenter=$keycenter loop_mode=$loopMode lochan=$lochan hichan=$hichan\")\n    fi\n\n    while (( fileID < filesIndex + numLayers )); do\n        if (( lovel > 127 )); then\n            lovel=127\n        fi\n        if (( hivel > 127 )); then\n            hivel=127\n        fi\n\n        textLines+=(\"<region> lovel=$lovel hivel=$hivel sample=${sampleFilePaths[$fileID]}\")\n\n        if (( layerNum == numBigIncrements )); then\n            (( velIncrement -= 1 ))\n        fi\n        (( lovel = hivel + 1 ))\n        (( hivel += velIncrement ))\n\n        (( layerNum++ ))\n        (( fileID++ ))\n    done\n\n    textLines+=(\"\")\n}\n\n\nfunction createSingleSfzFile()\n{\n    declare -a textLines\n    declare -ri numInstruments=${#instrumentNames[@]}\n    declare -i instrID=0\n    declare instrName=\"\"\n    declare -i numLayers=0\n    declare -i layerNum=0\n    declare -i fileID=0\n    declare -i filesIndex=0\n    declare -i lovel=0\n    declare -i hivel=0\n    declare -i velIncrement=0\n    declare -i numBigIncrements=0\n    declare input=\"\"\n\n    textLines+=(\"// This file was auto-generated by makesfz\")\n    textLines+=(\"\")\n\n    for (( instrID=0; instrID < numInstruments; instrID++ )); do\n        if (( key > KEY_MAX )); then\n            exitWithError \"Ran out of keys! Try assigning first sample to a lower key\"\n        fi\n        instrName=\"${instrumentNames[$instrID]}\"\n        numLayers=${instrNumLayers[$instrID]}\n        filesIndex=${instrFilesIndex[$instrID]}\n        fileID=$filesIndex\n        addGroup\n        (( key++ ))\n    done\n\n    if [ -f \"$sfzFileName\".sfz ]; then\n        setTextColour $WHITE; echo -e \"Warning: A file with the name $sfzFileName.sfz already exists \\nDo you want to overwrite it?\"\n        setTextColour $CYAN; read -e input\n        \n        if [[ $input == [yY] ]] || [[ $input == [yY][eE][sS] ]]; then\n            rm -f \"$sfzFileName\".sfz\n            echo\n        else\n            exitWithError \"Bailing out!\"\n        fi\n    fi\n\n    touch \"$sfzFileName\".sfz\n\n    for line in \"${textLines[@]}\"; do\n        printf '%s\\n' \"$line\" >> \"$sfzFileName\".sfz\n    done\n}\n\n\nfunction createMultiSfzFiles()\n{\n    declare -a textLines\n    declare -ri numInstruments=${#instrumentNames[@]}\n    declare -i instrID=0\n    declare instrName=\"\"\n    declare -i numLayers=0\n    declare -i layerNum=0\n    declare -i fileID=0\n    declare -i filesIndex=0\n    declare -i lovel=0\n    declare -i hivel=0\n    declare -i velIncrement=0\n    declare -i numBigIncrements=0\n    declare input=\"\"\n\n    for (( instrID=0; instrID < numInstruments; instrID++ )); do\n        unset textLines\n        textLines+=(\"// This file was auto-generated by makesfz\")\n        textLines+=(\"\")\n\n        instrName=\"${instrumentNames[$instrID]}\"\n        numLayers=${instrNumLayers[$instrID]}\n        filesIndex=${instrFilesIndex[$instrID]}\n        fileID=$filesIndex\n        addGroup\n\n        if [ -f \"$instrName\".sfz ]; then\n            setTextColour $WHITE; echo -e \"Warning: A file with the name $instrName.sfz already exists \\nDo you want to overwrite it?\"\n            setTextColour $CYAN; read -e input\n            \n            if [[ $input == [yY] ]] || [[ $input == [yY][eE][sS] ]]; then\n                rm -f \"$instrName\".sfz\n                echo\n            else\n                exitWithError \"Bailing out!\"\n            fi\n        fi\n\n        touch \"$instrName\".sfz\n\n        for line in \"${textLines[@]}\"; do\n            printf '%s\\n' \"$line\" >> \"$instrName\".sfz\n        done\n    done\n}\n\n\n# Main body of script\n\nprocessArgs\n\nif $isInteractiveModeOn; then\n    getValuesFromUser\nfi\n\nif (( lokey > hikey )); then\n    exitWithError \"lokey can't be higher than hikey\"\nfi\n\nmakePathsRelative\n\nif $isInteractiveModeOn; then\n    echo\nfi\n\nfindFiles\n\nif (( ${#sampleFilePaths[@]} == 0 )); then\n    exitWithError \"No $fileType files found!\"\nfi\n\nif $isSingleFile; then\n    createSingleSfzFile\nelse\n    createMultiSfzFiles\nfi\n\nif $isInteractiveModeOn; then\n    setTextColour $YELLOW; echo -e \"All done!\\n\"\nfi\n\nresetText\n"
    },
    {
      "filename": "mkv2ogg.sh",
      "path": "/staging/",
      "content": "#!/bin/sh\n\nvid=$1\n\nffmpeg -i \"$vid\" -vn -acodec libvorbis \"${vid%.mkv}.ogg\"\n"
    },
    {
      "filename": "noteinfo.rb",
      "path": "/staging/",
      "content": "#!/usr/bin/env ruby\n\nrequire 'childprocess'\nrequire 'clipboard'\nrequire 'coltrane'\nrequire 'highline/import'\nrequire 'pastel'\nrequire 'shellwords'\nrequire 'tty-box'\nrequire 'tty-cursor'\nrequire 'tty-prompt'\nrequire 'tty-screen'\nrequire 'tty-reader'\n\nargs = ARGV[0]\n\ndef forkoff(command)\n  fork do\n    exec(command.to_s)\n  end\nend\n\n# MIDI note names. NOTE_NAMES[0] is 'C', NOTE_NAMES[1] is 'C#', etc.\nNOTE_NAMES = [\n  'C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'\n]\n\nmodule Monk\n  module_function\n\n  include Coltrane::Theory\n\n  # Given a MIDI note number, return the name and octave as a string.\n  def midi_to_note(num)\n    Pitch.new(num).name\n  end\n\n  def note_to_midi(note)\n    Pitch.new(note).midi\n  end\n\n  def note_to_hz(note)\n    Pitch.new(note).frequency.frequency\n  end\n\n  # https://github.com/sonic-pi-net/sonic-pi/blob/main/app/server/ruby/lib/sonicpi/lang/western_theory.rb\n  def hz_to_midi(freq)\n    (12 * (Math.log(freq * 0.0022727272727) / Math.log(2))) + 69\n  end\n\n  def midi_to_hz(midi_note)\n    440.0 * (2**((midi_note - 69) / 12.0))\n  end\n\nend\n\ndef box_info(midi_note,symbolic_note,frequency)\n  return \"midi note:     #{midi_note} \\nsymbolic note: #{symbolic_note}\\nfrequency:     #{frequency.round(2)}\\n\"\nend\n\ncontinue = \"True\"\n\nreader = TTY::Reader.new\n\nreader.on(:keyctrl_x, :keyescape) do\n  puts \"Exiting...\"\n  exit\nend\n\nwhile continue == \"True\"\n  puts TTY::Cursor.clear_screen\n  prompt = TTY::Prompt.new(enable_color: true)\n  pastel = Pastel.new\n\n  active_color = Pastel.new.green.on_red.detach\n\n  input = pastel.yellow(\"Provide\") + \" midi notes \" + pastel.magenta(\"to convert: \")\n\n  notes = prompt.ask(input, color: :bright_green, convert: :array, active_color: :bright_yellow) do |q|\n    q.modify :up\n    q.convert -> (input) { input.split(/ \\s*/) }\n  end\n\n  info = []\n  to_clipboard = []\n\n  notes.each do |note|\n\n    if NOTE_NAMES.include?(note)\n      octave = prompt.ask(\"Note: '#{note}' Octave: \", convert: :int)\n      frequency = Monk.note_to_hz(note+octave.to_s)\n      midi_note = Monk.note_to_midi(note+octave.to_s)\n      symbolic_note = note+octave.to_s\n    else\n      midi_note = note.to_i\n      symbolic_note =Monk.midi_to_note(note.to_i).downcase\n      frequency = Monk.note_to_hz(note.to_i)\n    end\n\n    info << box_info(midi_note,symbolic_note,frequency)\n    to_clipboard << symbolic_note\n  #\n  end\n\n  box1 = TTY::Box.frame(\n    # top: 10,\n    # left: 20,\n    width: (TTY::Screen.width),\n    height: (TTY::Screen.height / 1.25).round,\n    border: :ascii,\n    align: :left,\n    padding: 3,\n    title: {\n      top_left: \" information \"\n    },\n    style: {\n      fg: :bright_yellow,\n      bg: :black,\n      border: {\n        fg: :bright_yellow,\n        bg: :black\n      }\n    }\n  ) do\n      \"#{info.join(\"\\n\")}\"\n\n  end\n\n  puts TTY::Cursor.clear_screen\n\n  print box1\n\n  Clipboard.copy(to_clipboard.map {|x| x.gsub(/\\#/,'s').to_sym})\n\n  #(0..2).each { |x| print \"\\n\"}\n  # system(\"notify-send 'midi note #{midi_note} #{symbolic_note}'\")\n\n  #loop do\n    line = reader.read_line(\"=> \")\n    break if line =~ /^exit/i\n  #end\n  continue = prompt.keypress('Depress any key for action ...', timeout: 60)\n\n  continue = \"True\" if continue\n\n  unless continue\n    puts \"bad bye\"\n    #puts TTY::Cursor.clear_screen\n    exit\n  end\nend\n"
    },
    {
      "filename": "notepad-small.sh",
      "path": "/staging/",
      "content": "#!/usr/bin/env bash\n\nTIMESTAMPE=$(date +%Y%m%d%H%M)\n\nTOPIC=$(gum input --placeholder='Topic' | sed 's/ /_/g')\n\nNOTESFOLDER=\"$HOME/Desktop/Notebook/Daily\"\n\nif [[ ! -d $NOTESFOLDER ]]; then\n  mkdir -pv \"$NOTESFOLDER\"\nfi\n\nclear\n\ngum write --width=50 --height=15 --char-limit=0 --header.margin=\"1 1\" --placeholder \"Words\" --value=\"# \" >> \"$NOTESFOLDER\"/\"$TOPIC\"_\"$TIMESTAMPE\".md\n"
    },
    {
      "filename": "notepad.sh",
      "path": "/staging/",
      "content": "#!/usr/bin/env bash\n\nTIMESTAMPE=$(date +%Y%m%d%H%M)\n\nTOPIC=$(gum input --placeholder='Topic' | sed 's/ /_/g')\n\nNOTESFOLDER=\"$HOME/Desktop/Notebook/Daily\"\n\nif [[ ! -d $NOTESFOLDER ]]; then\n  mkdir -pv \"$NOTESFOLDER\"\nfi\n\nclear\n\ngum write --width=50 --height=38 --char-limit=0 --header.margin=\"1 1\" --placeholder \"Words\" --value=\"# \" >> \"$NOTESFOLDER\"/\"$TOPIC\"_\"$TIMESTAMPE\".md\n"
    },
    {
      "filename": "pyenv_spacy_setup.sh",
      "path": "/staging/",
      "content": "#!/usr/bin/env bash\n\nAPP_ROOT=\"$HOME/Workspace/robotstuff\"\n\n# Check if pyenv is installed\nif ! command -v pyenv &> /dev/null; then\n    # install pyenv\n    paru -S pyenv --noconfirm\nfi\n\n# Check if Python 3.10.6 environment is installed\nif ! pyenv versions --bare | grep -q '3.10.6'; then\n    # create environment, using python 3.10.6\n    env CONFIGURE_OPTS=\"--enable-shared\" pyenv install 3.10.6\nfi\n\npyenv local 3.10.6\n\n# Check if spacy is installed\nif ! python -c \"import spacy\" &> /dev/null; then\n    # install spacy\n    pip install spacy\nfi\n\n# Check if language models are installed\nif ! python -c \"import spacy; spacy.load('en_core_web_sm')\" &> /dev/null; then\n    python -m spacy download en_core_web_sm\nfi\n\nif ! python -c \"import spacy; spacy.load('en_core_web_lg')\" &> /dev/null; then\n    python -m spacy download en_core_web_lg\nfi\n\n# Check if postgresql-libs is installed\nif ! $(paru -Q | awk '{ print $1}' |grep postgresql-libs) &> /dev/null; then\n    # install pyenv\n    paru -S postgresql-libs --noconfirm\nfi\n\ncd $APP_ROOT && sudo bundle update\n"
    },
    {
      "filename": "set-display-background.sh",
      "path": "/staging/",
      "content": "#!/usr/bin/env bash\n\ndeclare -rx BACKGROUNDS=\"/usr/share/backgrounds/syncopated\"\n\nif [ -x \"$(command -v autorandr)\" ]; then\n  profile=$(autorandr --detected)\n  autorandr -l \"$profile\"\nelse\n  echo \"autorandr not found\"\nfi\n\nsleep 1\n\n#note: tilda does not expand in quotes\nif [ -f ~/.fehbg ]; then\n  ~/.fehbg\nelse\n  #feh --no-fehbg --bg-scale $BACKGROUNDS/crescendo_01.png $BACKGROUNDS/str_01_03_bw_01.png\n  feh --recursive --bg-fill --randomize $BACKGROUNDS/*\nfi\n"
    },
    {
      "filename": "set-pulse-profiles.sh",
      "path": "/staging/",
      "content": "#!/usr/bin/env bash\n\n# set pulseaudio card profiles to off\n\ncards=($(pacmd list-cards | grep \"name:\" |\n                            awk '{ print $2 }' |\n                            sed 's/>//g' |\n                            sed 's/<//g'))\n\nfor i in \"${cards[@]}\"\ndo\n  pactl set-card-profile $i \"off\"\ndone\n"
    },
    {
      "filename": "stop-gvfs.sh",
      "path": "/staging/",
      "content": "#!/usr/bin/env bash\n\nservices=(\"gvfs-afc-volume-monitor\"\n          \"gvfs-daemon\"\n          \"gvfs-gphoto2-volume-monitor\"\n          \"gvfs-metadata\"\n          \"gvfs-mtp-volume-monitor\"\n          \"gvfs-udisks2-volume-monitor\")\n\nfor s in ${services[@]}; do\n  systemctl --user stop $s.service || continue\ndone\n\necho \"gvfs services stopped\"\n"
    },
    {
      "filename": "tmpdir-reaper.sh",
      "path": "/staging/",
      "content": "#!/usr/bin/env bash\n#\n\nif [[ ! -d /tmp/reaper ]]; then\n  mkdir -pv /tmp/reaper\nfi\n"
    },
    {
      "filename": "vts",
      "path": "/staging/",
      "content": "#!/usr/bin/env bash\n\ncd $HOME/Utils/vts\n\n"
    },
    {
      "filename": "requirements.txt",
      "path": "/",
      "content": "deepgram-sdk==3.7.4\npython-dotenv\ngroq\nffmpeg-python\npydub\nspacy\ngensim\nmoviepy\nvideogrep\nprogressbar2\ngoogle-generativeai\nsox"
    },
    {
      "filename": "PyDubAPI.md",
      "path": "/",
      "content": "# API Documentation\n\nThis document is a work in progress.\n\nIf you're looking for some functionality in particular, it's a good idea to take a look at the [source code](https://github.com/jiaaro/pydub). Core functionality is mostly in `pydub/audio_segment.py` – a number of `AudioSegment` methods are in the `pydub/effects.py` module, and added to `AudioSegment` via the effect registration process (the `register_pydub_effect()` decorator function)\n\nCurrently Undocumented:\n\n- Playback (`pydub.playback`)\n- Signal Processing (compression, EQ, normalize, speed change - `pydub.effects`, `pydub.scipy_effects`)\n- Signal generators (Sine, Square, Sawtooth, Whitenoise, etc - `pydub.generators`)\n- Effect registration system (basically the `pydub.utils.register_pydub_effect` decorator)\n\n\n## AudioSegment()\n\n`AudioSegment` objects are immutable, and support a number of operators.\n\n```python\nfrom pydub import AudioSegment\nsound1 = AudioSegment.from_file(\"/path/to/sound.wav\", format=\"wav\")\nsound2 = AudioSegment.from_file(\"/path/to/another_sound.wav\", format=\"wav\")\n\n# sound1 6 dB louder, then 3.5 dB quieter\nlouder = sound1 + 6\nquieter = sound1 - 3.5\n\n# sound1, with sound2 appended\ncombined = sound1 + sound2\n\n# sound1 repeated 3 times\nrepeated = sound1 * 3\n\n# duration\nduration_in_milliseconds = len(sound1)\n\n# first 5 seconds of sound1\nbeginning = sound1[:5000]\n\n# last 5 seconds of sound1\nend = sound1[-5000:]\n\n# split sound1 in 5-second slices\nslices = sound1[::5000]\n\n# Advanced usage, if you have raw audio data:\nsound = AudioSegment(\n    # raw audio data (bytes)\n    data=b'…',\n\n    # 2 byte (16 bit) samples\n    sample_width=2,\n\n    # 44.1 kHz frame rate\n    frame_rate=44100,\n\n    # stereo\n    channels=2\n)\n```\n\nAny operations that combine multiple `AudioSegment` objects in *any* way will first ensure that they have the same number of channels, frame rate, sample rate, bit depth, etc. When these things do not match, the lower quality sound is modified to match the quality of the higher quality sound so that quality is not lost: mono is converted to stereo, bit depth and frame rate/sample rate are increased as needed. If you do not want this behavior, you may explicitly reduce the number of channels, bits, etc using the appropriate `AudioSegment` methods.\n\n### AudioSegment(…).from_file()\n\nOpen an audio file as an `AudioSegment` instance and return it. there are also a number of wrappers provided for convenience, but you should probably just use this directly.\n\n```python\nfrom pydub import AudioSegment\n\n# wave and raw don’t use ffmpeg\nwav_audio = AudioSegment.from_file(\"/path/to/sound.wav\", format=\"wav\")\nraw_audio = AudioSegment.from_file(\"/path/to/sound.raw\", format=\"raw\",\n                                   frame_rate=44100, channels=2, sample_width=2)\n\n# all other formats use ffmpeg\nmp3_audio = AudioSegment.from_file(\"/path/to/sound.mp3\", format=\"mp3\")\n\n# use a file you've already opened (advanced …ish)\nwith open(\"/path/to/sound.wav\", \"rb\") as wav_file:\n    audio_segment = AudioSegment.from_file(wav_file, format=\"wav\")\n\n# also supports the os.PathLike protocol for python >= 3.6\nfrom pathlib import Path\nwav_path = Path(\"path/to/sound.wav\")\nwav_audio = AudioSegment.from_file(wav_path)\n```\n\nThe first argument is the path (as a string) of the file to read, **or** a file handle to read from.\n\n**Supported keyword arguments**:\n\n- `format` | example: `\"aif\"` | default: autodetected\n  Format of the output file. Supports `\"wav\"` and `\"raw\"` natively, requires ffmpeg for all other formats. `\"raw\"` files require 3 additional keyword arguments, `sample_width`, `frame_rate`, and `channels`, denoted below with: **`raw` only**. This extra info is required because raw audio files do not have headers to include this info in the file itself like wav files do.\n- `sample_width` | example: `2`\n  **`raw` only** — Use `1` for 8-bit audio `2` for 16-bit (CD quality) and `4` for 32-bit. It’s the number of bytes per sample.\n- `channels` | example: `1`\n  **`raw` only** — `1` for mono, `2` for stereo.\n- `frame_rate` | example: `2`\n  **`raw` only** — Also known as sample rate, common values are `44100` (44.1kHz - CD audio), and `48000` (48kHz - DVD audio)\n- `start_second` | example: `2.0` | default: `None`\n  Offset (in seconds) to start loading the audio file. If `None`, the audio will start loading from the beginning.\n- `duration` | example: `2.5` | default: `None`\n  Number of seconds to be loaded. If `None`, full audio will be loaded.\n\n\n### AudioSegment(…).export()\n\nWrite the `AudioSegment` object to a file – returns a file handle of the output file (you don't have to do anything with it, though).\n\n```python\nfrom pydub import AudioSegment\nsound = AudioSegment.from_file(\"/path/to/sound.wav\", format=\"wav\")\n\n# simple export\nfile_handle = sound.export(\"/path/to/output.mp3\", format=\"mp3\")\n\n# more complex export\nfile_handle = sound.export(\"/path/to/output.mp3\",\n                           format=\"mp3\",\n                           bitrate=\"192k\",\n                           tags={\"album\": \"The Bends\", \"artist\": \"Radiohead\"},\n                           cover=\"/path/to/albumcovers/radioheadthebends.jpg\")\n\n# split sound in 5-second slices and export\nfor i, chunk in enumerate(sound[::5000]):\n  with open(\"sound-%s.mp3\" % i, \"wb\") as f:\n    chunk.export(f, format=\"mp3\")\n```\n\nThe first argument is the location (as a string) to write the output, **or** a file handle to write to. If you do not pass an output file or path, a temporary file is generated.\n\n**Supported keyword arguments**:\n\n- `format` | example: `\"aif\"` | default: `\"mp3\"`\n  Format of the output file. Supports `\"wav\"` and `\"raw\"` natively, requires ffmpeg for all other formats.\n- `codec` | example: `\"libvorbis\"`\n  For formats that may contain content encoded with different codecs, you can specify the codec you'd like the encoder to use. For example, the \"ogg\" format is often used with the \"libvorbis\" codec. (requires ffmpeg)\n- `bitrate` | example: `\"128k\"`\n  For compressed formats, you can pass the bitrate you'd like the encoder to use (requires ffmpeg). Each codec accepts different bitrate arguments so take a look at the [ffmpeg documentation](https://www.ffmpeg.org/ffmpeg-codecs.html#Audio-Encoders) for details (bitrate usually shown as `-b`, `-ba` or `-a:b`).\n- `tags` | example: `{\"album\": \"1989\", \"artist\": \"Taylor Swift\"}`\n  Allows you to supply media info tags for the encoder (requires ffmpeg). Not all formats can receive tags (mp3 can).\n- `parameters` | example: `[\"-ac\", \"2\"]`\n  Pass additional [command line parameters](https://www.ffmpeg.org/ffmpeg.html) to the ffmpeg call. These are added to the end of the call (in the output file section).\n- `id3v2_version` | example: `\"3\"` | default: `\"4\"`\n  Set the ID3v2 version used by ffmpeg to add tags to the output file. If you want Windows Exlorer to display tags, use `\"3\"` here ([source](http://superuser.com/a/453133)).\n- `cover` | example: `\"/path/to/imgfile.png\"`\n  Allows you to supply a cover image (path to the image file). Currently, only MP3 files allow this keyword argument. Cover image must be a jpeg, png, bmp, or tiff file.\n\n\n### AudioSegment.empty()\n\nCreates a zero-duration `AudioSegment`.\n\n```python\nfrom pydub import AudioSegment\nempty = AudioSegment.empty()\n\nlen(empty) == 0\n```\n\nThis is useful for aggregation loops:\n```python\nfrom pydub import AudioSegment\n\nsounds = [\n  AudioSegment.from_wav(\"sound1.wav\"),\n  AudioSegment.from_wav(\"sound2.wav\"),\n  AudioSegment.from_wav(\"sound3.wav\"),\n]\n\nplaylist = AudioSegment.empty()\nfor sound in sounds:\n  playlist += sound\n```\n\n### AudioSegment.silent()\n\nCreates a silent audiosegment, which can be used as a placeholder, spacer, or as a canvas to overlay other sounds on top of.\n\n```python\nfrom pydub import AudioSegment\n\nten_second_silence = AudioSegment.silent(duration=10000)\n```\n\n**Supported keyword arguments**:\n\n- `duration` | example: `3000` | default: `1000` (1 second)\n  Length of the silent `AudioSegment`, in milliseconds\n- `frame_rate` | example `44100` | default: `11025` (11.025 kHz)\n  Frame rate (i.e., sample rate) of the silent `AudioSegment` in Hz\n\n### AudioSegment.from_mono_audiosegments()\n\nCreates a multi-channel audiosegment out of multiple mono audiosegments (two or more). Each mono audiosegment passed in should be exactly the same length, down to the frame count.\n\n```python\nfrom pydub import AudioSegment\n\nleft_channel = AudioSegment.from_wav(\"sound1.wav\")\nright_channel = AudioSegment.from_wav(\"sound1.wav\")\n\nstereo_sound = AudioSegment.from_mono_audiosegments(left_channel, right_channel)\n```\n\n### AudioSegment(…).dBFS\n\nReturns the loudness of the `AudioSegment` in dBFS (db relative to the maximum possible loudness). A Square wave at maximum amplitude will be roughly 0 dBFS (maximum loudness), whereas a Sine Wave at maximum amplitude will be roughly -3 dBFS.\n\n```python\nfrom pydub import AudioSegment\nsound = AudioSegment.from_file(\"sound1.wav\")\n\nloudness = sound.dBFS\n```\n\n### AudioSegment(…).channels\n\nNumber of channels in this audio segment (1 means mono, 2 means stereo)\n\n```python\nfrom pydub import AudioSegment\nsound = AudioSegment.from_file(\"sound1.wav\")\n\nchannel_count = sound.channels\n```\n\n### AudioSegment(…).sample_width\n\nNumber of bytes in each sample (1 means 8 bit, 2 means 16 bit, etc). CD Audio is 16 bit, (sample width of 2 bytes).\n\n```python\nfrom pydub import AudioSegment\nsound = AudioSegment.from_file(\"sound1.wav\")\n\nbytes_per_sample = sound.sample_width\n```\n\n### AudioSegment(…).frame_rate\n\nCD Audio has a 44.1kHz sample rate, which means `frame_rate` will be `44100` (same as sample rate, see `frame_width`). Common values are `44100` (CD), `48000` (DVD), `22050`, `24000`, `12000` and `11025`.\n\n```python\nfrom pydub import AudioSegment\nsound = AudioSegment.from_file(\"sound1.wav\")\n\nframes_per_second = sound.frame_rate\n```\n\n### AudioSegment(…).frame_width\n\nNumber of bytes for each \"frame\". A frame contains a sample for each channel (so for stereo you have 2 samples per frame, which are played simultaneously). `frame_width` is equal to `channels * sample_width`. For CD Audio it'll be `4` (2 channels times 2 bytes per sample).\n\n```python\nfrom pydub import AudioSegment\nsound = AudioSegment.from_file(\"sound1.wav\")\n\nbytes_per_frame = sound.frame_width\n```\n\n### AudioSegment(…).rms\n\nA measure of loudness. Used to compute dBFS, which is what you should use in most cases. Loudness is logarithmic (rms is not), which makes dB a much more natural scale.\n\n```python\nfrom pydub import AudioSegment\nsound = AudioSegment.from_file(\"sound1.wav\")\n\nloudness = sound.rms\n```\n\n### AudioSegment(…).max\n\nThe highest amplitude of any sample in the `AudioSegment`. Useful for things like normalization (which is provided in `pydub.effects.normalize`).\n\n```python\nfrom pydub import AudioSegment\nsound = AudioSegment.from_file(\"sound1.wav\")\n\npeak_amplitude = sound.max\n```\n\n### AudioSegment(…).max_dBFS\n\nThe highest amplitude of any sample in the `AudioSegment`, in dBFS (relative to the highest possible amplitude value). Useful for things like normalization (which is provided in `pydub.effects.normalize`).\n\n```python\nfrom pydub import AudioSegment\nsound = AudioSegment.from_file(\"sound1.wav\")\n\nnormalized_sound = sound.apply_gain(-sound.max_dBFS)\n```\n\n### AudioSegment(…).duration_seconds\n\nReturns the duration of the `AudioSegment` in seconds (`len(sound)` returns milliseconds). This is provided for convenience; it calls `len()` internally.\n\n```python\nfrom pydub import AudioSegment\nsound = AudioSegment.from_file(\"sound1.wav\")\n\nassert sound.duration_seconds == (len(sound) / 1000.0)\n```\n\n### AudioSegment(…).raw_data\n\nThe raw audio data of the AudioSegment. Useful for interacting with other audio libraries or weird APIs that want audio data in the form of a bytestring. Also comes in handy if you’re implementing effects or other direct signal processing.\n\nYou probably don’t need this, but if you do… you’ll know.\n\n```python\nfrom pydub import AudioSegment\nsound = AudioSegment.from_file(\"sound1.wav\")\n\nraw_audio_data = sound.raw_data\n```\n\n### AudioSegment(…).frame_count()\n\nReturns the number of frames in the `AudioSegment`. Optionally you may pass in a `ms` keywork argument to retrieve the number of frames in that number of milliseconds of audio in the `AudioSegment` (useful for slicing, etc).\n\n```python\nfrom pydub import AudioSegment\nsound = AudioSegment.from_file(\"sound1.wav\")\n\nnumber_of_frames_in_sound = sound.frame_count()\n\nnumber_of_frames_in_200ms_of_sound = sound.frame_count(ms=200)\n```\n\n**Supported keyword arguments**:\n\n- `ms` | example: `3000` | default: `None` (entire duration of `AudioSegment`)\n  When specified, method returns number of frames in X milliseconds of the `AudioSegment`\n\n### AudioSegment(…).append()\n\nReturns a new `AudioSegment`, created by appending another `AudioSegment` to this one (i.e., adding it to the end), Optionally using a crossfade. `AudioSegment(…).append()` is used internally when adding `AudioSegment` objects together with the `+` operator.\n\nBy default a 100ms (0.1 second) crossfade is used to eliminate pops and crackles.\n\n```python\nfrom pydub import AudioSegment\nsound1 = AudioSegment.from_file(\"sound1.wav\")\nsound2 = AudioSegment.from_file(\"sound2.wav\")\n\n# default 100 ms crossfade\ncombined = sound1.append(sound2)\n\n# 5000 ms crossfade\ncombined_with_5_sec_crossfade = sound1.append(sound2, crossfade=5000)\n\n# no crossfade\nno_crossfade1 = sound1.append(sound2, crossfade=0)\n\n# no crossfade\nno_crossfade2 = sound1 + sound2\n```\n\n**Supported keyword arguments**:\n\n- `crossfade` | example: `3000` | default: `100` (entire duration of `AudioSegment`)\n  When specified, method returns number of frames in X milliseconds of the `AudioSegment`\n\n### AudioSegment(…).overlay()\n\nOverlays an `AudioSegment` onto this one. In the resulting `AudioSegment` they will play simultaneously. If the overlaid `AudioSegment` is longer than this one, the result will be truncated (so the end of the overlaid sound will be cut off). The result is always the same length as this `AudioSegment` even when using the `loop`, and `times` keyword arguments.\n\nSince `AudioSegment` objects are immutable, you can get around this by overlaying the shorter sound on the longer one, or by creating a silent `AudioSegment` with the appropriate duration, and overlaying both sounds on to that one.\n\n```python\nfrom pydub import AudioSegment\nsound1 = AudioSegment.from_file(\"sound1.wav\")\nsound2 = AudioSegment.from_file(\"sound2.wav\")\n\nplayed_togther = sound1.overlay(sound2)\n\nsound2_starts_after_delay = sound1.overlay(sound2, position=5000)\n\nvolume_of_sound1_reduced_during_overlay = sound1.overlay(sound2, gain_during_overlay=-8)\n\nsound2_repeats_until_sound1_ends = sound1.overlay(sound2, loop=true)\n\nsound2_plays_twice = sound1.overlay(sound2, times=2)\n\n# assume sound1 is 30 sec long and sound2 is 5 sec long:\nsound2_plays_a_lot = sound1.overlay(sound2, times=10000)\nlen(sound1) == len(sound2_plays_a_lot)\n```\n\n**Supported keyword arguments**:\n\n- `position` | example: `3000` | default: `0` (beginning of this `AudioSegment`)\n  The overlaid `AudioSegment` will not begin until X milliseconds have passed\n- `loop` | example: `True` | default: `False` (entire duration of `AudioSegment`)\n  The overlaid `AudioSegment` will repeat (starting at `position`) until the end of this `AudioSegment`\n- `times` | example: `4` | default: `1` (entire duration of `AudioSegment`)\n  The overlaid `AudioSegment` will repeat X times (starting at `position`) but will still be truncated to the length of this `AudioSegment`\n- `gain_during_overlay` | example: `-6.0` | default: `0` (no change in volume during overlay)\n  Change the original audio by this many dB while overlaying audio. This can be used to make the original audio quieter while the overlaid audio plays.\n\n### AudioSegment(…).apply_gain(`gain`)\n\nChange the amplitude (generally, loudness) of the `AudioSegment`. Gain is specified in dB. This method is used internally by the `+` operator.\n\n```python\nfrom pydub import AudioSegment\nsound1 = AudioSegment.from_file(\"sound1.wav\")\n\n# make sound1 louder by 3.5 dB\nlouder_via_method = sound1.apply_gain(+3.5)\nlouder_via_operator = sound1 + 3.5\n\n# make sound1 quieter by 5.7 dB\nquieter_via_method = sound1.apply_gain(-5.7)\nquieter_via_operator = sound1 - 5.7\n```\n\n### AudioSegment(…).fade()\n\nA more general (more flexible) fade method. You may specify `start` and `end`, or one of the two along with duration (e.g., `start` and `duration`).\n\n```python\nfrom pydub import AudioSegment\nsound1 = AudioSegment.from_file(\"sound1.wav\")\n\nfade_louder_for_3_seconds_in_middle = sound1.fade(to_gain=+6.0, start=7500, duration=3000)\n\nfade_quieter_beteen_2_and_3_seconds = sound1.fade(to_gain=-3.5, start=2000, end=3000)\n\n# easy way is to use the .fade_in() convenience method. note: -120dB is basically silent.\nfade_in_the_hard_way = sound1.fade(from_gain=-120.0, start=0, duration=5000)\nfade_out_the_hard_way = sound1.fade(to_gain=-120.0, end=0, duration=5000)\n```\n\n**Supported keyword arguments**:\n\n- `to_gain` | example: `-3.0` | default: `0` (0dB, no change)\n  Resulting change at the end of the fade. `-6.0` means fade will be be from 0dB (no change) to -6dB, and everything after the fade will be -6dB.\n- `from_gain` | example: `-3.0` | default: `0` (0dB, no change)\n  Change at the beginning of the fade. `-6.0` means fade (and all audio before it) will be be at -6dB will fade up to 0dB – the rest of the audio after the fade will be at 0dB (i.e., unchanged).\n- `start` | example: `7500` | NO DEFAULT\n  Position to begin fading (in milliseconds). `5500` means fade will begin after 5.5 seconds.\n- `end` | example: `4` | NO DEFAULT\n  The overlaid `AudioSegment` will repeat X times (starting at `position`) but will still be truncated to the length of this `AudioSegment`\n- `duration` | example: `4` | NO DEFAULT\n  You can use `start` or `end` with duration, instead of specifying both - provided as a convenience.\n\n### AudioSegment(…).fade_out()\n\nFade out (to silent) the end of this `AudioSegment`. Uses `.fade()` internally.\n\n**Supported keyword arguments**:\n\n- `duration` | example: `5000` | NO DEFAULT\n  How long (in milliseconds) the fade should last. Passed directly to `.fade()` internally\n\n### AudioSegment(…).fade_in()\n\nFade in (from silent) the beginning of this `AudioSegment`. Uses `.fade()` internally.\n\n**Supported keyword arguments**:\n\n- `duration` | example: `5000` | NO DEFAULT\n  How long (in milliseconds) the fade should last. Passed directly to `.fade()` internally\n\n### AudioSegment(…).reverse()\n\nMake a copy of this `AudioSegment` that plays backwards. Useful for Pink Floyd, screwing around, and some audio processing algorithms.\n\n### AudioSegment(…).set_sample_width()\n\nCreates an equivalent version of this `AudioSegment` with the specified sample width (in bytes). Increasing this value does not generally cause a reduction in quality. Reducing it *definitely* does cause a loss in quality. Higher Sample width means more dynamic range.\n\n### AudioSegment(…).set_frame_rate()\n\nCreates an equivalent version of this `AudioSegment` with the specified frame rate (in Hz). Increasing this value does not generally cause a reduction in quality. Reducing it *definitely does* cause a loss in quality. Higher frame rate means larger frequency response (higher frequencies can be represented).\n\n### AudioSegment(…).set_channels()\n\nCreates an equivalent version of this `AudioSegment` with the specified number of channels (1 is Mono, 2 is Stereo). Converting from mono to stereo does not cause any audible change. Converting from stereo to mono may result in loss of quality (but only if the left and right chanels differ).\n\n### AudioSegment(…).split_to_mono()\n\nSplits a stereo `AudioSegment` into two, one for each channel (Left/Right). Returns a list with the new `AudioSegment` objects with the left channel at index 0 and the right channel at index 1.\n\n### AudioSegment(…).apply_gain_stereo()\n\n```python\nfrom pydub import AudioSegment\nsound1 = AudioSegment.from_file(\"sound1.wav\")\n\n# make left channel 6dB quieter and right channe 2dB louder\nstereo_balance_adjusted = sound1.apply_gain_stereo(-6, +2)\n```\nApply gain to the left and right channel of a stereo `AudioSegment`. If the `AudioSegment` is mono, it will be converted to stereo before applying the gain.\n\nBoth gain arguments are specified in dB.\n\n### AudioSegment(…).pan()\n\n```python\nfrom pydub import AudioSegment\nsound1 = AudioSegment.from_file(\"sound1.wav\")\n\n# pan the sound 15% to the right\npanned_right = sound1.pan(+0.15)\n\n# pan the sound 50% to the left\npanned_left = sound1.pan(-0.50)\n```\n\nTakes one positional argument, *pan amount*, which should be between -1.0 (100% left) and +1.0 (100% right)\n\nWhen pan_amount == 0.0 the left/right balance is not changed.\n\nPanning does not alter the *perceived* loundness, but since loudness\nis decreasing on one side, the other side needs to get louder to\ncompensate. When panned hard left, the left channel will be 3dB louder and\nthe right channel will be silent (and vice versa).\n\n### AudioSegment(…).get_array_of_samples()\n\nReturns the raw audio data as an array of (numeric) samples. Note: if the audio has multiple channels, the samples for each channel will be serialized – for example, stereo audio would look like `[sample_1_L, sample_1_R, sample_2_L, sample_2_R, …]`.\n\nThis method is mainly for use in implementing effects, and other processing.\n\n```python\nfrom pydub import AudioSegment\nsound = AudioSegment.from_file(“sound1.wav”)\n\nsamples = sound.get_array_of_samples()\n\n# then modify samples...\n\nnew_sound = sound._spawn(samples)\n```\n\nnote that when using numpy or scipy you will need to convert back to an array before you spawn:\n\n```python\nimport array\nimport numpy as np\nfrom pydub import AudioSegment\n\nsound = AudioSegment.from_file(“sound1.wav”)\nsamples = sound.get_array_of_samples()\n\n# Example operation on audio data\nshifted_samples = np.right_shift(samples, 1)\n\n# now you have to convert back to an array.array\nshifted_samples_array = array.array(sound.array_type, shifted_samples)\n\nnew_sound = sound._spawn(shifted_samples_array)\n```\n\nHere's how to convert to a numpy float32 array:\n\n```python\nimport numpy as np\nfrom pydub import AudioSegment\n\nsound = AudioSegment.from_file(\"sound1.wav\")\nsound = sound.set_frame_rate(16000)\nchannel_sounds = sound.split_to_mono()\nsamples = [s.get_array_of_samples() for s in channel_sounds]\n\nfp_arr = np.array(samples).T.astype(np.float32)\nfp_arr /= np.iinfo(samples[0].typecode).max\n```\n\nAnd how to convert it back to an AudioSegment:\n\n```python\nimport io\nimport scipy.io.wavfile\n\nwav_io = io.BytesIO()\nscipy.io.wavfile.write(wav_io, 16000, fp_arr)\nwav_io.seek(0)\nsound = pydub.AudioSegment.from_wav(wav_io)\n```\n\n### AudioSegment(…).get_dc_offset()\n\nReturns a value between -1.0 and 1.0 representing the DC offset of a channel. This is calculated using `audioop.avg()` and normalizing the result by samples max value.\n\n**Supported keyword arguments**:\n\n- `channel` | example: `2` | default: `1`\n  Selects left (1) or right (2) channel to calculate DC offset. If segment is mono, this value is ignored.\n\n### AudioSegment(…).remove_dc_offset()\n\nRemoves DC offset from channel(s). This is done by using `audioop.bias()`, so watch out for overflows.\n\n**Supported keyword arguments**:\n\n- `channel` | example: `2` | default: None\n  Selects left (1) or right (2) channel remove DC offset. If value if None, removes from all available channels. If segment is mono, this value is ignored.\n\n- `offset` | example: `-0.1` | default: None\n  Offset to be removed from channel(s). Calculates offset if it's None. Offset values must be between -1.0 and 1.0.\n\n## Effects\n\nCollection of DSP effects that are implemented by `AudioSegment` objects.\n\n### AudioSegment(…).invert_phase()\n\nMake a copy of this `AudioSegment` and inverts the phase of the signal. Can generate anti-phase waves for noise suppression or cancellation.\n\n## Silence\n\nVarious functions for finding/manipulating silence in AudioSegments. For creating silent AudioSegments, see AudioSegment.silent().\n\n### silence.detect_silence()\n\nReturns a list of all silent sections [start, end] in milliseconds of audio_segment. Inverse of detect_nonsilent(). Can be very slow since it has to iterate over the whole segment.\n\n```python\nfrom pydub import AudioSegment, silence\n\nprint(silence.detect_silence(AudioSegment.silent(2000)))\n# [[0, 2000]]\n```\n\n**Supported keyword arguments**:\n\n- `min_silence_len` | example: `500` | default: 1000\n  The minimum length for silent sections in milliseconds. If it is greater than the length of the audio segment an empty list will be returned.\n\n- `silence_thresh` | example: `-20` | default: -16\n  The upper bound for how quiet is silent in dBFS.\n\n- `seek_step` | example: `5` | default: 1\n  Size of the step for checking for silence in milliseconds. Smaller is more precise. Must be a positive whole number.\n\n### silence.detect_nonsilent()\n\nReturns a list of all silent sections [start, end] in milliseconds of audio_segment. Inverse of detect_silence() and has all the same arguments. Can be very slow since it has to iterate over the whole segment.\n\n**Supported keyword arguments**:\n\n- `min_silence_len` | example: `500` | default: 1000\n  The minimum length for silent sections in milliseconds. If it is greater than the length of the audio segment an empty list will be returned.\n\n- `silence_thresh` | example: `-20` | default: -16\n  The upper bound for how quiet is silent in dBFS.\n\n- `seek_step` | example: `5` | default: 1\n  Size of the step for checking for silence in milliseconds. Smaller is more precise. Must be a positive whole number.\n\n### silence.split_on_silence()\n\nReturns list of audio segments from splitting audio_segment on silent sections.\n\n**Supported keyword arguments**:\n\n- `min_silence_len` | example: `500` | default: 1000\n  The minimum length for silent sections in milliseconds. If it is greater than the length of the audio segment an empty list will be returned.\n\n- `silence_thresh` | example: `-20` | default: -16\n  The upper bound for how quiet is silent in dBFS.\n\n- `seek_step` | example: `5` | default: 1\n  Size of the step for checking for silence in milliseconds. Smaller is more precise. Must be a positive whole number.\n\n- `keep_silence` ~ example: True | default: 100\n  How much silence to keep in ms or a bool. leave some silence at the beginning and end of the chunks. Keeps the sound from sounding like it is abruptly cut off.\n  When the length of the silence is less than the keep_silence duration it is split evenly between the preceding and following non-silent segments.\n  If True is specified, all the silence is kept, if False none is kept.\n\n### silence.detect_leading_silence()\n\nReturns the millisecond/index that the leading silence ends. If there is no end it will return the length of the audio_segment.\n\n```python\nfrom pydub import AudioSegment, silence\n\nprint(silence.detect_silence(AudioSegment.silent(2000)))\n# 2000\n```\n\n**Supported keyword arguments**:\n\n- `silence_thresh` | example: `-20` | default: -50\n  The upper bound for how quiet is silent in dBFS.\n\n- `chunk_size` | example: `5` | default: 10\n  Size of the step for checking for silence in milliseconds. Smaller is more precise. Must be a positive whole number.\n"
    },
    {
      "filename": "audio_processing.py",
      "path": "/vts/",
      "content": "import os\nimport sys\nimport subprocess\nimport json\nimport progressbar\nfrom pydub import AudioSegment\n\nparent_directory = os.path.abspath('..')\n\nsys.path.append(parent_directory)\n\n\nfrom vts import (\n    audio_processing,\n    metadata_generation,\n    segment_analysis,\n    topic_modeling,\n    utils,\n)\n\ndef convert_to_mono_and_resample(input_file, output_file, sample_rate=16000):\n    \"\"\"Converts audio to mono, resamples, applies gain control, and a high-pass filter.\"\"\"\n    try:\n        command = [\n            \"ffmpeg\",\n            \"-i\",\n            input_file,\n            \"-af\",\n            \"highpass=f=200, acompressor=threshold=-12dB:ratio=4:attack=5:release=50, loudnorm\",\n            \"-ar\",\n            str(sample_rate),\n            \"-ac\",\n            \"1\",\n            \"-c:a\",\n            \"aac\",\n            \"-b:a\",\n            \"128k\",  # Use AAC codec for M4A\n            output_file,\n        ]\n        subprocess.run(command, check=True)\n        return {\n            \"status\": \"success\",\n            \"message\": f\"Audio converted to mono, resampled to {sample_rate}Hz, gain-adjusted, high-pass filtered, and saved to {output_file}\",\n        }\n    except subprocess.CalledProcessError as e:\n        return {\n            \"status\": \"error\",\n            \"message\": f\"Error during audio conversion: {str(e)}\",\n        }\n\n\ndef normalize_audio(input_file, output_file, lowpass_freq=8000, highpass_freq=100):\n    \"\"\"Normalizes audio using ffmpeg-normalize.\"\"\"\n    try:\n        command = [\n            \"ffmpeg-normalize\",\n            \"-pr\",  # Preserve ReplayGain tags\n            \"-tp\",\n            \"-3.0\",\n            \"-nt\",\n            \"rms\",\n            input_file,\n            \"-prf\",\n            f\"highpass=f={highpass_freq}, loudnorm\",\n            \"-prf\",\n            \"dynaudnorm=p=0.4:s=15\",\n            \"-pof\",\n            f\"lowpass=f={lowpass_freq}\",\n            \"-ar\",\n            \"48000\",\n            \"-c:a\",\n            \"pcm_s16le\",\n            \"--keep-loudness-range-target\",\n            \"-o\",\n            output_file,\n        ]\n        subprocess.run(command, check=True)\n        return {\n            \"status\": \"success\",\n            \"message\": f\"Audio normalized and saved to {output_file}\",\n        }\n    except subprocess.CalledProcessError as e:\n        return {\n            \"status\": \"error\",\n            \"message\": f\"Error during audio normalization: {str(e)}\",\n        }\n\n\ndef remove_silence(input_file, output_file, duration=\"1.5\", threshold=\"-25\"):\n    \"\"\"Removes silence from audio using unsilence.\"\"\"\n    try:\n        command = [\n            \"unsilence\",\n            \"-y\",  # non-interactive mode\n            \"-d\",  # Delete silent parts\n            \"-ss\",\n            duration,  # Minimum silence duration\n            \"-sl\",\n            threshold,  # Silence threshold\n            input_file,\n            output_file,\n        ]\n        subprocess.run(command, check=True)\n        return {\n            \"status\": \"success\",\n            \"message\": f\"Silence removed from audio and saved to {output_file}\",\n        }\n    except subprocess.CalledProcessError as e:\n        return {\"status\": \"error\", \"message\": f\"Error during silence removal: {str(e)}\"}\n\n\ndef extract_audio(video_path, output_path):\n    print(\"Extracting audio from video...\")\n    try:\n        audio = AudioSegment.from_file(video_path)\n        audio.export(output_path, format=\"wav\")\n        print(\"Audio extraction complete.\")\n    except Exception as e:\n        print(f\"Error during audio extraction: {str(e)}\")\n        raise"
    },
    {
      "filename": "metadata_generation.py",
      "path": "/vts/",
      "content": "import os\nimport sys\nimport json\nimport progressbar\n\nparent_directory = os.path.abspath('..')\n\nsys.path.append(parent_directory)\n\n\nfrom vts import (\n    audio_processing,\n    metadata_generation,\n    segment_analysis,\n    topic_modeling,\n    utils,\n)\n\ndef save_transcription(transcription, project_path):\n    transcription_path = os.path.join(project_path, \"transcription.json\")\n    with open(transcription_path, \"w\") as f:\n        json.dump(transcription, f, indent=2)\n    print(f\"Transcription saved to: {transcription_path}\")\n\n\ndef save_transcript(transcript, project_path):\n    transcript_path = os.path.join(project_path, \"transcript.json\")\n    with open(transcript_path, \"w\") as f:\n        json.dump(transcript, f, indent=2)\n    print(f\"Transcript saved to: {transcript_path}\")\n\n\ndef load_transcript(transcript_path):\n    with open(transcript_path, \"r\") as f:\n        return json.load(f)\n\n\ndef generate_metadata(segments, lda_model):\n    print(\"Generating metadata for segments...\")\n    metadata = []\n    for i, segment in enumerate(progressbar.progressbar(segments)):\n        segment_metadata = {\n            \"segment_id\": i + 1,\n            \"start_time\": segment[\"start\"],\n            \"end_time\": segment[\"end\"],\n            \"duration\": segment[\"end\"] - segment[\"start\"],\n            \"dominant_topic\": segment[\"topic\"],\n            \"top_keywords\": [\n                word for word, _ in lda_model.show_topic(segment[\"topic\"], topn=5)\n            ],\n            \"transcript\": segment[\"content\"],\n        }\n        metadata.append(segment_metadata)\n    print(\"Metadata generation complete.\")\n    return metadata\n"
    },
    {
      "filename": "segment_analysis.py",
      "path": "/vts/",
      "content": "import os\nimport sys\nimport json\nimport progressbar\nimport google.generativeai as genai\nfrom PIL import Image\nfrom moviepy.editor import VideoFileClip\n\nparent_directory = os.path.abspath('..')\n\nsys.path.append(parent_directory)\n\n\nfrom vts import (\n    audio_processing,\n    metadata_generation,\n    segment_analysis,\n    topic_modeling,\n    utils,\n)\n\ndef analyze_segment_with_gemini(segment_path, transcript):\n    print(f\"Analyzing segment: {segment_path}\")\n    # Set up the Gemini model\n    genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n\n    # Load the video segment as an image (first frame)\n    video = VideoFileClip(segment_path)\n    frame = video.get_frame(0)\n    image = Image.fromarray(frame)\n    video.close()\n\n    # Prepare the prompt\n    prompt = f\"Analyze this video segment. The transcript for this segment is: '{transcript}'. Describe the main subject matter, key visual elements, and how they relate to the transcript.\"\n\n    # Generate content\n    response = model.generate_content([prompt, image])\n\n    return response.text\n\n\ndef split_and_analyze_video(input_video, segments, output_dir):\n    print(\"Splitting video into segments and analyzing...\")\n    try:\n        video = VideoFileClip(input_video)\n        analyzed_segments = []\n\n        for i, segment in enumerate(progressbar.progressbar(segments)):\n            start_time = segment[\"start_time\"]\n            end_time = segment[\"end_time\"]\n            segment_clip = video.subclip(start_time, end_time)\n            output_path = os.path.join(output_dir, f\"segment_{i+1}.mp4\")\n            segment_clip.write_videofile(\n                output_path, codec=\"libx264\", audio_codec=\"aac\"\n            )\n\n            # Analyze the segment with Gemini\n            gemini_analysis = analyze_segment_with_gemini(\n                output_path, segment[\"transcript\"]\n            )\n\n            analyzed_segments.append(\n                {\n                    \"segment_id\": i + 1,\n                    \"start_time\": start_time,\n                    \"end_time\": end_time,\n                    \"transcript\": segment[\"transcript\"],\n                    \"topic\": segment[\"dominant_topic\"],\n                    \"keywords\": segment[\"top_keywords\"],\n                    \"gemini_analysis\": gemini_analysis,\n                }\n            )\n\n        video.close()\n        print(\"Video splitting and analysis complete.\")\n        return analyzed_segments\n    except Exception as e:\n        print(f\"Error during video splitting and analysis: {str(e)}\")\n        raise"
    },
    {
      "filename": "topic_modeling.py",
      "path": "/vts/",
      "content": "import os\nimport sys\nimport json\nimport progressbar\nimport spacy\nfrom gensim import corpora\nfrom gensim.models import LdaMulticore\n\nparent_directory = os.path.abspath('..')\n\nsys.path.append(parent_directory)\n\n\nfrom vts import (\n    audio_processing,\n    metadata_generation,\n    segment_analysis,\n    topic_modeling,\n    utils,\n)\n\n# Load spaCy model (move this to a setup function or main if possible)\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef preprocess_text(text):\n    print(\"Preprocessing text...\")\n    doc = nlp(text)\n    subjects = []\n\n    for sent in doc.sents:\n        for token in sent:\n            if \"subj\" in token.dep_:\n                if token.dep_ in [\"nsubj\", \"nsubjpass\", \"csubj\"]:\n                    subject = get_compound_subject(token)\n                    subjects.append(subject)\n\n    cleaned_subjects = [\n        [\n            token.lemma_.lower()\n            for token in nlp(subject)\n            if not token.is_stop and not token.is_punct and token.is_alpha\n        ]\n        for subject in subjects\n    ]\n\n    cleaned_subjects = [\n        list(s) for s in set(tuple(sub) for sub in cleaned_subjects) if s\n    ]\n\n    print(\n        f\"Text preprocessing complete. Extracted {len(cleaned_subjects)} unique subjects.\"\n    )\n    return cleaned_subjects\n\n\ndef get_compound_subject(token):\n    subject = [token.text]\n    for left_token in token.lefts:\n        if left_token.dep_ == \"compound\":\n            subject.insert(0, left_token.text)\n    for right_token in token.rights:\n        if right_token.dep_ == \"compound\":\n            subject.append(right_token.text)\n    return \" \".join(subject)\n\n\ndef perform_topic_modeling(subjects, num_topics=5):\n    print(f\"Performing topic modeling with {num_topics} topics...\")\n    dictionary = corpora.Dictionary(subjects)\n    corpus = [dictionary.doc2bow(subject) for subject in subjects]\n    lda_model = LdaMulticore(\n        corpus=corpus,\n        id2word=dictionary,\n        num_topics=num_topics,\n        random_state=100,\n        chunksize=100,\n        passes=10,\n        per_word_topics=True,\n    )\n    print(\"Topic modeling complete.\")\n    return lda_model, corpus, dictionary\n\n\ndef identify_segments(transcript, lda_model, dictionary, num_topics):\n    print(\"Identifying segments based on topics...\")\n    segments = []\n    current_segment = {\"start\": 0, \"end\": 0, \"content\": \"\", \"topic\": None}\n\n    for sentence in progressbar.progressbar(transcript):\n        subjects = preprocess_text(sentence[\"content\"])\n        if not subjects:\n            continue\n\n        bow = dictionary.doc2bow([token for subject in subjects for token in subject])\n        topic_dist = lda_model.get_document_topics(bow)\n        dominant_topic = max(topic_dist, key=lambda x: x[1])[0] if topic_dist else None\n\n        if dominant_topic != current_segment[\"topic\"]:\n            if current_segment[\"content\"]:\n                current_segment[\"end\"] = sentence[\"start\"]\n                segments.append(current_segment)\n            current_segment = {\n                \"start\": sentence[\"start\"],\n                \"end\": sentence[\"end\"],\n                \"content\": sentence[\"content\"],\n                \"topic\": dominant_topic,\n            }\n        else:\n            current_segment[\"end\"] = sentence[\"end\"]\n            current_segment[\"content\"] += \" \" + sentence[\"content\"]\n\n    if current_segment[\"content\"]:\n        segments.append(current_segment)\n\n    print(f\"Identified {len(segments)} segments.\")\n    return segments"
    },
    {
      "filename": "utils.py",
      "path": "/vts/",
      "content": "import os\nimport sys\nimport time\nimport json\nimport progressbar\nimport videogrep  # Make sure videogrep is installed\nfrom deepgram import DeepgramClient, PrerecordedOptions, FileSource, DeepgramError\nfrom groq import Groq\n\nparent_directory = os.path.abspath('..')\n\nsys.path.append(parent_directory)\n\n\nfrom vts import (\n    audio_processing,\n    metadata_generation,\n    segment_analysis,\n    topic_modeling,\n    utils,\n)\n\n\ndef create_project_folder(input_path, base_output_dir):\n    base_name = os.path.splitext(os.path.basename(input_path))[0]\n    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n    project_name = f\"{base_name}_{timestamp}\"\n    project_path = os.path.join(base_output_dir, project_name)\n    os.makedirs(project_path, exist_ok=True)\n    return project_path\n\ndef transcribe_file_deepgram(client, file_path, options, max_retries=3, retry_delay=5):\n    print(\"Transcribing audio using Deepgram...\")\n    for attempt in range(max_retries):\n        try:\n            with open(file_path, \"rb\") as audio:\n                buffer_data = audio.read()\n                payload: FileSource = {\"buffer\": buffer_data, \"mimetype\": \"audio/mp4\"}\n                response = client.listen.rest.v(\"1\").transcribe_file(payload, options)\n            print(\"Transcription complete.\")\n            return json.loads(response.to_json())\n        except DeepgramError as e:\n            if attempt < max_retries - 1:\n                print(\n                    f\"API call failed. Retrying in {retry_delay} seconds... (Attempt {attempt + 1}/{max_retries})\"\n                )\n                time.sleep(retry_delay)\n            else:\n                print(f\"Transcription failed after {max_retries} attempts: {str(e)}\")\n                raise\n        except Exception as e:\n            print(f\"Unexpected error during transcription: {str(e)}\")\n            raise\n\n\ndef transcribe_file_groq(\n    client, file_path, model=\"whisper-large-v3\", language=\"en\", prompt=None\n):\n    print(\"Transcribing audio using Groq...\")\n    try:\n        with open(file_path, \"rb\") as file:\n            transcription = client.audio.transcriptions.create(\n                file=(file_path, file.read()),\n                model=model,\n                prompt=prompt,\n                response_format=\"verbose_json\",\n                language=language,\n                temperature=0.2\n            )\n        print(\"Transcription complete.\")\n        return json.loads(transcription.text)\n    except Exception as e:\n        print(f\"Error during Groq transcription: {str(e)}\")\n        raise\n\ndef handle_audio_video(video_path, project_path):\n    audio_dir = os.path.join(project_path, \"audio\")\n    os.makedirs(audio_dir, exist_ok=True)\n\n    normalized_video_path = os.path.join(project_path, \"normalized_video.mkv\")\n    normalize_result = audio_processing.normalize_audio(video_path, normalized_video_path)\n    if normalize_result[\"status\"] == \"error\":\n        print(f\"Error during audio normalization: {normalize_result['message']}\")\n        # Handle the error (e.g., exit or continue without normalization)\n    else:\n        print(normalize_result[\"message\"])\n\n    # Remove silence\n    unsilenced_video_path = os.path.join(project_path, \"unsilenced_video.mkv\")\n    silence_removal_result = audio_processing.remove_silence(\n        normalized_video_path, unsilenced_video_path\n    )\n    if silence_removal_result[\"status\"] == \"error\":\n        print(f\"Error during silence removal: {silence_removal_result['message']}\")\n        # Handle the error (e.g., exit or continue without silence removal)\n    else:\n        print(silence_removal_result[\"message\"])\n\n    # Extract audio from unsilenced video\n    raw_audio_path = os.path.join(audio_dir, \"extracted_audio.wav\")\n    audio_processing.extract_audio(unsilenced_video_path, raw_audio_path)\n\n    # Convert to mono and resample for transcription\n    mono_resampled_audio_path = os.path.join(audio_dir, \"mono_resampled_audio.m4a\")\n    conversion_result = audio_processing.convert_to_mono_and_resample(\n        raw_audio_path, mono_resampled_audio_path\n    )\n    if conversion_result[\"status\"] == \"error\":\n        print(f\"Error during audio conversion: {conversion_result['message']}\")\n        # Handle the error (e.g., exit or continue without conversion)\n    else:\n        print(conversion_result[\"message\"])\n\n    return unsilenced_video_path, mono_resampled_audio_path\n\n\ndef handle_transcription(\n    video_path, audio_path, project_path, api=\"deepgram\", num_topics=2, groq_prompt=None\n):\n    segments_dir = os.path.join(project_path, \"segments\")\n    os.makedirs(segments_dir, exist_ok=True)\n\n    print(\"Parsing transcript with Videogrep...\")\n    transcript = videogrep.parse_transcript(video_path)\n    print(\"Transcript parsing complete.\")\n\n    if not transcript:\n        print(\"No transcript found. Transcribing audio...\")\n        deepgram_key = os.getenv(\"DG_API_KEY\")\n        groq_key = os.getenv(\"GROQ_API_KEY\")\n\n        if not deepgram_key:\n            raise ValueError(\"DG_API_KEY environment variable is not set\")\n        if not groq_key and api == \"groq\":\n            raise ValueError(\"GROQ_API_KEY environment variable is not set\")\n\n        if api == \"deepgram\":\n            deepgram_client = DeepgramClient(deepgram_key)\n            deepgram_options = PrerecordedOptions(\n                model=\"nova-2\",\n                language=\"en\",\n                topics=True,\n                intents=True,\n                smart_format=True,\n                punctuate=True,\n                paragraphs=True,\n                utterances=True,\n                diarize=True,\n                filler_words=True,\n                sentiment=True,\n            )\n            # Transcribe the normalized audio\n            transcription = transcribe_file_deepgram(\n                deepgram_client, audio_path, deepgram_options\n            )\n            transcript = [\n                {\n                    \"content\": utterance[\"transcript\"],\n                    \"start\": utterance[\"start\"],\n                    \"end\": utterance[\"end\"],\n                }\n                for utterance in transcription[\"results\"][\"utterances\"]\n            ]\n        else:  # Groq\n            groq_client = Groq(api_key=groq_key)\n            # Transcribe the normalized audio\n            transcription = transcribe_file_groq(\n                groq_client, audio_path, prompt=groq_prompt\n            )\n            transcript = [\n                {\n                    \"content\": segment[\"text\"],\n                    \"start\": segment[\"start\"],\n                    \"end\": segment[\"end\"],\n                }\n                for segment in transcription[\"segments\"]\n            ]\n\n    metadata_generation.save_transcription(transcription, project_path)\n\n    metadata_generation.save_transcript(transcript, project_path)\n    results = process_transcript(transcript, project_path, num_topics)\n\n    # Split the video and analyze segments\n    analyzed_segments = segment_analysis.split_and_analyze_video(\n        video_path, results[\"segments\"], segments_dir\n    )\n\n    # Update results with analyzed segments\n    results[\"analyzed_segments\"] = analyzed_segments\n\n    # Save updated results\n    results_path = os.path.join(project_path, \"results.json\")\n    with open(results_path, \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # print(\"Cleaning up temporary files...\")\n    # os.remove(raw_audio_path)\n    # os.remove(normalized_audio_path) # Uncomment to remove normalized audio\n\n    return results\n\n\ndef process_video(\n    video_path, project_path, api=\"deepgram\", num_topics=2, groq_prompt=None\n):\n\n    unsilenced_video_path, mono_resampled_audio_path = handle_audio_video(\n        video_path, project_path\n    )\n    results = handle_transcription(\n        unsilenced_video_path,\n        mono_resampled_audio_path,\n        project_path,\n        api,\n        num_topics,\n        groq_prompt,\n    )\n\n    return results\n\n\ndef process_transcript(transcript, project_path, num_topics=5):\n    full_text = \" \".join([sentence[\"content\"] for sentence in transcript])\n    preprocessed_subjects = topic_modeling.preprocess_text(full_text)\n    lda_model, corpus, dictionary = topic_modeling.perform_topic_modeling(\n        preprocessed_subjects, num_topics\n    )\n\n    segments = topic_modeling.identify_segments(transcript, lda_model, dictionary, num_topics)\n    metadata = metadata_generation.generate_metadata(segments, lda_model)\n\n    results = {\n        \"topics\": [\n            {\n                \"topic_id\": topic_id,\n                \"words\": [word for word, _ in lda_model.show_topic(topic_id, topn=10)],\n            }\n            for topic_id in range(num_topics)\n        ],\n        \"segments\": metadata,\n    }\n\n    results_path = os.path.join(project_path, \"results.json\")\n    with open(results_path, \"w\") as f:\n        json.dump(results, f, indent=2)\n    print(f\"Results saved to: {results_path}\")\n\n    return results\n"
    },
    {
      "filename": "main.py",
      "path": "/vts/",
      "content": "#!/usr/bin/env python \n\nimport os\nimport sys\nimport argparse\nimport json\nfrom dotenv import load_dotenv\nimport videogrep  # Make sure videogrep is installed\nimport progressbar  # Make sure progressbar2 is installed\n\nparent_directory = os.path.abspath('..')\n\nsys.path.append(parent_directory)\n\n\nfrom vts import (\n    audio_processing,\n    metadata_generation,\n    segment_analysis,\n    topic_modeling,\n    utils,\n)\n\n\nload_dotenv()\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Process video or transcript for topic-based segmentation and multi-modal analysis\"\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--input\",\n        required=True,\n        help=\"Path to the input video file or transcript JSON\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output\",\n        default=os.getcwd(),\n        help=\"Base output directory for project folders\",\n    )\n    parser.add_argument(\n        \"--api\",\n        choices=[\"deepgram\", \"groq\"],\n        default=\"deepgram\",\n        help=\"Choose API: deepgram or groq\",\n    )\n    parser.add_argument(\n        \"--topics\", type=int, default=5, help=\"Number of topics for LDA model\"\n    )\n    parser.add_argument(\"--groq-prompt\", help=\"Optional prompt for Groq transcription\")\n    args = parser.parse_args()\n\n    project_path = utils.create_project_folder(args.input, args.output)\n    print(f\"Created project folder: {project_path}\")\n\n    try:\n        if args.input.endswith(\".json\"):\n            transcript = load_transcript(args.input)\n            results = utils.process_transcript(transcript, project_path, args.topics)\n            print(\n                \"Note: Video splitting and Gemini analysis are not performed when processing a transcript file.\"\n            )\n        else:\n            results = utils.process_video(\n                args.input, project_path, args.api, args.topics, args.groq_prompt\n            )\n\n        print(f\"\\nProcessing complete. Project folder: {project_path}\")\n        print(f\"Results saved in: {os.path.join(project_path, 'results.json')}\")\n        print(\"\\nTop words for each topic:\")\n        for topic in results[\"topics\"]:\n            print(f\"Topic {topic['topic_id'] + 1}: {', '.join(topic['words'])}\")\n        print(f\"\\nGenerated and analyzed {len(results['analyzed_segments'])} segments\")\n\n        if not args.input.endswith(\".json\"):\n            print(f\"Video segments saved in: {os.path.join(project_path, 'segments')}\")\n    except Exception as e:\n        print(f\"\\nAn error occurred during processing: {str(e)}\")\n        print(\"Please check the project folder for any partial results or logs.\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "filename": "__init__.py",
      "path": "/vts/",
      "content": null
    },
    {
      "filename": "vts.json",
      "path": "/",
      "content": "{\n  \"files\": [\n    {\n      \"filename\": \"audio_processing.py\",\n      \"content\": \"import os\\nimport sys\\nimport subprocess\\nimport json\\nimport progressbar\\nfrom pydub import AudioSegment\\n\\nparent_directory = os.path.abspath('..')\\n\\nsys.path.append(parent_directory)\\n\\n\\nfrom vts import (\\n    audio_processing,\\n    metadata_generation,\\n    segment_analysis,\\n    topic_modeling,\\n    utils,\\n)\\n\\ndef convert_to_mono_and_resample(input_file, output_file, sample_rate=16000):\\n    \\\"\\\"\\\"Converts audio to mono, resamples, applies gain control, and a high-pass filter.\\\"\\\"\\\"\\n    try:\\n        command = [\\n            \\\"ffmpeg\\\",\\n            \\\"-i\\\",\\n            input_file,\\n            \\\"-af\\\",\\n            \\\"highpass=f=200, acompressor=threshold=-12dB:ratio=4:attack=5:release=50, loudnorm\\\",\\n            \\\"-ar\\\",\\n            str(sample_rate),\\n            \\\"-ac\\\",\\n            \\\"1\\\",\\n            \\\"-c:a\\\",\\n            \\\"aac\\\",\\n            \\\"-b:a\\\",\\n            \\\"128k\\\",  # Use AAC codec for M4A\\n            output_file,\\n        ]\\n        subprocess.run(command, check=True)\\n        return {\\n            \\\"status\\\": \\\"success\\\",\\n            \\\"message\\\": f\\\"Audio converted to mono, resampled to {sample_rate}Hz, gain-adjusted, high-pass filtered, and saved to {output_file}\\\",\\n        }\\n    except subprocess.CalledProcessError as e:\\n        return {\\n            \\\"status\\\": \\\"error\\\",\\n            \\\"message\\\": f\\\"Error during audio conversion: {str(e)}\\\",\\n        }\\n\\n\\ndef normalize_audio(input_file, output_file, lowpass_freq=8000, highpass_freq=100):\\n    \\\"\\\"\\\"Normalizes audio using ffmpeg-normalize.\\\"\\\"\\\"\\n    try:\\n        command = [\\n            \\\"ffmpeg-normalize\\\",\\n            \\\"-pr\\\",  # Preserve ReplayGain tags\\n            \\\"-tp\\\",\\n            \\\"-3.0\\\",\\n            \\\"-nt\\\",\\n            \\\"rms\\\",\\n            input_file,\\n            \\\"-prf\\\",\\n            f\\\"highpass=f={highpass_freq}, loudnorm\\\",\\n            \\\"-prf\\\",\\n            \\\"dynaudnorm=p=0.4:s=15\\\",\\n            \\\"-pof\\\",\\n            f\\\"lowpass=f={lowpass_freq}\\\",\\n            \\\"-ar\\\",\\n            \\\"48000\\\",\\n            \\\"-c:a\\\",\\n            \\\"pcm_s16le\\\",\\n            \\\"--keep-loudness-range-target\\\",\\n            \\\"-o\\\",\\n            output_file,\\n        ]\\n        subprocess.run(command, check=True)\\n        return {\\n            \\\"status\\\": \\\"success\\\",\\n            \\\"message\\\": f\\\"Audio normalized and saved to {output_file}\\\",\\n        }\\n    except subprocess.CalledProcessError as e:\\n        return {\\n            \\\"status\\\": \\\"error\\\",\\n            \\\"message\\\": f\\\"Error during audio normalization: {str(e)}\\\",\\n        }\\n\\n\\ndef remove_silence(input_file, output_file, duration=\\\"1.5\\\", threshold=\\\"-25\\\"):\\n    \\\"\\\"\\\"Removes silence from audio using unsilence.\\\"\\\"\\\"\\n    try:\\n        command = [\\n            \\\"unsilence\\\",\\n            \\\"-y\\\",  # non-interactive mode\\n            \\\"-d\\\",  # Delete silent parts\\n            \\\"-ss\\\",\\n            duration,  # Minimum silence duration\\n            \\\"-sl\\\",\\n            threshold,  # Silence threshold\\n            input_file,\\n            output_file,\\n        ]\\n        subprocess.run(command, check=True)\\n        return {\\n            \\\"status\\\": \\\"success\\\",\\n            \\\"message\\\": f\\\"Silence removed from audio and saved to {output_file}\\\",\\n        }\\n    except subprocess.CalledProcessError as e:\\n        return {\\\"status\\\": \\\"error\\\", \\\"message\\\": f\\\"Error during silence removal: {str(e)}\\\"}\\n\\n\\ndef extract_audio(video_path, output_path):\\n    print(\\\"Extracting audio from video...\\\")\\n    try:\\n        audio = AudioSegment.from_file(video_path)\\n        audio.export(output_path, format=\\\"wav\\\")\\n        print(\\\"Audio extraction complete.\\\")\\n    except Exception as e:\\n        print(f\\\"Error during audio extraction: {str(e)}\\\")\\n        raise\",\n      \"path\": \"/\"\n    },\n    {\n      \"filename\": \"metadata_generation.py\",\n      \"content\": \"import os\\nimport sys\\nimport json\\nimport progressbar\\n\\nparent_directory = os.path.abspath('..')\\n\\nsys.path.append(parent_directory)\\n\\n\\nfrom vts import (\\n    audio_processing,\\n    metadata_generation,\\n    segment_analysis,\\n    topic_modeling,\\n    utils,\\n)\\n\\ndef save_transcription(transcription, project_path):\\n    transcription_path = os.path.join(project_path, \\\"transcription.json\\\")\\n    with open(transcription_path, \\\"w\\\") as f:\\n        json.dump(transcription, f, indent=2)\\n    print(f\\\"Transcription saved to: {transcription_path}\\\")\\n\\n\\ndef save_transcript(transcript, project_path):\\n    transcript_path = os.path.join(project_path, \\\"transcript.json\\\")\\n    with open(transcript_path, \\\"w\\\") as f:\\n        json.dump(transcript, f, indent=2)\\n    print(f\\\"Transcript saved to: {transcript_path}\\\")\\n\\n\\ndef load_transcript(transcript_path):\\n    with open(transcript_path, \\\"r\\\") as f:\\n        return json.load(f)\\n\\n\\ndef generate_metadata(segments, lda_model):\\n    print(\\\"Generating metadata for segments...\\\")\\n    metadata = []\\n    for i, segment in enumerate(progressbar.progressbar(segments)):\\n        segment_metadata = {\\n            \\\"segment_id\\\": i + 1,\\n            \\\"start_time\\\": segment[\\\"start\\\"],\\n            \\\"end_time\\\": segment[\\\"end\\\"],\\n            \\\"duration\\\": segment[\\\"end\\\"] - segment[\\\"start\\\"],\\n            \\\"dominant_topic\\\": segment[\\\"topic\\\"],\\n            \\\"top_keywords\\\": [\\n                word for word, _ in lda_model.show_topic(segment[\\\"topic\\\"], topn=5)\\n            ],\\n            \\\"transcript\\\": segment[\\\"content\\\"],\\n        }\\n        metadata.append(segment_metadata)\\n    print(\\\"Metadata generation complete.\\\")\\n    return metadata\\n\",\n      \"path\": \"/\"\n    },\n    {\n      \"filename\": \"segment_analysis.py\",\n      \"content\": \"import os\\nimport sys\\nimport json\\nimport progressbar\\nimport google.generativeai as genai\\nfrom PIL import Image\\nfrom moviepy.editor import VideoFileClip\\n\\nparent_directory = os.path.abspath('..')\\n\\nsys.path.append(parent_directory)\\n\\n\\nfrom vts import (\\n    audio_processing,\\n    metadata_generation,\\n    segment_analysis,\\n    topic_modeling,\\n    utils,\\n)\\n\\ndef analyze_segment_with_gemini(segment_path, transcript):\\n    print(f\\\"Analyzing segment: {segment_path}\\\")\\n    # Set up the Gemini model\\n    genai.configure(api_key=os.getenv(\\\"GEMINI_API_KEY\\\"))\\n    model = genai.GenerativeModel(\\\"gemini-1.5-flash\\\")\\n\\n    # Load the video segment as an image (first frame)\\n    video = VideoFileClip(segment_path)\\n    frame = video.get_frame(0)\\n    image = Image.fromarray(frame)\\n    video.close()\\n\\n    # Prepare the prompt\\n    prompt = f\\\"Analyze this video segment. The transcript for this segment is: '{transcript}'. Describe the main subject matter, key visual elements, and how they relate to the transcript.\\\"\\n\\n    # Generate content\\n    response = model.generate_content([prompt, image])\\n\\n    return response.text\\n\\n\\ndef split_and_analyze_video(input_video, segments, output_dir):\\n    print(\\\"Splitting video into segments and analyzing...\\\")\\n    try:\\n        video = VideoFileClip(input_video)\\n        analyzed_segments = []\\n\\n        for i, segment in enumerate(progressbar.progressbar(segments)):\\n            start_time = segment[\\\"start_time\\\"]\\n            end_time = segment[\\\"end_time\\\"]\\n            segment_clip = video.subclip(start_time, end_time)\\n            output_path = os.path.join(output_dir, f\\\"segment_{i+1}.mp4\\\")\\n            segment_clip.write_videofile(\\n                output_path, codec=\\\"libx264\\\", audio_codec=\\\"aac\\\"\\n            )\\n\\n            # Analyze the segment with Gemini\\n            gemini_analysis = analyze_segment_with_gemini(\\n                output_path, segment[\\\"transcript\\\"]\\n            )\\n\\n            analyzed_segments.append(\\n                {\\n                    \\\"segment_id\\\": i + 1,\\n                    \\\"start_time\\\": start_time,\\n                    \\\"end_time\\\": end_time,\\n                    \\\"transcript\\\": segment[\\\"transcript\\\"],\\n                    \\\"topic\\\": segment[\\\"dominant_topic\\\"],\\n                    \\\"keywords\\\": segment[\\\"top_keywords\\\"],\\n                    \\\"gemini_analysis\\\": gemini_analysis,\\n                }\\n            )\\n\\n        video.close()\\n        print(\\\"Video splitting and analysis complete.\\\")\\n        return analyzed_segments\\n    except Exception as e:\\n        print(f\\\"Error during video splitting and analysis: {str(e)}\\\")\\n        raise\",\n      \"path\": \"/\"\n    },\n    {\n      \"filename\": \"topic_modeling.py\",\n      \"content\": \"import os\\nimport sys\\nimport json\\nimport progressbar\\nimport spacy\\nfrom gensim import corpora\\nfrom gensim.models import LdaMulticore\\n\\nparent_directory = os.path.abspath('..')\\n\\nsys.path.append(parent_directory)\\n\\n\\nfrom vts import (\\n    audio_processing,\\n    metadata_generation,\\n    segment_analysis,\\n    topic_modeling,\\n    utils,\\n)\\n\\n# Load spaCy model (move this to a setup function or main if possible)\\nnlp = spacy.load(\\\"en_core_web_sm\\\")\\n\\ndef preprocess_text(text):\\n    print(\\\"Preprocessing text...\\\")\\n    doc = nlp(text)\\n    subjects = []\\n\\n    for sent in doc.sents:\\n        for token in sent:\\n            if \\\"subj\\\" in token.dep_:\\n                if token.dep_ in [\\\"nsubj\\\", \\\"nsubjpass\\\", \\\"csubj\\\"]:\\n                    subject = get_compound_subject(token)\\n                    subjects.append(subject)\\n\\n    cleaned_subjects = [\\n        [\\n            token.lemma_.lower()\\n            for token in nlp(subject)\\n            if not token.is_stop and not token.is_punct and token.is_alpha\\n        ]\\n        for subject in subjects\\n    ]\\n\\n    cleaned_subjects = [\\n        list(s) for s in set(tuple(sub) for sub in cleaned_subjects) if s\\n    ]\\n\\n    print(\\n        f\\\"Text preprocessing complete. Extracted {len(cleaned_subjects)} unique subjects.\\\"\\n    )\\n    return cleaned_subjects\\n\\n\\ndef get_compound_subject(token):\\n    subject = [token.text]\\n    for left_token in token.lefts:\\n        if left_token.dep_ == \\\"compound\\\":\\n            subject.insert(0, left_token.text)\\n    for right_token in token.rights:\\n        if right_token.dep_ == \\\"compound\\\":\\n            subject.append(right_token.text)\\n    return \\\" \\\".join(subject)\\n\\n\\ndef perform_topic_modeling(subjects, num_topics=5):\\n    print(f\\\"Performing topic modeling with {num_topics} topics...\\\")\\n    dictionary = corpora.Dictionary(subjects)\\n    corpus = [dictionary.doc2bow(subject) for subject in subjects]\\n    lda_model = LdaMulticore(\\n        corpus=corpus,\\n        id2word=dictionary,\\n        num_topics=num_topics,\\n        random_state=100,\\n        chunksize=100,\\n        passes=10,\\n        per_word_topics=True,\\n    )\\n    print(\\\"Topic modeling complete.\\\")\\n    return lda_model, corpus, dictionary\\n\\n\\ndef identify_segments(transcript, lda_model, dictionary, num_topics):\\n    print(\\\"Identifying segments based on topics...\\\")\\n    segments = []\\n    current_segment = {\\\"start\\\": 0, \\\"end\\\": 0, \\\"content\\\": \\\"\\\", \\\"topic\\\": None}\\n\\n    for sentence in progressbar.progressbar(transcript):\\n        subjects = preprocess_text(sentence[\\\"content\\\"])\\n        if not subjects:\\n            continue\\n\\n        bow = dictionary.doc2bow([token for subject in subjects for token in subject])\\n        topic_dist = lda_model.get_document_topics(bow)\\n        dominant_topic = max(topic_dist, key=lambda x: x[1])[0] if topic_dist else None\\n\\n        if dominant_topic != current_segment[\\\"topic\\\"]:\\n            if current_segment[\\\"content\\\"]:\\n                current_segment[\\\"end\\\"] = sentence[\\\"start\\\"]\\n                segments.append(current_segment)\\n            current_segment = {\\n                \\\"start\\\": sentence[\\\"start\\\"],\\n                \\\"end\\\": sentence[\\\"end\\\"],\\n                \\\"content\\\": sentence[\\\"content\\\"],\\n                \\\"topic\\\": dominant_topic,\\n            }\\n        else:\\n            current_segment[\\\"end\\\"] = sentence[\\\"end\\\"]\\n            current_segment[\\\"content\\\"] += \\\" \\\" + sentence[\\\"content\\\"]\\n\\n    if current_segment[\\\"content\\\"]:\\n        segments.append(current_segment)\\n\\n    print(f\\\"Identified {len(segments)} segments.\\\")\\n    return segments\",\n      \"path\": \"/\"\n    },\n    {\n      \"filename\": \"utils.py\",\n      \"content\": \"import os\\nimport sys\\nimport time\\nimport json\\nimport progressbar\\nimport videogrep  # Make sure videogrep is installed\\nfrom deepgram import DeepgramClient, PrerecordedOptions, FileSource, DeepgramError\\nfrom groq import Groq\\n\\nparent_directory = os.path.abspath('..')\\n\\nsys.path.append(parent_directory)\\n\\n\\nfrom vts import (\\n    audio_processing,\\n    metadata_generation,\\n    segment_analysis,\\n    topic_modeling,\\n    utils,\\n)\\n\\n\\ndef create_project_folder(input_path, base_output_dir):\\n    base_name = os.path.splitext(os.path.basename(input_path))[0]\\n    timestamp = time.strftime(\\\"%Y%m%d_%H%M%S\\\")\\n    project_name = f\\\"{base_name}_{timestamp}\\\"\\n    project_path = os.path.join(base_output_dir, project_name)\\n    os.makedirs(project_path, exist_ok=True)\\n    return project_path\\n\\ndef transcribe_file_deepgram(client, file_path, options, max_retries=3, retry_delay=5):\\n    print(\\\"Transcribing audio using Deepgram...\\\")\\n    for attempt in range(max_retries):\\n        try:\\n            with open(file_path, \\\"rb\\\") as audio:\\n                buffer_data = audio.read()\\n                payload: FileSource = {\\\"buffer\\\": buffer_data, \\\"mimetype\\\": \\\"audio/mp4\\\"}\\n                response = client.listen.rest.v(\\\"1\\\").transcribe_file(payload, options)\\n            print(\\\"Transcription complete.\\\")\\n            return json.loads(response.to_json())\\n        except DeepgramError as e:\\n            if attempt < max_retries - 1:\\n                print(\\n                    f\\\"API call failed. Retrying in {retry_delay} seconds... (Attempt {attempt + 1}/{max_retries})\\\"\\n                )\\n                time.sleep(retry_delay)\\n            else:\\n                print(f\\\"Transcription failed after {max_retries} attempts: {str(e)}\\\")\\n                raise\\n        except Exception as e:\\n            print(f\\\"Unexpected error during transcription: {str(e)}\\\")\\n            raise\\n\\n\\ndef transcribe_file_groq(\\n    client, file_path, model=\\\"whisper-large-v3\\\", language=\\\"en\\\", prompt=None\\n):\\n    print(\\\"Transcribing audio using Groq...\\\")\\n    try:\\n        with open(file_path, \\\"rb\\\") as file:\\n            transcription = client.audio.transcriptions.create(\\n                file=(file_path, file.read()),\\n                model=model,\\n                prompt=prompt,\\n                response_format=\\\"verbose_json\\\",\\n                language=language,\\n                temperature=0.2\\n            )\\n        print(\\\"Transcription complete.\\\")\\n        return json.loads(transcription.text)\\n    except Exception as e:\\n        print(f\\\"Error during Groq transcription: {str(e)}\\\")\\n        raise\\n\\ndef handle_audio_video(video_path, project_path):\\n    audio_dir = os.path.join(project_path, \\\"audio\\\")\\n    os.makedirs(audio_dir, exist_ok=True)\\n\\n    normalized_video_path = os.path.join(project_path, \\\"normalized_video.mkv\\\")\\n    normalize_result = audio_processing.normalize_audio(video_path, normalized_video_path)\\n    if normalize_result[\\\"status\\\"] == \\\"error\\\":\\n        print(f\\\"Error during audio normalization: {normalize_result['message']}\\\")\\n        # Handle the error (e.g., exit or continue without normalization)\\n    else:\\n        print(normalize_result[\\\"message\\\"])\\n\\n    # Remove silence\\n    unsilenced_video_path = os.path.join(project_path, \\\"unsilenced_video.mkv\\\")\\n    silence_removal_result = audio_processing.remove_silence(\\n        normalized_video_path, unsilenced_video_path\\n    )\\n    if silence_removal_result[\\\"status\\\"] == \\\"error\\\":\\n        print(f\\\"Error during silence removal: {silence_removal_result['message']}\\\")\\n        # Handle the error (e.g., exit or continue without silence removal)\\n    else:\\n        print(silence_removal_result[\\\"message\\\"])\\n\\n    # Extract audio from unsilenced video\\n    raw_audio_path = os.path.join(audio_dir, \\\"extracted_audio.wav\\\")\\n    audio_processing.extract_audio(unsilenced_video_path, raw_audio_path)\\n\\n    # Convert to mono and resample for transcription\\n    mono_resampled_audio_path = os.path.join(audio_dir, \\\"mono_resampled_audio.m4a\\\")\\n    conversion_result = audio_processing.convert_to_mono_and_resample(\\n        raw_audio_path, mono_resampled_audio_path\\n    )\\n    if conversion_result[\\\"status\\\"] == \\\"error\\\":\\n        print(f\\\"Error during audio conversion: {conversion_result['message']}\\\")\\n        # Handle the error (e.g., exit or continue without conversion)\\n    else:\\n        print(conversion_result[\\\"message\\\"])\\n\\n    return unsilenced_video_path, mono_resampled_audio_path\\n\\n\\ndef handle_transcription(\\n    video_path, audio_path, project_path, api=\\\"deepgram\\\", num_topics=2, groq_prompt=None\\n):\\n    segments_dir = os.path.join(project_path, \\\"segments\\\")\\n    os.makedirs(segments_dir, exist_ok=True)\\n\\n    print(\\\"Parsing transcript with Videogrep...\\\")\\n    transcript = videogrep.parse_transcript(video_path)\\n    print(\\\"Transcript parsing complete.\\\")\\n\\n    if not transcript:\\n        print(\\\"No transcript found. Transcribing audio...\\\")\\n        deepgram_key = os.getenv(\\\"DG_API_KEY\\\")\\n        groq_key = os.getenv(\\\"GROQ_API_KEY\\\")\\n\\n        if not deepgram_key:\\n            raise ValueError(\\\"DG_API_KEY environment variable is not set\\\")\\n        if not groq_key and api == \\\"groq\\\":\\n            raise ValueError(\\\"GROQ_API_KEY environment variable is not set\\\")\\n\\n        if api == \\\"deepgram\\\":\\n            deepgram_client = DeepgramClient(deepgram_key)\\n            deepgram_options = PrerecordedOptions(\\n                model=\\\"nova-2\\\",\\n                language=\\\"en\\\",\\n                topics=True,\\n                intents=True,\\n                smart_format=True,\\n                punctuate=True,\\n                paragraphs=True,\\n                utterances=True,\\n                diarize=True,\\n                filler_words=True,\\n                sentiment=True,\\n            )\\n            # Transcribe the normalized audio\\n            transcription = transcribe_file_deepgram(\\n                deepgram_client, audio_path, deepgram_options\\n            )\\n            transcript = [\\n                {\\n                    \\\"content\\\": utterance[\\\"transcript\\\"],\\n                    \\\"start\\\": utterance[\\\"start\\\"],\\n                    \\\"end\\\": utterance[\\\"end\\\"],\\n                }\\n                for utterance in transcription[\\\"results\\\"][\\\"utterances\\\"]\\n            ]\\n        else:  # Groq\\n            groq_client = Groq(api_key=groq_key)\\n            # Transcribe the normalized audio\\n            transcription = transcribe_file_groq(\\n                groq_client, audio_path, prompt=groq_prompt\\n            )\\n            transcript = [\\n                {\\n                    \\\"content\\\": segment[\\\"text\\\"],\\n                    \\\"start\\\": segment[\\\"start\\\"],\\n                    \\\"end\\\": segment[\\\"end\\\"],\\n                }\\n                for segment in transcription[\\\"segments\\\"]\\n            ]\\n\\n    metadata_generation.save_transcription(transcription, project_path)\\n\\n    metadata_generation.save_transcript(transcript, project_path)\\n    results = process_transcript(transcript, project_path, num_topics)\\n\\n    # Split the video and analyze segments\\n    analyzed_segments = segment_analysis.split_and_analyze_video(\\n        video_path, results[\\\"segments\\\"], segments_dir\\n    )\\n\\n    # Update results with analyzed segments\\n    results[\\\"analyzed_segments\\\"] = analyzed_segments\\n\\n    # Save updated results\\n    results_path = os.path.join(project_path, \\\"results.json\\\")\\n    with open(results_path, \\\"w\\\") as f:\\n        json.dump(results, f, indent=2)\\n\\n    # print(\\\"Cleaning up temporary files...\\\")\\n    # os.remove(raw_audio_path)\\n    # os.remove(normalized_audio_path) # Uncomment to remove normalized audio\\n\\n    return results\\n\\n\\ndef process_video(\\n    video_path, project_path, api=\\\"deepgram\\\", num_topics=2, groq_prompt=None\\n):\\n\\n    unsilenced_video_path, mono_resampled_audio_path = handle_audio_video(\\n        video_path, project_path\\n    )\\n    results = handle_transcription(\\n        unsilenced_video_path,\\n        mono_resampled_audio_path,\\n        project_path,\\n        api,\\n        num_topics,\\n        groq_prompt,\\n    )\\n\\n    return results\\n\\n\\ndef process_transcript(transcript, project_path, num_topics=5):\\n    full_text = \\\" \\\".join([sentence[\\\"content\\\"] for sentence in transcript])\\n    preprocessed_subjects = topic_modeling.preprocess_text(full_text)\\n    lda_model, corpus, dictionary = topic_modeling.perform_topic_modeling(\\n        preprocessed_subjects, num_topics\\n    )\\n\\n    segments = topic_modeling.identify_segments(transcript, lda_model, dictionary, num_topics)\\n    metadata = metadata_generation.generate_metadata(segments, lda_model)\\n\\n    results = {\\n        \\\"topics\\\": [\\n            {\\n                \\\"topic_id\\\": topic_id,\\n                \\\"words\\\": [word for word, _ in lda_model.show_topic(topic_id, topn=10)],\\n            }\\n            for topic_id in range(num_topics)\\n        ],\\n        \\\"segments\\\": metadata,\\n    }\\n\\n    results_path = os.path.join(project_path, \\\"results.json\\\")\\n    with open(results_path, \\\"w\\\") as f:\\n        json.dump(results, f, indent=2)\\n    print(f\\\"Results saved to: {results_path}\\\")\\n\\n    return results\\n\",\n      \"path\": \"/\"\n    },\n    {\n      \"filename\": \"main.py\",\n      \"content\": \"#!/usr/bin/env python \\n\\nimport os\\nimport sys\\nimport argparse\\nimport json\\nfrom dotenv import load_dotenv\\nimport videogrep  # Make sure videogrep is installed\\nimport progressbar  # Make sure progressbar2 is installed\\n\\nparent_directory = os.path.abspath('..')\\n\\nsys.path.append(parent_directory)\\n\\n\\nfrom vts import (\\n    audio_processing,\\n    metadata_generation,\\n    segment_analysis,\\n    topic_modeling,\\n    utils,\\n)\\n\\n\\nload_dotenv()\\n\\n\\ndef main():\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Process video or transcript for topic-based segmentation and multi-modal analysis\\\"\\n    )\\n    parser.add_argument(\\n        \\\"-i\\\",\\n        \\\"--input\\\",\\n        required=True,\\n        help=\\\"Path to the input video file or transcript JSON\\\",\\n    )\\n    parser.add_argument(\\n        \\\"-o\\\",\\n        \\\"--output\\\",\\n        default=os.getcwd(),\\n        help=\\\"Base output directory for project folders\\\",\\n    )\\n    parser.add_argument(\\n        \\\"--api\\\",\\n        choices=[\\\"deepgram\\\", \\\"groq\\\"],\\n        default=\\\"deepgram\\\",\\n        help=\\\"Choose API: deepgram or groq\\\",\\n    )\\n    parser.add_argument(\\n        \\\"--topics\\\", type=int, default=5, help=\\\"Number of topics for LDA model\\\"\\n    )\\n    parser.add_argument(\\\"--groq-prompt\\\", help=\\\"Optional prompt for Groq transcription\\\")\\n    args = parser.parse_args()\\n\\n    project_path = utils.create_project_folder(args.input, args.output)\\n    print(f\\\"Created project folder: {project_path}\\\")\\n\\n    try:\\n        if args.input.endswith(\\\".json\\\"):\\n            transcript = load_transcript(args.input)\\n            results = utils.process_transcript(transcript, project_path, args.topics)\\n            print(\\n                \\\"Note: Video splitting and Gemini analysis are not performed when processing a transcript file.\\\"\\n            )\\n        else:\\n            results = utils.process_video(\\n                args.input, project_path, args.api, args.topics, args.groq_prompt\\n            )\\n\\n        print(f\\\"\\\\nProcessing complete. Project folder: {project_path}\\\")\\n        print(f\\\"Results saved in: {os.path.join(project_path, 'results.json')}\\\")\\n        print(\\\"\\\\nTop words for each topic:\\\")\\n        for topic in results[\\\"topics\\\"]:\\n            print(f\\\"Topic {topic['topic_id'] + 1}: {', '.join(topic['words'])}\\\")\\n        print(f\\\"\\\\nGenerated and analyzed {len(results['analyzed_segments'])} segments\\\")\\n\\n        if not args.input.endswith(\\\".json\\\"):\\n            print(f\\\"Video segments saved in: {os.path.join(project_path, 'segments')}\\\")\\n    except Exception as e:\\n        print(f\\\"\\\\nAn error occurred during processing: {str(e)}\\\")\\n        print(\\\"Please check the project folder for any partial results or logs.\\\")\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n\",\n      \"path\": \"/\"\n    },\n    {\n      \"filename\": \"__init__.py\",\n      \"content\": null,\n      \"path\": \"/\"\n    }\n  ]\n}\n"
    },
    {
      "filename": "__init__.py",
      "path": "/vtsV2/src/vts/",
      "content": "from vts.core.audio import AudioProcessor\nfrom vts.core.metadata import MetadataManager\nfrom vts.core.segmentation import VideoSegmenter\nfrom vts.core.transcription import TranscriptionManager\nfrom vts.analysis.topics import TopicAnalyzer\nfrom vts.analysis.video import VideoAnalyzer\nfrom vts.models import VideoProject, Segment, TranscriptionResult\nfrom vts.config import Settings\n\n__version__ = \"0.2.0\""
    },
    {
      "filename": "models.py",
      "path": "/vtsV2/src/vts/",
      "content": "from dataclasses import dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import List, Optional, Dict, Union\n\n@dataclass\nclass Segment:\n    \"\"\"Represents a video segment with associated metadata\"\"\"\n    id: int\n    start_time: float\n    end_time: float\n    content: str\n    topic_id: Optional[int] = None\n    keywords: Optional[List[str]] = None\n    analysis: Optional[str] = None\n\n    @property\n    def duration(self) -> float:\n        \"\"\"Get segment duration in seconds\"\"\"\n        return self.end_time - self.start_time\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Union[int, float, str]]) -> 'Segment':\n        \"\"\"Create a Segment instance from a dictionary\"\"\"\n        return cls(\n            id=data.get('id', 0),\n            start_time=float(data['start_time']),\n            end_time=float(data['end_time']),\n            content=data['content'],\n            topic_id=data.get('topic_id'),\n            keywords=data.get('keywords', []),\n            analysis=data.get('analysis')\n        )\n\n@dataclass\nclass TranscriptionResult:\n    \"\"\"Holds transcription data and metadata\"\"\"\n    segments: List[Dict]\n    raw_response: Dict\n    api_used: str\n    timestamp: datetime\n\n    def to_dict(self) -> Dict:\n        \"\"\"Convert to dictionary format\"\"\"\n        return {\n            \"segments\": self.segments,\n            \"raw_response\": self.raw_response,\n            \"api_used\": self.api_used,\n            \"timestamp\": self.timestamp.isoformat()\n        }\n\n@dataclass\nclass VideoProject:\n    \"\"\"Represents a video processing project\"\"\"\n    id: str\n    input_path: Path\n    output_dir: Path\n    created_at: datetime\n    segments: Optional[List[Segment]] = None\n    transcription: Optional[TranscriptionResult] = None\n    metadata: Optional[Dict] = None\n\n    @property\n    def project_dir(self) -> Path:\n        \"\"\"Get the project's working directory\"\"\"\n        return self.output_dir / self.id\n\n    @property\n    def segments_dir(self) -> Path:\n        \"\"\"Get the directory for video segments\"\"\"\n        return self.project_dir / \"segments\"\n\n    @property\n    def audio_dir(self) -> Path:\n        \"\"\"Get the directory for audio files\"\"\"\n        return self.project_dir / \"audio\"\n\n    def ensure_directories(self) -> None:\n        \"\"\"Create all necessary project directories\"\"\"\n        for directory in [self.project_dir, self.segments_dir, self.audio_dir]:\n            directory.mkdir(parents=True, exist_ok=True)\n\n    def get_segment_path(self, segment_id: int) -> Path:\n        \"\"\"Get the path for a specific segment's video file\"\"\"\n        return self.segments_dir / f\"segment_{segment_id}.mp4\"\n\n    def to_dict(self) -> Dict:\n        \"\"\"Convert project to dictionary format\"\"\"\n        return {\n            \"id\": self.id,\n            \"input_path\": str(self.input_path),\n            \"output_dir\": str(self.output_dir),\n            \"created_at\": self.created_at.isoformat(),\n            \"segments\": [vars(s) for s in (self.segments or [])],\n            \"transcription\": self.transcription.to_dict() if self.transcription else None,\n            \"metadata\": self.metadata\n        }"
    },
    {
      "filename": "config.py",
      "path": "/vtsV2/src/vts/",
      "content": "from pathlib import Path\nfrom typing import Optional\nfrom pydantic_settings import BaseSettings\nfrom pydantic import validator\n\nclass Settings(BaseSettings):\n    \"\"\"Application settings with environment variable support\"\"\"\n    \n    # API Keys\n    DEEPGRAM_API_KEY: Optional[str] = None\n    GROQ_API_KEY: Optional[str] = None\n    GEMINI_API_KEY: Optional[str] = None\n\n    # Audio Processing\n    SAMPLE_RATE: int = 16000\n    AUDIO_CHANNELS: int = 1\n    AUDIO_BITRATE: str = \"128k\"\n    HIGHPASS_FREQ: int = 200\n    LOWPASS_FREQ: int = 8000\n    SILENCE_THRESHOLD: float = -40.0\n    MIN_SILENCE_DURATION: float = 1.0\n    NORMALIZE_TARGET_LEVEL: float = -23.0\n\n    # Topic Modeling\n    DEFAULT_NUM_TOPICS: int = 5\n    MIN_TOPIC_COHERENCE: float = 0.3\n    MAX_TOPIC_ITERATIONS: int = 400\n    NUM_TOPIC_WORDS: int = 10\n\n    # Video Processing\n    MIN_SEGMENT_DURATION: float = 1.0\n    MAX_SEGMENT_DURATION: float = 300.0  # 5 minutes\n    VIDEO_CODEC: str = \"libx264\"\n    AUDIO_CODEC: str = \"aac\"\n    VIDEO_BITRATE: str = \"2M\"\n    \n    # File Paths\n    BASE_OUTPUT_DIR: Path = Path.cwd() / \"output\"\n    CACHE_DIR: Path = Path.cwd() / \"cache\"\n    \n    # Logging\n    LOG_LEVEL: str = \"INFO\"\n    LOG_FORMAT: str = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    \n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n\n    @validator(\"BASE_OUTPUT_DIR\", \"CACHE_DIR\")\n    def create_directories(cls, v: Path) -> Path:\n        \"\"\"Ensure directories exist\"\"\"\n        v.mkdir(parents=True, exist_ok=True)\n        return v\n\n    def get_api_key(self, api: str) -> Optional[str]:\n        \"\"\"Get API key by service name\"\"\"\n        api_keys = {\n            \"deepgram\": self.DEEPGRAM_API_KEY,\n            \"groq\": self.GROQ_API_KEY,\n            \"gemini\": self.GEMINI_API_KEY\n        }\n        return api_keys.get(api.lower())\n\n    def validate_api_keys(self, required_apis: list[str]) -> None:\n        \"\"\"Validate that required API keys are present\"\"\"\n        missing_keys = [\n            api for api in required_apis \n            if not self.get_api_key(api)\n        ]\n        if missing_keys:\n            raise ValueError(\n                f\"Missing required API keys for: {', '.join(missing_keys)}\"\n            )"
    },
    {
      "filename": "audio.py",
      "path": "/vtsV2/src/vts/core/",
      "content": "from pathlib import Path\nimport subprocess\nfrom typing import Dict, Union\nfrom pydub import AudioSegment\n\nfrom vts.config import Settings\nfrom vts.models import VideoProject\n\nclass AudioProcessor:\n    def __init__(self, settings: Settings):\n        self.settings = settings\n\n    def convert_to_mono(self, input_path: Path, output_path: Path) -> Dict[str, str]:\n        try:\n            command = [\n                \"ffmpeg\",\n                \"-i\", str(input_path),\n                \"-af\", f\"highpass=f={self.settings.HIGHPASS_FREQ}, acompressor=threshold=-12dB:ratio=4:attack=5:release=50, loudnorm\",\n                \"-ar\", str(self.settings.SAMPLE_RATE),\n                \"-ac\", str(self.settings.AUDIO_CHANNELS),\n                \"-c:a\", \"aac\",\n                \"-b:a\", self.settings.AUDIO_BITRATE,\n                str(output_path)\n            ]\n            subprocess.run(command, check=True, capture_output=True)\n            return {\"status\": \"success\", \"message\": f\"Audio converted successfully\"}\n        except subprocess.CalledProcessError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n\n    def normalize_audio(self, input_path: Path, output_path: Path) -> Dict[str, str]:\n        try:\n            command = [\n                \"ffmpeg-normalize\",\n                \"-pr\",\n                \"-tp\", \"-3.0\",\n                \"-nt\", \"rms\",\n                str(input_path),\n                \"-prf\", f\"highpass=f={self.settings.HIGHPASS_FREQ}, loudnorm\",\n                \"-prf\", \"dynaudnorm=p=0.4:s=15\",\n                \"-pof\", f\"lowpass=f={self.settings.LOWPASS_FREQ}\",\n                \"-ar\", str(self.settings.SAMPLE_RATE),\n                \"-c:a\", \"pcm_s16le\",\n                \"--keep-loudness-range-target\",\n                \"-o\", str(output_path)\n            ]\n            subprocess.run(command, check=True, capture_output=True)\n            return {\"status\": \"success\", \"message\": \"Audio normalized successfully\"}\n        except subprocess.CalledProcessError as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n\n    def extract_audio(self, video_path: Path, output_path: Path) -> None:\n        audio = AudioSegment.from_file(str(video_path))\n        audio.export(str(output_path), format=\"wav\")"
    },
    {
      "filename": "transcription.py",
      "path": "/vtsV2/src/vts/core/",
      "content": "from pathlib import Path\nfrom typing import Dict, Optional\nfrom datetime import datetime\nfrom deepgram import DeepgramClient, PrerecordedOptions\nfrom groq import Groq\n\nfrom vts.config import Settings\nfrom vts.models import TranscriptionResult\n\nclass TranscriptionManager:\n    def __init__(self, settings: Settings):\n        self.settings = settings\n        self.deepgram = DeepgramClient(settings.DEEPGRAM_API_KEY) if settings.DEEPGRAM_API_KEY else None\n        self.groq = Groq(api_key=settings.GROQ_API_KEY) if settings.GROQ_API_KEY else None\n\n    def transcribe_with_deepgram(self, audio_path: Path) -> TranscriptionResult:\n        if not self.deepgram:\n            raise ValueError(\"Deepgram API key not configured\")\n\n        options = PrerecordedOptions(\n            model=\"nova-2\",\n            language=\"en\",\n            topics=True,\n            smart_format=True,\n            punctuate=True,\n            paragraphs=True,\n            utterances=True\n        )\n\n        with open(audio_path, \"rb\") as audio:\n            response = self.deepgram.listen.rest.v(\"1\").transcribe_file(\n                {\"buffer\": audio.read(), \"mimetype\": \"audio/wav\"},\n                options\n            )\n        \n        result = response.to_dict()\n        segments = [\n            {\n                \"content\": u[\"transcript\"],\n                \"start\": u[\"start\"],\n                \"end\": u[\"end\"]\n            }\n            for u in result[\"results\"][\"utterances\"]\n        ]\n\n        return TranscriptionResult(\n            segments=segments,\n            raw_response=result,\n            timestamp=datetime.now()\n        )\n\n    def transcribe_with_groq(\n        self, \n        audio_path: Path, \n        prompt: Optional[str] = None\n    ) -> TranscriptionResult:\n        if not self.groq:\n            raise ValueError(\"Groq API key not configured\")\n\n        with open(audio_path, \"rb\") as audio:\n            response = self.groq.audio.transcriptions.create(\n                file=audio,\n                model=\"whisper-large-v3\",\n                prompt=prompt,\n                response_format=\"verbose_json\",\n                language=\"en\"\n            )\n        \n        result = response.to_dict()\n        segments = [\n            {\n                \"content\": s[\"text\"],\n                \"start\": s[\"start\"],\n                \"end\": s[\"end\"]\n            }\n            for s in result[\"segments\"]\n        ]\n\n        return TranscriptionResult(\n            segments=segments,\n            raw_response=result,\n            timestamp=datetime.now()\n        )"
    },
    {
      "filename": "reporting.py",
      "path": "/vtsV2/src/vts/core/",
      "content": "from pathlib import Path\nfrom datetime import datetime\nfrom typing import List, Dict\nimport json\nfrom jinja2 import Environment, PackageLoader, select_autoescape\n\nfrom vts.models import AnalysisReport, Topic, Segment\n\nclass ReportGenerator:\n    def __init__(self):\n        self.env = Environment(\n            loader=PackageLoader('vts', 'templates'),\n            autoescape=select_autoescape(['html', 'xml'])\n        )\n        \n    def generate_markdown(self, report: AnalysisReport) -> str:\n        template = self.env.get_template('report.md')\n        return template.render(report=report)\n    \n    def save_report(self, report: AnalysisReport, output_dir: Path) -> Path:\n        markdown = self.generate_markdown(report)\n        output_path = output_dir / 'analysis_report.md'\n        output_path.write_text(markdown)\n        return output_path"
    },
    {
      "filename": "metadata.py",
      "path": "/vtsV2/src/vts/core/",
      "content": "from dataclasses import asdict\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nimport json\nimport logging\n\nfrom vts.models import Segment, Topic, VideoProject\nfrom vts.config import Settings\n\nlogger = logging.getLogger(__name__)\n\nclass MetadataManager:\n    \"\"\"Manages metadata generation and handling for video analysis\"\"\"\n    \n    def __init__(self, settings: Settings):\n        self.settings = settings\n        \n    def create_segment_metadata(\n        self,\n        segment: Segment,\n        analysis_results: Optional[Dict] = None\n    ) -> Dict:\n        \"\"\"Create metadata for a single segment\"\"\"\n        metadata = {\n            \"id\": segment.id,\n            \"timestamp\": {\n                \"start\": segment.start_time,\n                \"end\": segment.end_time,\n                \"duration\": str(segment.duration)\n            },\n            \"content\": {\n                \"transcript\": segment.content,\n                \"topic_id\": segment.topic_id,\n                \"keywords\": segment.keywords\n            },\n            \"analysis\": {\n                \"confidence\": analysis_results.get(\"confidence\", 0.0) if analysis_results else 0.0,\n                \"visual_description\": segment.analysis if segment.analysis else \"\",\n                \"entities\": analysis_results.get(\"entities\", []) if analysis_results else []\n            }\n        }\n        \n        return metadata\n    \n    def create_topic_metadata(self, topic: Topic) -> Dict:\n        \"\"\"Create metadata for a single topic\"\"\"\n        return {\n            \"id\": topic.id,\n            \"keywords\": topic.keywords,\n            \"weight\": topic.weight,\n            \"segment_count\": len(topic.segments) if topic.segments else 0,\n            \"total_duration\": str(sum(\n                (s.duration for s in topic.segments),\n                start=datetime.timedelta(0)\n            )) if topic.segments else \"0:00:00\"\n        }\n    \n    def create_project_metadata(self, project: VideoProject) -> Dict:\n        \"\"\"Create comprehensive project metadata\"\"\"\n        return {\n            \"project\": {\n                \"id\": project.id,\n                \"created_at\": project.created_at.isoformat(),\n                \"input_file\": str(project.input_path),\n                \"output_directory\": str(project.output_dir)\n            },\n            \"analysis\": {\n                \"total_segments\": len(project.segments) if project.segments else 0,\n                \"total_topics\": project.metadata.get(\"topic_count\", 0) if project.metadata else 0,\n                \"transcription_api\": project.metadata.get(\"transcription_api\", \"unknown\") if project.metadata else \"unknown\",\n                \"processing_time\": project.metadata.get(\"processing_time\", \"unknown\") if project.metadata else \"unknown\"\n            },\n            \"settings\": {\n                \"sample_rate\": self.settings.SAMPLE_RATE,\n                \"num_topics\": self.settings.DEFAULT_NUM_TOPICS,\n                \"min_topic_coherence\": self.settings.MIN_TOPIC_COHERENCE\n            }\n        }\n        \n    def save_metadata(self, metadata: Dict, output_path: Path) -> None:\n        \"\"\"Save metadata to JSON file\"\"\"\n        try:\n            with open(output_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2, ensure_ascii=False)\n            logger.info(f\"Metadata saved to: {output_path}\")\n        except Exception as e:\n            logger.error(f\"Error saving metadata: {str(e)}\")\n            raise\n            \n    def load_metadata(self, metadata_path: Path) -> Dict:\n        \"\"\"Load metadata from JSON file\"\"\"\n        try:\n            with open(metadata_path, 'r', encoding='utf-8') as f:\n                return json.load(f)\n        except Exception as e:\n            logger.error(f\"Error loading metadata: {str(e)}\")\n            raise"
    },
    {
      "filename": "segmentation.py",
      "path": "/vtsV2/src/vts/core/",
      "content": "from pathlib import Path\nimport logging\nfrom typing import List, Dict, Optional, Tuple\nfrom datetime import datetime, timedelta\nimport numpy as np\nfrom moviepy.editor import VideoFileClip\nfrom pydub import AudioSegment\nimport librosa\n\nfrom vts.models import Segment, VideoProject\nfrom vts.config import Settings\n\nlogger = logging.getLogger(__name__)\n\nclass VideoSegmenter:\n    \"\"\"Handles video segmentation based on topic changes and audio analysis\"\"\"\n    \n    def __init__(self, settings: Settings):\n        self.settings = settings\n        \n    def detect_scene_changes(\n        self,\n        video_path: Path,\n        threshold: float = 0.3,\n        min_scene_duration: float = 2.0\n    ) -> List[float]:\n        \"\"\"Detect major visual scene changes in the video\"\"\"\n        logger.info(\"Detecting scene changes...\")\n        \n        try:\n            video = VideoFileClip(str(video_path))\n            fps = video.fps\n            duration = video.duration\n            \n            # Sample frames at regular intervals\n            frame_times = np.arange(0, duration, 1/fps)\n            scenes = []\n            last_frame = None\n            last_scene_time = 0\n            \n            for time in frame_times:\n                frame = video.get_frame(time)\n                \n                if last_frame is not None:\n                    # Calculate frame difference\n                    diff = np.mean(np.abs(frame - last_frame))\n                    \n                    # If difference exceeds threshold and minimum duration has passed\n                    if diff > threshold and (time - last_scene_time) >= min_scene_duration:\n                        scenes.append(time)\n                        last_scene_time = time\n                        \n                last_frame = frame\n                \n            video.close()\n            return scenes\n            \n        except Exception as e:\n            logger.error(f\"Error detecting scene changes: {str(e)}\")\n            raise\n            \n    def detect_audio_segments(\n        self,\n        audio_path: Path,\n        min_silence_duration: float = 1.0,\n        silence_threshold: float = -40\n    ) -> List[Tuple[float, float]]:\n        \"\"\"Detect segments based on audio characteristics\"\"\"\n        logger.info(\"Detecting audio segments...\")\n        \n        try:\n            # Load audio file\n            audio = AudioSegment.from_file(str(audio_path))\n            \n            # Convert to numpy array for processing\n            samples = np.array(audio.get_array_of_samples())\n            sample_rate = audio.frame_rate\n            \n            # Calculate energy\n            frame_length = int(sample_rate * 0.025)  # 25ms frames\n            hop_length = int(sample_rate * 0.010)    # 10ms hop\n            \n            rms = librosa.feature.rms(\n                y=samples.astype(float),\n                frame_length=frame_length,\n                hop_length=hop_length\n            )[0]\n            \n            # Find silence boundaries\n            is_silence = rms < (10 ** (silence_threshold / 20))\n            \n            # Group continuous silence frames\n            silence_regions = []\n            start = None\n            \n            for i, silent in enumerate(is_silence):\n                time = i * hop_length / sample_rate\n                \n                if silent and start is None:\n                    start = time\n                elif not silent and start is not None:\n                    if (time - start) >= min_silence_duration:\n                        silence_regions.append((start, time))\n                    start = None\n                    \n            return silence_regions\n            \n        except Exception as e:\n            logger.error(f\"Error detecting audio segments: {str(e)}\")\n            raise\n            \n    def create_segments(\n        self,\n        project: VideoProject,\n        scene_changes: List[float],\n        silence_regions: List[Tuple[float, float]],\n        transcription_segments: List[Dict]\n    ) -> List[Segment]:\n        \"\"\"Create final segments by combining visual, audio, and transcription data\"\"\"\n        logger.info(\"Creating final segments...\")\n        \n        # Combine all potential segment boundaries\n        boundaries = set()\n        boundaries.add(0)  # Start of video\n        \n        # Add scene changes\n        boundaries.update(scene_changes)\n        \n        # Add silence boundaries\n        for start, end in silence_regions:\n            boundaries.add(start)\n            boundaries.add(end)\n            \n        # Add transcription segment boundaries\n        for seg in transcription_segments:\n            boundaries.add(seg[\"start\"])\n            boundaries.add(seg[\"end\"])\n            \n        # Sort boundaries\n        boundaries = sorted(list(boundaries))\n        \n        # Create segments\n        segments = []\n        for i in range(len(boundaries) - 1):\n            start_time = boundaries[i]\n            end_time = boundaries[i + 1]\n            \n            # Find transcription content for this segment\n            content = []\n            for seg in transcription_segments:\n                if (seg[\"start\"] >= start_time and seg[\"start\"] < end_time) or \\\n                   (seg[\"end\"] > start_time and seg[\"end\"] <= end_time):\n                    content.append(seg[\"content\"])\n                    \n            # Create segment only if it has content\n            if content:\n                segment = Segment(\n                    id=len(segments) + 1,\n                    start_time=start_time,\n                    end_time=end_time,\n                    content=\" \".join(content)\n                )\n                segments.append(segment)\n                \n        return segments\n        \n    def split_video(\n        self,\n        project: VideoProject,\n        segments: List[Segment],\n        output_dir: Path\n    ) -> None:\n        \"\"\"Split video into segment files\"\"\"\n        logger.info(\"Splitting video into segments...\")\n        \n        try:\n            video = VideoFileClip(str(project.input_path))\n            \n            for segment in segments:\n                output_path = output_dir / f\"segment_{segment.id}.mp4\"\n                \n                # Extract segment\n                clip = video.subclip(segment.start_time, segment.end_time)\n                \n                # Write segment\n                clip.write_videofile(\n                    str(output_path),\n                    codec=\"libx264\",\n                    audio_codec=\"aac\",\n                    logger=None  # Suppress moviepy logging\n                )\n                \n            video.close()\n            logger.info(\"Video splitting complete\")\n            \n        except Exception as e:\n            logger.error(f\"Error splitting video: {str(e)}\")\n            raise\n            \n    def process_video(\n        self,\n        project: VideoProject,\n        transcription_segments: List[Dict],\n        output_dir: Optional[Path] = None\n    ) -> List[Segment]:\n        \"\"\"Main method to process video and create segments\"\"\"\n        try:\n            # Detect scene changes\n            scene_changes = self.detect_scene_changes(project.input_path)\n            \n            # Detect audio segments\n            audio_path = project.project_dir / \"audio\" / \"processed_audio.m4a\"\n            silence_regions = self.detect_audio_segments(audio_path)\n            \n            # Create segments\n            segments = self.create_segments(\n                project,\n                scene_changes,\n                silence_regions,\n                transcription_segments\n            )\n            \n            # Split video if output directory is provided\n            if output_dir:\n                self.split_video(project, segments, output_dir)\n                \n            return segments\n            \n        except Exception as e:\n            logger.error(f\"Error processing video: {str(e)}\")\n            raise"
    },
    {
      "filename": "processor.py",
      "path": "/vtsV2/src/vts/core/",
      "content": "from pathlib import Path\nimport logging\nfrom typing import Dict, List, Optional, Tuple\nfrom datetime import datetime\nimport json\nimport videogrep\nfrom deepgram import DeepgramClient, PrerecordedOptions\nfrom groq import Groq\nimport google.generativeai as genai\nfrom moviepy.editor import VideoFileClip\n\nfrom vts.core.audio import AudioProcessor\nfrom vts.analysis.topics import TopicAnalyzer\nfrom vts.models import VideoProject, Segment\nfrom vts.config import Settings\n\nlogger = logging.getLogger(__name__)\n\nclass VideoProcessor:\n    \"\"\"Main processor class that orchestrates the video analysis pipeline\"\"\"\n    \n    def __init__(self, settings: Settings):\n        self.settings = settings\n        self.audio_processor = AudioProcessor(settings)\n        self.topic_analyzer = TopicAnalyzer(settings)\n        \n        # Initialize API clients\n        self.deepgram = (DeepgramClient(settings.DEEPGRAM_API_KEY) \n                        if settings.DEEPGRAM_API_KEY else None)\n        self.groq = (Groq(api_key=settings.GROQ_API_KEY)\n                    if settings.GROQ_API_KEY else None)\n        \n        if settings.GEMINI_API_KEY:\n            genai.configure(api_key=settings.GEMINI_API_KEY)\n            self.gemini = genai.GenerativeModel(\"gemini-1.5-flash\")\n        else:\n            self.gemini = None\n\n    def process_video(\n        self, \n        input_path: Path,\n        output_dir: Path,\n        api: str = \"deepgram\",\n        num_topics: int = 5,\n        groq_prompt: Optional[str] = None\n    ) -> Dict:\n        \"\"\"Main processing pipeline\"\"\"\n        logger.info(f\"Processing video: {input_path}\")\n        \n        # Create project\n        project = self._create_project(input_path, output_dir)\n        \n        try:\n            # Process audio\n            audio_path = self._process_audio(project)\n            \n            # Get transcript\n            transcript = self._get_transcript(project, audio_path, api, groq_prompt)\n            \n            # Perform topic modeling and segmentation\n            results = self._analyze_content(project, transcript, num_topics)\n            \n            # Save results\n            self._save_results(project, results)\n            \n            return results\n            \n        except Exception as e:\n            logger.error(f\"Error processing video: {str(e)}\")\n            raise\n\n    def _create_project(self, input_path: Path, output_dir: Path) -> VideoProject:\n        \"\"\"Create and setup project directory structure\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        project_id = f\"{input_path.stem}_{timestamp}\"\n        \n        project = VideoProject(\n            id=project_id,\n            input_path=input_path,\n            output_dir=output_dir,\n            created_at=datetime.now()\n        )\n        \n        # Create project directories\n        project.project_dir.mkdir(parents=True, exist_ok=True)\n        (project.project_dir / \"audio\").mkdir(exist_ok=True)\n        (project.project_dir / \"segments\").mkdir(exist_ok=True)\n        \n        return project\n\n    def _process_audio(self, project: VideoProject) -> Path:\n        \"\"\"Process audio from video file\"\"\"\n        logger.info(\"Processing audio...\")\n        \n        audio_dir = project.project_dir / \"audio\"\n        \n        # Normalize audio\n        normalized_path = audio_dir / \"normalized_audio.wav\"\n        self.audio_processor.normalize_audio(project.input_path, normalized_path)\n        \n        # Remove silence\n        unsilenced_path = audio_dir / \"unsilenced_audio.wav\"\n        self.audio_processor.remove_silence(normalized_path, unsilenced_path)\n        \n        # Convert to mono and resample\n        processed_path = audio_dir / \"processed_audio.m4a\"\n        self.audio_processor.convert_to_mono(unsilenced_path, processed_path)\n        \n        return processed_path\n\n    def _get_transcript(\n        self,\n        project: VideoProject,\n        audio_path: Path,\n        api: str,\n        groq_prompt: Optional[str]\n    ) -> List[Dict]:\n        \"\"\"Get transcript either from video or through transcription\"\"\"\n        logger.info(\"Getting transcript...\")\n        \n        # Try to parse embedded transcript\n        transcript = videogrep.parse_transcript(str(project.input_path))\n        \n        if not transcript:\n            logger.info(\"No embedded transcript found. Transcribing audio...\")\n            \n            if api == \"deepgram\" and self.deepgram:\n                transcription = self._transcribe_with_deepgram(audio_path)\n                transcript = [\n                    {\n                        \"content\": u[\"transcript\"],\n                        \"start\": u[\"start\"],\n                        \"end\": u[\"end\"]\n                    }\n                    for u in transcription[\"results\"][\"utterances\"]\n                ]\n                \n            elif api == \"groq\" and self.groq:\n                transcription = self._transcribe_with_groq(audio_path, groq_prompt)\n                transcript = [\n                    {\n                        \"content\": s[\"text\"],\n                        \"start\": s[\"start\"],\n                        \"end\": s[\"end\"]\n                    }\n                    for s in transcription[\"segments\"]\n                ]\n                \n            else:\n                raise ValueError(f\"Invalid or unconfigured API: {api}\")\n                \n            # Save raw transcription\n            with open(project.project_dir / \"transcription.json\", 'w') as f:\n                json.dump(transcription, f, indent=2)\n        \n        # Save formatted transcript\n        with open(project.project_dir / \"transcript.json\", 'w') as f:\n            json.dump(transcript, f, indent=2)\n            \n        return transcript\n\n    def _transcribe_with_deepgram(self, audio_path: Path) -> Dict:\n        \"\"\"Transcribe using Deepgram\"\"\"\n        options = PrerecordedOptions(\n            model=\"nova-2\",\n            language=\"en\",\n            topics=True,\n            smart_format=True,\n            punctuate=True,\n            paragraphs=True,\n            utterances=True\n        )\n        \n        with open(audio_path, 'rb') as audio:\n            response = self.deepgram.listen.rest.v1.transcribe_file(\n                {\"buffer\": audio.read(), \"mimetype\": \"audio/m4a\"},\n                options\n            )\n        return response.to_dict()\n\n    def _transcribe_with_groq(self, audio_path: Path, prompt: Optional[str]) -> Dict:\n        \"\"\"Transcribe using Groq\"\"\"\n        with open(audio_path, 'rb') as audio:\n            response = self.groq.audio.transcriptions.create(\n                file=audio,\n                model=\"whisper-large-v3\",\n                prompt=prompt,\n                response_format=\"verbose_json\",\n                language=\"en\"\n            )\n        return json.loads(response.text)\n\n    def _analyze_content(\n        self,\n        project: VideoProject,\n        transcript: List[Dict],\n        num_topics: int\n    ) -> Dict:\n        \"\"\"Analyze content using topic modeling and generate segments\"\"\"\n        logger.info(\"Analyzing content...\")\n        \n        # Combine transcript text\n        full_text = \" \".join(seg[\"content\"] for seg in transcript)\n        \n        # Build topic model\n        subjects = self.topic_analyzer.preprocess_text(full_text)\n        model, dictionary = self.topic_analyzer.build_topic_model(subjects)\n        \n        # Create segments based on topics\n        segments = []\n        for i, seg in enumerate(transcript):\n            topic_id, confidence = self.topic_analyzer.get_dominant_topic(\n                seg[\"content\"], model, dictionary\n            )\n            segments.append(Segment(\n                id=i + 1,\n                start_time=float(seg[\"start\"]),\n                end_time=float(seg[\"end\"]),\n                content=seg[\"content\"],\n                topic_id=topic_id\n            ))\n        \n        # Split video and analyze segments\n        analyzed_segments = self._analyze_segments(project, segments)\n        \n        # Generate results\n        return {\n            \"project_id\": project.id,\n            \"topics\": [\n                {\n                    \"topic_id\": tid,\n                    \"words\": [word for word, _ in model.show_topic(tid, topn=10)]\n                }\n                for tid in range(num_topics)\n            ],\n            \"segments\": analyzed_segments\n        }\n\n    def _analyze_segments(\n        self,\n        project: VideoProject,\n        segments: List[Segment]\n    ) -> List[Dict]:\n        \"\"\"Split video into segments and analyze each one\"\"\"\n        logger.info(\"Analyzing segments...\")\n        \n        video = VideoFileClip(str(project.input_path))\n        analyzed_segments = []\n        \n        for segment in segments:\n            try:\n                # Extract segment video\n                output_path = project.project_dir / \"segments\" / f\"segment_{segment.id}.mp4\"\n                clip = video.subclip(segment.start_time, segment.end_time)\n                clip.write_videofile(\n                    str(output_path),\n                    codec=\"libx264\",\n                    audio_codec=\"aac\",\n                    logger=None\n                )\n                \n                # Analyze with Gemini if available\n                analysis = None\n                if self.gemini:\n                    frame = clip.get_frame(0)\n                    response = self.gemini.generate_content([\n                        f\"Analyze this video segment. The transcript is: '{segment.content}'.\",\n                        Image.fromarray(frame)\n                    ])\n                    analysis = response.text\n                \n                analyzed_segments.append({\n                    \"segment_id\": segment.id,\n                    \"start_time\": segment.start_time,\n                    \"end_time\": segment.end_time,\n                    \"transcript\": segment.content,\n                    \"topic\": segment.topic_id,\n                    \"gemini_analysis\": analysis\n                })\n                \n            except Exception as e:\n                logger.error(f\"Error processing segment {segment.id}: {str(e)}\")\n                continue\n        \n        video.close()\n        return analyzed_segments\n\n    def _save_results(self, project: VideoProject, results: Dict) -> None:\n        \"\"\"Save analysis results\"\"\"\n        results_path = project.project_dir / \"results.json\"\n        with open(results_path, 'w') as f:\n            json.dump(results, f, indent=2)\n        logger.info(f\"Results saved to: {results_path}\")"
    },
    {
      "filename": "topics.py",
      "path": "/vtsV2/src/vts/analysis/",
      "content": "import spacy\nfrom gensim import corpora\nfrom gensim.models import LdaMulticore\nfrom typing import List, Tuple, Dict, Optional\nimport logging\nfrom pathlib import Path\n\nfrom vts.config import Settings\nfrom vts.models import Topic, Segment\n\nlogger = logging.getLogger(__name__)\n\nclass TopicAnalyzer:\n    def __init__(self, settings: Settings):\n        self.settings = settings\n        self.nlp = spacy.load(\"en_core_web_sm\")\n\n    def preprocess_text(self, text: str) -> List[List[str]]:\n        doc = self.nlp(text)\n        subjects = []\n\n        for sent in doc.sents:\n            for token in sent:\n                if \"subj\" in token.dep_:\n                    subject = self._get_compound_subject(token)\n                    subjects.append(subject)\n\n        return [\n            [token.lemma_.lower() for token in self.nlp(subject)\n             if not token.is_stop and not token.is_punct and token.is_alpha]\n            for subject in subjects\n        ]\n\n    def _get_compound_subject(self, token) -> str:\n        subject = [token.text]\n        for left_token in token.lefts:\n            if left_token.dep_ == \"compound\":\n                subject.insert(0, left_token.text)\n        for right_token in token.rights:\n            if right_token.dep_ == \"compound\":\n                subject.append(right_token.text)\n        return \" \".join(subject)\n\n    def build_topic_model(\n        self, \n        subjects: List[List[str]]\n    ) -> Tuple[LdaMulticore, corpora.Dictionary]:\n        dictionary = corpora.Dictionary(subjects)\n        corpus = [dictionary.doc2bow(subject) for subject in subjects]\n\n        model = LdaMulticore(\n            corpus=corpus,\n            id2word=dictionary,\n            num_topics=self.settings.DEFAULT_NUM_TOPICS,\n            random_state=100,\n            chunksize=100,\n            passes=10,\n            per_word_topics=True\n        )\n\n        return model, dictionary\n\n    def get_dominant_topic(\n        self, \n        text: str, \n        model: LdaMulticore, \n        dictionary: corpora.Dictionary\n    ) -> Tuple[Optional[int], float]:\n        subjects = self.preprocess_text(text)\n        if not subjects:\n            return None, 0.0\n\n        bow = dictionary.doc2bow([token for subject in subjects for token in subject])\n        topic_dist = model.get_document_topics(bow)\n        \n        if not topic_dist:\n            return None, 0.0\n            \n        return max(topic_dist, key=lambda x: x[1])\n\n    def generate_topic_summary(\n        self, \n        model: LdaMulticore, \n        segments: List[Segment]\n    ) -> List[Topic]:\n        topics = []\n        for topic_id in range(model.num_topics):\n            keywords = [word for word, _ in model.show_topic(topic_id, topn=10)]\n            topic_segments = [s for s in segments if s.topic_id == topic_id]\n            weight = len(topic_segments) / len(segments) if segments else 0.0\n            \n            topics.append(Topic(\n                id=topic_id,\n                keywords=keywords,\n                weight=weight,\n                segments=topic_segments\n            ))\n    \n        return topics\n\n    def analyze_coherence(\n        self, \n        model: LdaMulticore, \n        dictionary: corpora.Dictionary, \n        texts: List[List[str]]\n    ) -> float:\n        \"\"\"Analyze topic model coherence\"\"\"\n        from gensim.models.coherencemodel import CoherenceModel\n        \n        coherence_model = CoherenceModel(\n            model=model,\n            texts=texts,\n            dictionary=dictionary,\n            coherence='c_v'\n        )\n        return coherence_model.get_coherence()"
    },
    {
      "filename": "video.py",
      "path": "/vtsV2/src/vts/analysis/",
      "content": "from pathlib import Path\nimport logging\nfrom typing import List, Dict, Union, Optional\nimport google.generativeai as genai\nfrom PIL import Image\nfrom moviepy.editor import VideoFileClip\n\nfrom vts.config import Settings\nfrom vts.models import Segment, VideoProject\n\nlogger = logging.getLogger(__name__)\n\nclass VideoAnalyzer:\n    def __init__(self, settings: Settings):\n        self.settings = settings\n        if settings.GEMINI_API_KEY:\n            genai.configure(api_key=settings.GEMINI_API_KEY)\n            self.model = genai.GenerativeModel(\"gemini-1.5-flash\")\n        else:\n            logger.warning(\"No GEMINI_API_KEY provided. Visual analysis will be disabled.\")\n            self.model = None\n\n    def split_video(\n        self, \n        project: VideoProject,\n        segments: Union[List[Segment], List[Dict]]\n    ) -> List[Dict]:\n        \"\"\"Split video into segments and analyze each one\"\"\"\n        logger.info(f\"Splitting video into {len(segments)} segments...\")\n        \n        # Convert dictionaries to Segment objects if necessary\n        if segments and isinstance(segments[0], dict):\n            segments = [\n                Segment(\n                    id=s.get(\"id\", i+1),\n                    start_time=float(s[\"start_time\"]),\n                    end_time=float(s[\"end_time\"]),\n                    content=s[\"content\"],\n                    topic_id=s.get(\"topic_id\"),\n                    keywords=s.get(\"keywords\", [])\n                )\n                for i, s in enumerate(segments)\n            ]\n        \n        try:\n            video = VideoFileClip(str(project.input_path))\n            analyzed_segments = []\n\n            for segment in segments:\n                try:\n                    # Create the segment video\n                    logger.debug(f\"Processing segment {segment.id} ({segment.start_time} - {segment.end_time})\")\n                    clip = video.subclip(segment.start_time, segment.end_time)\n                    output_path = project.project_dir / \"segments\" / f\"segment_{segment.id}.mp4\"\n                    \n                    # Ensure parent directory exists\n                    output_path.parent.mkdir(parents=True, exist_ok=True)\n                    \n                    clip.write_videofile(\n                        str(output_path), \n                        codec=\"libx264\", \n                        audio_codec=\"aac\",\n                        logger=None  # Suppress moviepy output\n                    )\n                    \n                    # Analyze the segment if Gemini is available\n                    analysis = None\n                    if self.model:\n                        analysis = self.analyze_segment(segment, output_path)\n                    \n                    # Add to results\n                    analyzed_segments.append({\n                        \"segment_id\": segment.id,\n                        \"start_time\": segment.start_time,\n                        \"end_time\": segment.end_time,\n                        \"transcript\": segment.content,\n                        \"topic\": segment.topic_id,\n                        \"keywords\": segment.keywords,\n                        \"gemini_analysis\": analysis\n                    })\n                    \n                except Exception as e:\n                    logger.error(f\"Error processing segment {segment.id}: {str(e)}\")\n                    # Continue with next segment instead of failing completely\n                    continue\n\n            video.close()\n            return analyzed_segments\n            \n        except Exception as e:\n            logger.error(f\"Error processing video: {str(e)}\")\n            raise\n\n    def analyze_segment(self, segment: Segment, video_path: Path) -> Optional[str]:\n        \"\"\"Analyze a single video segment using Gemini\"\"\"\n        if not self.model:\n            return None\n            \n        try:\n            video = VideoFileClip(str(video_path))\n            frame = video.get_frame(0)  # Get first frame\n            image = Image.fromarray(frame)\n            video.close()\n\n            prompt = (\n                f\"Analyze this video segment. The transcript is: '{segment.content}'. \"\n                f\"Describe the main subject matter, key visual elements, and how they \"\n                f\"relate to the transcript.\"\n            )\n\n            response = self.model.generate_content([prompt, image])\n            return response.text\n            \n        except Exception as e:\n            logger.error(f\"Error analyzing segment {segment.id}: {str(e)}\")\n            return None\n\n    def get_segment_thumbnail(self, video_path: Path, time: float) -> Optional[Image.Image]:\n        \"\"\"Extract thumbnail from video at specified time\"\"\"\n        try:\n            video = VideoFileClip(str(video_path))\n            frame = video.get_frame(time)\n            video.close()\n            return Image.fromarray(frame)\n        except Exception as e:\n            logger.error(f\"Error extracting thumbnail: {str(e)}\")\n            return None\n\n    def process_segment(\n        self,\n        segment: Segment,\n        input_video: Path,\n        output_path: Path,\n        analyze: bool = True\n    ) -> Dict:\n        \"\"\"Process a single segment with all steps\"\"\"\n        try:\n            # Extract segment video\n            video = VideoFileClip(str(input_video))\n            clip = video.subclip(segment.start_time, segment.end_time)\n            clip.write_videofile(\n                str(output_path),\n                codec=\"libx264\",\n                audio_codec=\"aac\",\n                logger=None\n            )\n            video.close()\n\n            # Get thumbnail\n            thumbnail = self.get_segment_thumbnail(output_path, 0)\n            \n            # Analyze if requested\n            analysis = None\n            if analyze and self.model and thumbnail:\n                analysis = self.analyze_segment(segment, output_path)\n\n            return {\n                \"segment_id\": segment.id,\n                \"start_time\": segment.start_time,\n                \"end_time\": segment.end_time,\n                \"duration\": segment.end_time - segment.start_time,\n                \"transcript\": segment.content,\n                \"topic\": segment.topic_id,\n                \"keywords\": segment.keywords,\n                \"analysis\": analysis,\n                \"output_path\": str(output_path)\n            }\n\n        except Exception as e:\n            logger.error(f\"Error processing segment {segment.id}: {str(e)}\")\n            raise"
    },
    {
      "filename": "report.md",
      "path": "/vtsV2/src/vts/templates/",
      "content": "# Video Analysis Report: {{ report.title }}\n\n## Overview\n- **Analysis Date:** {{ report.created_at.strftime('%Y-%m-%d %H:%M:%S') }}\n- **Total Duration:** {{ report.total_duration }}\n- **Number of Segments:** {{ report.segment_count }}\n- **Number of Topics:** {{ report.topic_count }}\n\n{{ report.description }}\n\n## Topics Identified\n{% for topic in report.topics %}\n### Topic {{ topic.id + 1 }}\n- **Keywords:** {{ topic.keywords|join(', ') }}\n- **Segments:** {{ topic.segments|length }}\n{% endfor %}\n\n## Detailed Segment Analysis\n{% for segment in report.segments %}\n### Segment {{ segment.id }}\n- **Timestamp:** {{ segment.timestamp }}\n- **Duration:** {{ segment.duration }}\n- **Topic:** Topic {{ segment.topic_id + 1 }}\n- **Keywords:** {{ segment.keywords|join(', ') }}\n\n**Content:**\n{{ segment.content }}\n\n**Analysis:**\n{{ segment.analysis }}\n\n---\n{% endfor %}\n\n## Metadata\n```json\n{{ report.metadata | tojson(indent=2) }}\n```\n\n*Report generated by VTS (Video Topic Segmentation) on {{ report.created_at.strftime('%Y-%m-%d %H:%M:%S') }}*"
    },
    {
      "filename": "main.py",
      "path": "/vtsV2/src/vts/",
      "content": "import logging\nimport sys\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Optional, Dict, Any\nimport click\nfrom rich.console import Console\nfrom rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn\n\nfrom vts.config import Settings\nfrom vts.core.audio import AudioProcessor\nfrom vts.core.transcription import TranscriptionManager\nfrom vts.core.reporting import ReportGenerator\nfrom vts.analysis.topics import TopicAnalyzer\nfrom vts.analysis.video import VideoAnalyzer\nfrom vts.models import VideoProject, AnalysisReport\nfrom vts.utils import setup_logging\n\nlogger = logging.getLogger(__name__)\nconsole = Console()\n\nclass VideoTopicSegmentation:\n    \"\"\"Main application class for Video Topic Segmentation\"\"\"\n    \n    def __init__(self, settings: Optional[Settings] = None):\n        self.settings = settings or Settings()\n        self.audio_processor = AudioProcessor(self.settings)\n        self.transcription_manager = TranscriptionManager(self.settings)\n        self.topic_analyzer = TopicAnalyzer(self.settings)\n        self.video_analyzer = VideoAnalyzer(self.settings)\n        self.report_generator = ReportGenerator()\n        \n    def create_project(self, input_path: Path) -> VideoProject:\n        \"\"\"Create a new project instance\"\"\"\n        project_id = f\"{input_path.stem}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        project_dir = self.settings.BASE_OUTPUT_DIR / project_id\n        project_dir.mkdir(parents=True, exist_ok=True)\n        \n        return VideoProject(\n            id=project_id,\n            input_path=input_path,\n            output_dir=self.settings.BASE_OUTPUT_DIR,\n            created_at=datetime.now()\n        )\n\n    def process_audio(self, project: VideoProject) -> Path:\n        \"\"\"Process audio from video file\"\"\"\n        logger.info(\"Processing audio...\")\n        audio_dir = project.project_dir / \"audio\"\n        audio_dir.mkdir(exist_ok=True)\n        \n        # Extract and normalize audio\n        raw_audio = audio_dir / \"raw_audio.wav\"\n        self.audio_processor.extract_audio(project.input_path, raw_audio)\n        \n        normalized_audio = audio_dir / \"normalized_audio.wav\"\n        result = self.audio_processor.normalize_audio(raw_audio, normalized_audio)\n        if result[\"status\"] == \"error\":\n            raise RuntimeError(f\"Audio normalization failed: {result['message']}\")\n            \n        # Convert to mono and resample\n        processed_audio = audio_dir / \"processed_audio.m4a\"\n        result = self.audio_processor.convert_to_mono(normalized_audio, processed_audio)\n        if result[\"status\"] == \"error\":\n            raise RuntimeError(f\"Audio conversion failed: {result['message']}\")\n            \n        return processed_audio\n\n    def transcribe(self, audio_path: Path, api: str = \"deepgram\", prompt: Optional[str] = None):\n        \"\"\"Transcribe audio using specified API\"\"\"\n        logger.info(f\"Transcribing audio using {api}...\")\n        \n        if api == \"deepgram\":\n            return self.transcription_manager.transcribe_with_deepgram(audio_path)\n        elif api == \"groq\":\n            return self.transcription_manager.transcribe_with_groq(audio_path, prompt)\n        else:\n            raise ValueError(f\"Unsupported transcription API: {api}\")\n\n    def analyze_topics(self, project: VideoProject):\n        \"\"\"Perform topic analysis on transcribed content\"\"\"\n        logger.info(\"Analyzing topics...\")\n        \n        # Combine all transcript segments\n        full_text = \" \".join(seg[\"content\"] for seg in project.transcription.segments)\n        \n        # Preprocess and build topic model\n        subjects = self.topic_analyzer.preprocess_text(full_text)\n        model, dictionary = self.topic_analyzer.build_topic_model(subjects)\n        \n        # Analyze segments\n        segments = []\n        for i, seg in enumerate(project.transcription.segments):\n            topic_id, confidence = self.topic_analyzer.get_dominant_topic(\n                seg[\"content\"], model, dictionary\n            )\n            # Create Segment object instead of dictionary\n            segment = Segment(\n                id=i + 1,\n                start_time=float(seg[\"start\"]),  # Convert to float\n                end_time=float(seg[\"end\"]),      # Convert to float\n                content=seg[\"content\"],\n                topic_id=topic_id,\n                keywords=None  # Will be populated later\n            )\n            segments.append(segment)\n            \n        return model, segments\n\n    def analyze_video(self, project: VideoProject, segments):\n        \"\"\"Perform video analysis on segments\"\"\"\n        logger.info(\"Analyzing video segments...\")\n        segments_dir = project.project_dir / \"segments\"\n        segments_dir.mkdir(exist_ok=True)\n        \n        # Ensure segments are Segment objects\n        if segments and isinstance(segments[0], dict):\n            segments = [\n                Segment(\n                    id=s.get(\"id\", i+1),\n                    start_time=float(s[\"start_time\"]),\n                    end_time=float(s[\"end_time\"]),\n                    content=s[\"content\"],\n                    topic_id=s.get(\"topic_id\"),\n                    keywords=s.get(\"keywords\", [])\n                )\n                for i, s in enumerate(segments)\n            ]\n        \n        return self.video_analyzer.split_video(project, segments)\n\n    def generate_report(self, project: VideoProject, analysis_results: Dict[str, Any]) -> Path:\n        \"\"\"Generate analysis report\"\"\"\n        logger.info(\"Generating analysis report...\")\n        report = create_analysis_report(analysis_results, project.project_dir)\n        return self.report_generator.save_report(report, project.project_dir)\n\n    def process_video(\n        self,\n        input_path: Path,\n        api: str = \"deepgram\",\n        prompt: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Main processing pipeline\"\"\"\n        try:\n            with Progress(\n                SpinnerColumn(),\n                TextColumn(\"[progress.description]{task.description}\"),\n                BarColumn(),\n                TimeElapsedColumn(),\n                console=console\n            ) as progress:\n                # Create project\n                task = progress.add_task(\"Creating project...\", total=None)\n                project = self.create_project(input_path)\n                progress.update(task, completed=True)\n\n                # Process audio\n                task = progress.add_task(\"Processing audio...\", total=None)\n                processed_audio = self.process_audio(project)\n                progress.update(task, completed=True)\n\n                # Transcribe\n                task = progress.add_task(f\"Transcribing with {api}...\", total=None)\n                project.transcription = self.transcribe(processed_audio, api, prompt)\n                progress.update(task, completed=True)\n\n                # Analyze topics\n                task = progress.add_task(\"Analyzing topics...\", total=None)\n                topic_model, segments = self.analyze_topics(project)\n                progress.update(task, completed=True)\n\n                # Analyze video\n                task = progress.add_task(\"Analyzing video segments...\", total=None)\n                analyzed_segments = self.analyze_video(project, segments)\n                progress.update(task, completed=True)\n\n                # Generate report\n                task = progress.add_task(\"Generating report...\", total=None)\n                analysis_results = {\n                    \"project_id\": project.id,\n                    \"topics\": topic_model.show_topics(),\n                    \"segments\": analyzed_segments,\n                    \"metadata\": {\n                        \"input_file\": str(input_path),\n                        \"created_at\": datetime.now().isoformat(),\n                        \"transcription_api\": api\n                    }\n                }\n                report_path = self.generate_report(project, analysis_results)\n                progress.update(task, completed=True)\n\n                return {\n                    \"project_id\": project.id,\n                    \"project_dir\": str(project.project_dir),\n                    \"report_path\": str(report_path),\n                    \"results\": analysis_results\n                }\n\n        except Exception as e:\n            logger.error(f\"Error processing video: {str(e)}\", exc_info=True)\n            raise\n\n@click.command()\n@click.option(\n    \"-i\",\n    \"--input\",\n    required=True,\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Input video file path\"\n)\n@click.option(\n    \"-o\",\n    \"--output\",\n    type=click.Path(path_type=Path),\n    help=\"Base output directory\"\n)\n@click.option(\n    \"--api\",\n    type=click.Choice([\"deepgram\", \"groq\"]),\n    default=\"deepgram\",\n    help=\"Transcription API to use\"\n)\n@click.option(\n    \"--prompt\",\n    help=\"Optional prompt for Groq transcription\"\n)\n@click.option(\n    \"--debug\",\n    is_flag=True,\n    help=\"Enable debug logging\"\n)\ndef main(\n    input: Path,\n    output: Optional[Path],\n    api: str,\n    prompt: Optional[str],\n    debug: bool\n):\n    \"\"\"Video Topic Segmentation - Analyze and segment videos based on content topics\"\"\"\n    try:\n        # Setup logging\n        log_level = logging.DEBUG if debug else logging.INFO\n        setup_logging(log_level)\n        \n        # Initialize settings\n        settings = Settings()\n        if output:\n            settings.BASE_OUTPUT_DIR = output\n            \n        # Process video\n        vts = VideoTopicSegmentation(settings)\n        results = vts.process_video(input, api, prompt)\n        \n        # Print results\n        console.print(\"\\n✨ Processing complete!\")\n        console.print(f\"Project ID: {results['project_id']}\")\n        console.print(f\"Project directory: {results['project_dir']}\")\n        console.print(f\"Analysis report: {results['report_path']}\")\n        \n    except Exception as e:\n        console.print(f\"\\n❌ Error: {str(e)}\", style=\"bold red\")\n        if debug:\n            console.print_exception()\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
      "filename": "utils.py",
      "path": "/vtsV2/src/vts/",
      "content": "import logging\nimport sys\nfrom pathlib import Path\nfrom rich.logging import RichHandler\n\ndef setup_logging(level: int = logging.INFO) -> None:\n    \"\"\"Setup logging configuration\"\"\"\n    logging.basicConfig(\n        level=level,\n        format=\"%(message)s\",\n        datefmt=\"[%X]\",\n        handlers=[RichHandler(rich_tracebacks=True)]\n    )"
    },
    {
      "filename": "Untitled-1.ipynb",
      "path": "/vtsV2/src/vts/",
      "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"print(\\\"hello\\\")\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"language_info\": {\n   \"name\": \"python\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n"
    },
    {
      "filename": "README.md",
      "path": "/vtsV2/",
      "content": "# Video Topic Segmentation (VTS)\n\nVTS is a powerful Python application that automatically segments and analyzes video content based on topics. It combines audio processing, speech recognition, natural language processing, and computer vision to provide detailed insights into video content.\n\n[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n## 🚀 Features\n\n- **Automated Video Segmentation**: Intelligently splits videos into coherent segments based on topic changes\n- **Multi-Modal Analysis**: Combines audio, text, and visual analysis for comprehensive understanding\n- **Topic Modeling**: Identifies and clusters main topics discussed in the video\n- **Advanced Audio Processing**:\n  - Noise reduction and normalization\n  - Silence removal\n  - High-pass filtering\n  - Automated gain control\n- **Multiple Transcription Options**:\n  - Deepgram API integration\n  - Groq API support\n- **Visual Analysis**: Uses Google's Gemini to analyze visual content\n- **Detailed Reports**: Generates comprehensive markdown reports including:\n  - Topic summaries\n  - Segment analysis\n  - Timestamps\n  - Content transcriptions\n  - Visual scene descriptions\n\n## 📋 Prerequisites\n\n- Python 3.9 or higher\n- FFmpeg\n- Required API keys:\n  - Deepgram and/or Groq for transcription\n  - Google Gemini for visual analysis\n\n## 🔧 Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/vts.git\ncd vts\n```\n\n2. Create and activate a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n3. Install the package:\n```bash\npip install -e .\n```\n\n4. Set up environment variables:\n```bash\ncp .env.example .env\n# Edit .env with your API keys and preferences\n```\n\n## 🎯 Quick Start\n\n1. Basic usage:\n```bash\npython -m vts.main -i path/to/video.mp4\n```\n\n2. With custom options:\n```bash\npython -m vts.main -i video.mp4 -o output/dir --api groq --debug\n```\n\n3. Using as a Python package:\n```python\nfrom vts import VideoTopicSegmentation\nfrom vts.config import Settings\n\n# Initialize\nsettings = Settings()\nvts = VideoTopicSegmentation(settings)\n\n# Process video\nresults = vts.process_video(\"video.mp4\", api=\"deepgram\")\nprint(f\"Analysis report generated at: {results['report_path']}\")\n```\n\n## ⚙️ Configuration\n\nCreate a `.env` file with your settings:\n\n```env\n# API Keys\nDEEPGRAM_API_KEY=your_deepgram_key\nGROQ_API_KEY=your_groq_key\nGEMINI_API_KEY=your_gemini_key\n\n# Audio Processing\nSAMPLE_RATE=16000\nAUDIO_CHANNELS=1\nAUDIO_BITRATE=128k\nHIGHPASS_FREQ=200\nLOWPASS_FREQ=8000\n\n# Topic Modeling\nDEFAULT_NUM_TOPICS=5\nMIN_TOPIC_COHERENCE=0.3\n\n# Output\nBASE_OUTPUT_DIR=./output\n```\n\n## 📝 Example Output\n\nThe application generates a detailed markdown report for each processed video:\n\n```markdown\n# Video Analysis Report: Example Video\n\n## Overview\n- Analysis Date: 2024-10-22 14:30:00\n- Total Duration: 00:15:30\n- Number of Segments: 8\n- Number of Topics: 3\n\n## Topics Identified\n### Topic 1\n- Keywords: technology, AI, future, innovation\n- Segments: 3\n\n[... detailed segment analysis ...]\n```\n\n## 🛠️ Development\n\n1. Set up development environment:\n```bash\npip install -e \".[dev]\"\n```\n\n2. Run tests:\n```bash\npytest tests/\n```\n\n3. Format code:\n```bash\nblack src/\nisort src/\n```\n\n## 📊 Project Structure\n\n```\nvts/\n├── pyproject.toml\n├── README.md\n├── src/\n│   └── vts/\n│       ├── core/\n│       │   ├── audio.py\n│       │   ├── metadata.py\n│       │   ├── reporting.py\n│       │   └── transcription.py\n│       ├── analysis/\n│       │   ├── topics.py\n│       │   └── video.py\n│       ├── config.py\n│       ├── models.py\n│       └── utils.py\n└── tests/\n```\n\n## 🤝 Contributing\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## 📜 License\n\nDistributed under the MIT License. See `LICENSE` for more information.\n\n## 🙏 Acknowledgments\n\n- [Deepgram](https://deepgram.com/) for speech recognition\n- [Groq](https://groq.com/) for alternative transcription\n- [Google Gemini](https://ai.google/discover/gemini/) for visual analysis\n- [spaCy](https://spacy.io/) for NLP capabilities\n- [Gensim](https://radimrehurek.com/gensim/) for topic modeling\n\n## 📮 Contact\n\nYour Name - [@yourusername](https://twitter.com/yourusername)\n\nProject Link: [https://github.com/yourusername/vts](https://github.com/yourusername/vts)"
    },
    {
      "filename": "pyproject.toml",
      "path": "/vtsV2/",
      "content": "[project]\nname = \"vts\"\nversion = \"0.2.0\"\ndescription = \"Video Topic Segmentation - Analyze and segment videos based on content topics\"\nauthors = [\n    {name = \"Your Name\", email = \"your.email@example.com\"},\n]\ndependencies = [\n    \"scipy>=1.13.0\",\n    \"numpy>=1.26.0\",\n    \"pydub>=0.25.1\",\n    \"spacy>=3.7.2\",\n    \"gensim>=4.3.2\",\n    \"videogrep>=2.0.0\",\n    \"deepgram-sdk==3.7.4\",\n    \"groq==0.11.0\",\n    \"google-generativeai>=0.6.0\",\n    \"moviepy>=1.0.3\",\n    \"progressbar2>=4.2.0\",\n    \"python-dotenv>=1.0.0\",\n    \"pydantic_settings==2.6.0\",\n    \"timedelta\",\n    \"librosa\",\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.pytest.ini_options]\npythonpath = [\"src\"]\ntestpaths = [\"tests\"]"
    }
  ]
}
